{"cells":[{"cell_type":"markdown","metadata":{"id":"TnczxE21oTqw"},"source":["## Implementation of \"Stage 1 \" of **StackGAN**\n","\n","### Stage I of StackGAN "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:11:47.105993Z","iopub.status.busy":"2024-06-18T12:11:47.105320Z","iopub.status.idle":"2024-06-18T12:11:58.525434Z","shell.execute_reply":"2024-06-18T12:11:58.524687Z","shell.execute_reply.started":"2024-06-18T12:11:47.105958Z"},"id":"StpwZzOYxFRo","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["\"\"\"StackGAN.\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import pickle\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","assert tf.__version__.startswith('2')\n","\n","import PIL\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras.layers import LeakyReLU, BatchNormalization, ReLU, Activation\n","from tensorflow.keras.layers import UpSampling2D, Conv2D, Concatenate, Dense, concatenate\n","from tensorflow.keras.layers import Flatten, Lambda, Reshape, ZeroPadding2D, add\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"markdown","metadata":{"id":"8Rb4IqOBxN1C"},"source":["\n","############################################################\n","# Conditioning Augmentation Network\n","############################################################\n","\n","\n","Computer doesn’t understand words, but it can represent the words in terms of something it does “understand”. That’s the “text embedding”, and it’s used as the c\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:30.038699Z","iopub.status.busy":"2024-06-18T12:12:30.037550Z","iopub.status.idle":"2024-06-18T12:12:30.047102Z","shell.execute_reply":"2024-06-18T12:12:30.045944Z","shell.execute_reply.started":"2024-06-18T12:12:30.038662Z"},"id":"SO_uUHnyxNOU","trusted":true},"outputs":[],"source":["# conditioned by the text.\n","def conditioning_augmentation(x):\n","\t\"\"\"The mean_logsigma passed as argument is converted into the text conditioning variable.\n","\n","\tArgs:\n","\t\tx: The output of the text embedding passed through a FC layer with LeakyReLU non-linearity.\n","\n","\tReturns:\n","\t \tc: The text conditioning variable after computation.\n","\t\"\"\"\n","\tmean = x[:, :128]\n","\tlog_sigma = x[:, 128:]\n","\n","\tstddev = tf.math.exp(log_sigma)\n","\tepsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32'))\n","\tc = mean + stddev * epsilon\n","\treturn c\n","\n","def build_ca_network():\n","\t\"\"\"Builds the conditioning augmentation network.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,)) #size of the vocabulary in the text data\n","\tmls = Dense(256)(input_layer1)\n","\tmls = LeakyReLU(alpha=0.2)(mls)\n","\tca = Lambda(conditioning_augmentation)(mls)\n","\treturn Model(inputs=[input_layer1], outputs=[ca]) \n"]},{"cell_type":"markdown","metadata":{},"source":["############################################################\n","# Stage 1 Generator Network \n","############################################################\n","\n","1. The generator is fed with the text captions in the form of Embedding vectors which will be used to condition its generation of features.\n","2. A vector with random noise.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:32.107798Z","iopub.status.busy":"2024-06-18T12:12:32.107190Z","iopub.status.idle":"2024-06-18T12:12:32.118873Z","shell.execute_reply":"2024-06-18T12:12:32.118016Z","shell.execute_reply.started":"2024-06-18T12:12:32.107764Z"},"id":"8BaEBiaCxFUd","trusted":true},"outputs":[],"source":["\n","\n","def UpSamplingBlock(x, num_kernels):\n","\t\"\"\"An Upsample block with Upsampling2D, Conv2D, BatchNormalization and a ReLU activation.\n","\n","\tArgs:\n","\t\tx: The preceding layer as input.\n","\t\tnum_kernels: Number of kernels for the Conv2D layer.\n","\n","\tReturns:\n","\t\tx: The final activation layer after the Upsampling block.\n","\t\"\"\"\n","\tx = UpSampling2D(size=(2,2))(x)\n","\tx = Conv2D(num_kernels, kernel_size=(3,3), padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x) #prevent from mode collapse\n","\tx = ReLU()(x)\n","\treturn x\n","\n","\n","def build_stage1_generator():\n","\n","\tinput_layer1 = Input(shape=(1024,))\n","\tca = Dense(256)(input_layer1)\n","\tca = LeakyReLU(alpha=0.2)(ca)\n","\n","\t# Obtain the conditioned text\n","\tc = Lambda(conditioning_augmentation)(ca)\n","\n","\tinput_layer2 = Input(shape=(100,))\n","\tconcat = Concatenate(axis=1)([c, input_layer2]) \n","\n","\tx = Dense(16384, use_bias=False)(concat) \n","\tx = ReLU()(x)\n","\tx = Reshape((4, 4, 1024), input_shape=(16384,))(x)\n","\n","\tx = UpSamplingBlock(x, 512) \n","\tx = UpSamplingBlock(x, 256)\n","\tx = UpSamplingBlock(x, 128)\n","\tx = UpSamplingBlock(x, 64)   # upsampled our image to 64*64*3 \n","\n","\tx = Conv2D(3, kernel_size=3, padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = Activation('tanh')(x)\n","\n","\tstage1_gen = Model(inputs=[input_layer1, input_layer2], outputs=[x, ca]) \n","\treturn stage1_gen\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:36.313660Z","iopub.status.busy":"2024-06-18T12:12:36.313279Z","iopub.status.idle":"2024-06-18T12:12:37.559526Z","shell.execute_reply":"2024-06-18T12:12:37.558620Z","shell.execute_reply.started":"2024-06-18T12:12:36.313628Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 1024)]               0         []                            \n","                                                                                                  \n"," dense (Dense)               (None, 256)                  262400    ['input_1[0][0]']             \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)     (None, 256)                  0         ['dense[0][0]']               \n","                                                                                                  \n"," lambda (Lambda)             (None, 128)                  0         ['leaky_re_lu[0][0]']         \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 228)                  0         ['lambda[0][0]',              \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dense_1 (Dense)             (None, 16384)                3735552   ['concatenate[0][0]']         \n","                                                                                                  \n"," re_lu (ReLU)                (None, 16384)                0         ['dense_1[0][0]']             \n","                                                                                                  \n"," reshape (Reshape)           (None, 4, 4, 1024)           0         ['re_lu[0][0]']               \n","                                                                                                  \n"," up_sampling2d (UpSampling2  (None, 8, 8, 1024)           0         ['reshape[0][0]']             \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 8, 8, 512)            4718592   ['up_sampling2d[0][0]']       \n","                                                                                                  \n"," batch_normalization (Batch  (None, 8, 8, 512)            2048      ['conv2d[0][0]']              \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_1 (ReLU)              (None, 8, 8, 512)            0         ['batch_normalization[0][0]'] \n","                                                                                                  \n"," up_sampling2d_1 (UpSamplin  (None, 16, 16, 512)          0         ['re_lu_1[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_1 (Conv2D)           (None, 16, 16, 256)          1179648   ['up_sampling2d_1[0][0]']     \n","                                                                                                  \n"," batch_normalization_1 (Bat  (None, 16, 16, 256)          1024      ['conv2d_1[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_2 (ReLU)              (None, 16, 16, 256)          0         ['batch_normalization_1[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," up_sampling2d_2 (UpSamplin  (None, 32, 32, 256)          0         ['re_lu_2[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_2 (Conv2D)           (None, 32, 32, 128)          294912    ['up_sampling2d_2[0][0]']     \n","                                                                                                  \n"," batch_normalization_2 (Bat  (None, 32, 32, 128)          512       ['conv2d_2[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_3 (ReLU)              (None, 32, 32, 128)          0         ['batch_normalization_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," up_sampling2d_3 (UpSamplin  (None, 64, 64, 128)          0         ['re_lu_3[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_3 (Conv2D)           (None, 64, 64, 64)           73728     ['up_sampling2d_3[0][0]']     \n","                                                                                                  \n"," batch_normalization_3 (Bat  (None, 64, 64, 64)           256       ['conv2d_3[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_4 (ReLU)              (None, 64, 64, 64)           0         ['batch_normalization_3[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_4 (Conv2D)           (None, 64, 64, 3)            1728      ['re_lu_4[0][0]']             \n","                                                                                                  \n"," activation (Activation)     (None, 64, 64, 3)            0         ['conv2d_4[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 10270400 (39.18 MB)\n","Trainable params: 10268480 (39.17 MB)\n","Non-trainable params: 1920 (7.50 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["generator = build_stage1_generator()\n","generator.summary()"]},{"cell_type":"markdown","metadata":{"id":"bc7hLipAxcXb"},"source":["\n","############################################################\n","# Stage 1 Discriminator Network\n","############################################################\t\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:38.252131Z","iopub.status.busy":"2024-06-18T12:12:38.251290Z","iopub.status.idle":"2024-06-18T12:12:38.264226Z","shell.execute_reply":"2024-06-18T12:12:38.263279Z","shell.execute_reply.started":"2024-06-18T12:12:38.252095Z"},"id":"JwHBP8sCxFXg","trusted":true},"outputs":[],"source":["def ConvBlock(x, num_kernels, kernel_size=(4,4), strides=2, activation=True):\n","\t\"\"\"A ConvBlock with a Conv2D, BatchNormalization and LeakyReLU activation.\n","\n","\tArgs:\n","\t\tx: The preceding layer as input.\n","\t\tnum_kernels: Number of kernels for the Conv2D layer.\n","\n","\tReturns:\n","\t\tx: The final activation layer after the ConvBlock block.\n","\t\"\"\"\n","\tx = Conv2D(num_kernels, kernel_size=kernel_size, padding='same', strides=strides, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\t\n","\tif activation:\n","\t\tx = LeakyReLU(alpha=0.2)(x)\n","\treturn x\n","\n","\n","def build_embedding_compressor():\n","    \"\"\"Build embedding compressor model\n","    \"\"\"\n","    input_layer1 = Input(shape=(1024,)) \n","    x = Dense(128)(input_layer1)\n","    x = ReLU()(x)\n","\n","    model = Model(inputs=[input_layer1], outputs=[x])\n","    return model\n","\n","# the discriminator is fed with two inputs, the feature from Generator and the text embedding\n","def build_stage1_discriminator():\n","\t\"\"\"Builds the Stage 1 Discriminator that uses the 64x64 resolution images from the generator\n","\tand the compressed and spatially replicated embedding.\n","\n","\tReturns:\n","\t\tStage 1 Discriminator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(64, 64, 3))  \n","\n","\tx = Conv2D(64, kernel_size=(4,4), strides=2, padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n","\tx = LeakyReLU(alpha=0.2)(x)\n","\n","\tx = ConvBlock(x, 128)\n","\tx = ConvBlock(x, 256)\n","\tx = ConvBlock(x, 512)\n","\n","\t# Obtain the compressed and spatially replicated text embedding\n","\tinput_layer2 = Input(shape=(4, 4, 128)) #2nd input to discriminator, text embedding\n","\tconcat = concatenate([x, input_layer2])\n","\n","\tx1 = Conv2D(512, kernel_size=(1,1), padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(concat)\n","\tx1 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx1 = LeakyReLU(alpha=0.2)(x)\n","\n","\t# Flatten and add a FC layer to predict.\n","\tx1 = Flatten()(x1)\n","\tx1 = Dense(1)(x1)\n","\tx1 = Activation('sigmoid')(x1)\n","\n","\tstage1_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x1])  \n","\treturn stage1_dis\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:40.108759Z","iopub.status.busy":"2024-06-18T12:12:40.108371Z","iopub.status.idle":"2024-06-18T12:12:40.283388Z","shell.execute_reply":"2024-06-18T12:12:40.282427Z","shell.execute_reply.started":"2024-06-18T12:12:40.108727Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 32, 32, 64)           3072      ['input_3[0][0]']             \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 64)           0         ['conv2d_5[0][0]']            \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 16, 16, 128)          131072    ['leaky_re_lu_1[0][0]']       \n","                                                                                                  \n"," batch_normalization_4 (Bat  (None, 16, 16, 128)          512       ['conv2d_6[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_4[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 8, 8, 256)            524288    ['leaky_re_lu_2[0][0]']       \n","                                                                                                  \n"," batch_normalization_5 (Bat  (None, 8, 8, 256)            1024      ['conv2d_7[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 4, 4, 512)            2097152   ['leaky_re_lu_3[0][0]']       \n","                                                                                                  \n"," batch_normalization_6 (Bat  (None, 4, 4, 512)            2048      ['conv2d_8[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_4 (LeakyReLU)   (None, 4, 4, 512)            0         ['batch_normalization_6[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 512)            0         ['leaky_re_lu_4[0][0]']       \n","                                                                                                  \n"," flatten (Flatten)           (None, 8192)                 0         ['leaky_re_lu_5[0][0]']       \n","                                                                                                  \n"," dense_2 (Dense)             (None, 1)                    8193      ['flatten[0][0]']             \n","                                                                                                  \n"," input_4 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," activation_1 (Activation)   (None, 1)                    0         ['dense_2[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 2767361 (10.56 MB)\n","Trainable params: 2765569 (10.55 MB)\n","Non-trainable params: 1792 (7.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["discriminator = build_stage1_discriminator()\n","discriminator.summary()"]},{"cell_type":"markdown","metadata":{"id":"SXNLnmkFxjI0"},"source":["\n","############################################################\n","# Stage 1 Adversarial Model  (Building a GAN)\n","############################################################\n","\n","Generator and discriminator are stacked together. Output of the former is the input of the latter."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:42.934563Z","iopub.status.busy":"2024-06-18T12:12:42.934187Z","iopub.status.idle":"2024-06-18T12:12:42.943515Z","shell.execute_reply":"2024-06-18T12:12:42.942510Z","shell.execute_reply.started":"2024-06-18T12:12:42.934533Z"},"id":"8rjVyqYhxFag","trusted":true},"outputs":[],"source":["# Building GAN with Generator and Discriminator\n","\n","def build_adversarial(generator_model, discriminator_model):\n","\t\"\"\"Stage 1 Adversarial model.\n","\n","\tArgs:\n","\t\tgenerator_model: Stage 1 Generator Model\n","\t\tdiscriminator_model: Stage 1 Discriminator Model\n","\n","\tReturns:\n","\t\tAdversarial Model.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,))  \n","\tinput_layer2 = Input(shape=(100,)) \n","\tinput_layer3 = Input(shape=(4, 4, 128)) \n","\n","\tx, ca = generator_model([input_layer1, input_layer2]) #text,noise\n","\n","\tdiscriminator_model.trainable = False \n","\n","\tprobabilities = discriminator_model([x, input_layer3]) \n","\tadversarial_model = Model(inputs=[input_layer1, input_layer2, input_layer3], outputs=[probabilities, ca])\n","\treturn adversarial_model\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:45.528863Z","iopub.status.busy":"2024-06-18T12:12:45.528047Z","iopub.status.idle":"2024-06-18T12:12:45.671047Z","shell.execute_reply":"2024-06-18T12:12:45.669975Z","shell.execute_reply.started":"2024-06-18T12:12:45.528827Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 1024)]               0         []                            \n","                                                                                                  \n"," input_6 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," model (Functional)          [(None, 64, 64, 3),          1027040   ['input_5[0][0]',             \n","                              (None, 256)]                0          'input_6[0][0]']             \n","                                                                                                  \n"," input_7 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," model_1 (Functional)        (None, 1)                    2767361   ['model[0][0]',               \n","                                                                     'input_7[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 13037761 (49.74 MB)\n","Trainable params: 10268480 (39.17 MB)\n","Non-trainable params: 2769281 (10.56 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["ganstage1 = build_adversarial(generator, discriminator)\n","ganstage1.summary()"]},{"cell_type":"markdown","metadata":{"id":"g4Dy_RuO42Am"},"source":["############################################################\n","# Train Utilities\n","############################################################\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:48.069506Z","iopub.status.busy":"2024-06-18T12:12:48.068891Z","iopub.status.idle":"2024-06-18T12:12:48.091620Z","shell.execute_reply":"2024-06-18T12:12:48.090732Z","shell.execute_reply.started":"2024-06-18T12:12:48.069461Z"},"id":"tuXx7HDj41XW","trusted":true},"outputs":[],"source":["\n","def checkpoint_prefix():\n","\tcheckpoint_dir = '/kaggle/working/Text to Image/lr_model_checkpoints'\n","\tcheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","\n","\treturn checkpoint_prefix\n","\n","def adversarial_loss(y_true, y_pred):\n","\tmean = y_pred[:, :128]\n","\tls = y_pred[:, 128:]\n","\tloss = -ls + 0.5 * (-1 + tf.math.exp(2.0 * ls) + tf.math.square(mean))\n","\tloss = K.mean(loss)\n","\treturn loss\n","\n","def normalize(input_image, real_image):\n","\tinput_image = (input_image / 127.5) - 1\n","\treal_image = (real_image / 127.5) - 1\n","\n","\treturn input_image, real_image\n","\n","def load_class_ids_filenames(class_id_path, filename_path):\n","\twith open(class_id_path, 'rb') as file:\n","\t\tclass_id = pickle.load(file, encoding='latin1')\n","\n","\twith open(filename_path, 'rb') as file:\n","\t\tfilename = pickle.load(file, encoding='latin1')\n","\n","\treturn class_id, filename\n","\n","def load_text_embeddings(text_embeddings):\n","\twith open(text_embeddings, 'rb') as file:\n","\t\tembeds = pickle.load(file, encoding='latin1')\n","\t\tembeds = np.array(embeds)\n","\n","\treturn embeds\n","\n","def load_bbox(data_path):\n","\tbbox_path = data_path + '/bounding_boxes.txt'\n","\timage_path = data_path + '/images.txt'\n","\tbbox_df = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n","\tfilename_df = pd.read_csv(image_path, delim_whitespace=True, header=None)\n","\n","\tfilenames = filename_df[1].tolist()\n","\tbbox_dict = {i[:-4]:[] for i in filenames[:2]}\n","\n","\tfor i in range(0, len(filenames)):\n","\t\tbbox = bbox_df.iloc[i][1:].tolist()\n","\t\tdict_key = filenames[i][:-4]\n","\t\tbbox_dict[dict_key] = bbox\n","\n","\treturn bbox_dict\n","\n","def load_images(image_path, bounding_box, size):\n","\t\"\"\"Crops the image to the bounding box and then resizes it.\n","\t\"\"\"\n","\timage = Image.open(image_path).convert('RGB')\n","\tw, h = image.size\n","\tif bounding_box is not None:\n","\t\tr = int(np.maximum(bounding_box[2], bounding_box[3]) * 0.75)\n","\t\tc_x = int((bounding_box[0] + bounding_box[2]) / 2)\n","\t\tc_y = int((bounding_box[1] + bounding_box[3]) / 2)\n","\t\ty1 = np.maximum(0, c_y - r)\n","\t\ty2 = np.minimum(h, c_y + r)\n","\t\tx1 = np.maximum(0, c_x - r)\n","\t\tx2 = np.minimum(w, c_x + r)\n","\t\timage = image.crop([x1, y1, x2, y2])\n","\n","\timage = image.resize(size, PIL.Image.BILINEAR)\n","\treturn image\n","\n","def load_data(filename_path, class_id_path, dataset_path, embeddings_path, size):\n","\t\"\"\"Loads the Dataset.\n","\t\"\"\"\n","\tdata_dir = \"/kaggle/input/birdsembedded/birds\"\n","\ttrain_dir = data_dir + \"/train\"\n","\ttest_dir = data_dir + \"/test\"\n","    \n","\tembeddings_path_train =\"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-train.pickle\"\n","\tembeddings_path_test = \"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-test (1).pickle\"\n","\tfilename_path_train = train_dir + \"/filenames.pickle\"\n","\tfilename_path_test = test_dir + \"/filenames.pickle\"\n","\tclass_id_path_train = train_dir + \"/class_info.pickle\"\n","\tclass_id_path_test = test_dir + \"/class_info.pickle\"\n","\tdataset_path = \"/kaggle/input/cub2002011/CUB_200_2011\"\n","\tclass_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n","\tembeddings = load_text_embeddings(embeddings_path)\n","\tbbox_dict = load_bbox(dataset_path)\n","\n","\tx, y, embeds = [], [], []\n","\n","\tfor i, filename in enumerate(filenames):\n","\t\tbbox = bbox_dict[filename]\n","\n","\t\ttry:\t\n","\t\t\timage_path = f'{dataset_path}/images/{filename}.jpg'\n","\t\t\timage = load_images(image_path, bbox, size)\n","\t\t\te = embeddings[i, :, :]\n","\t\t\tembed_index = np.random.randint(0, e.shape[0] - 1)\n","\t\t\tembed = e[embed_index, :]\n","\n","\t\t\tx.append(np.array(image))\n","\t\t\ty.append(class_id[i])\n","\t\t\tembeds.append(embed)\n","\n","\t\texcept Exception as e:\n","\t\t\tprint(f'{e}')\n","\t\n","\tx = np.array(x)\n","\ty = np.array(y)\n","\tembeds = np.array(embeds)\n","\t\n","\treturn x, y, embeds\n","\n","def save_image(file, save_path):\n","\t\"\"\"Saves the image at the specified file path.\n","\t\"\"\"\n","\timage = plt.figure()\n","\tax = image.add_subplot(1,1,1)\n","\tax.imshow(file)\n","\tax.axis(\"off\")\n","\tplt.savefig(save_path)\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:13:11.756731Z","iopub.status.busy":"2024-06-18T12:13:11.755996Z","iopub.status.idle":"2024-06-18T12:13:11.761051Z","shell.execute_reply":"2024-06-18T12:13:11.760138Z","shell.execute_reply.started":"2024-06-18T12:13:11.756699Z"},"trusted":true},"outputs":[],"source":["\n","os.mkdir('/kaggle/working/test')\n","os.mkdir('/kaggle/working/weights')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:12:50.645895Z","iopub.status.busy":"2024-06-18T12:12:50.645246Z","iopub.status.idle":"2024-06-18T12:12:50.651336Z","shell.execute_reply":"2024-06-18T12:12:50.650287Z","shell.execute_reply.started":"2024-06-18T12:12:50.645860Z"},"trusted":true},"outputs":[],"source":["data_dir = \"/kaggle/input/birdsembedded/birds\"\n","train_dir = data_dir + \"/train\"\n","test_dir = data_dir + \"/test\"\n","embeddings_path_train = \"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-train.pickle\"\n","embeddings_path_test = \"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-test (1).pickle\"\n","filename_path_train = train_dir + \"/filenames.pickle\"\n","filename_path_test = test_dir + \"/filenames.pickle\"\n","class_id_path_train = train_dir + \"/class_info.pickle\"\n","class_id_path_test = test_dir + \"/class_info.pickle\"\n","dataset_path = \"/kaggle/input/cub2002011/CUB_200_2011\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:40:57.672541Z","iopub.status.busy":"2024-06-18T12:40:57.672049Z","iopub.status.idle":"2024-06-18T12:40:57.710355Z","shell.execute_reply":"2024-06-18T12:40:57.709361Z","shell.execute_reply.started":"2024-06-18T12:40:57.672493Z"},"id":"pjcH1V8ax9A1","trusted":true},"outputs":[],"source":["\n","############################################################\n","# StackGAN class\n","############################################################\n","\n","class StackGanStage1(object):\n","    \"\"\"StackGAN Stage 1 class.\"\"\"\n","\n","    data_dir = \"/kaggle/input/birdsembedded/birds\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","    embeddings_path_train = \"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-train.pickle\"\n","    embeddings_path_test = \"/kaggle/input/embedded-for-cub/Embedded_Text/embedded-test (1).pickle\"\n","    filename_path_train = train_dir + \"/filenames.pickle\"\n","    filename_path_test = test_dir + \"/filenames.pickle\"\n","    class_id_path_train = train_dir + \"/class_info.pickle\"\n","    class_id_path_test = test_dir + \"/class_info.pickle\"\n","    dataset_path = \"/kaggle/input/cub2002011/CUB_200_2011\"\n","    def __init__(self, epochs=100, z_dim=100, batch_size=64, enable_function=True, stage1_generator_lr=0.0002, stage1_discriminator_lr=0.0002):\n","        self.epochs = epochs\n","        self.z_dim = z_dim\n","        self.enable_function = enable_function\n","        self.stage1_generator_lr = stage1_generator_lr\n","        self.stage1_discriminator_lr = stage1_discriminator_lr\n","        self.image_size = 64\n","        self.conditioning_dim = 128\n","        self.batch_size = batch_size\n","\n","        self.stage1_generator_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","        self.stage1_discriminator_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","\n","        self.stage1_generator = build_stage1_generator()\n","        self.stage1_generator.compile(loss='mse', optimizer=self.stage1_generator_optimizer)\n","\n","        self.stage1_discriminator = build_stage1_discriminator()\n","        self.stage1_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage1_discriminator_optimizer)\n","\n","        self.ca_network = build_ca_network()\n","        self.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","        self.embedding_compressor = build_embedding_compressor()\n","        self.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","        self.stage1_adversarial = build_adversarial(self.stage1_generator, self.stage1_discriminator)\n","        self.stage1_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage1_generator_optimizer)\n","\n","        self.checkpoint1 = tf.train.Checkpoint(\n","            generator_optimizer=self.stage1_generator_optimizer,\n","            discriminator_optimizer=self.stage1_discriminator_optimizer,\n","            generator=self.stage1_generator,\n","            discriminator=self.stage1_discriminator)\n","\n","    def visualize_stage1(self):\n","        \"\"\"Running Tensorboard visualizations.\n","        \"\"\"\n","        tb = TensorBoard(log_dir=\"logs/\".format(time.time()))\n","        tb.set_model(self.stage1_generator)\n","        tb.set_model(self.stage1_discriminator)\n","        tb.set_model(self.ca_network)\n","        tb.set_model(self.embedding_compressor)\n","\n","    def train_stage1(self):\n","        \"\"\"Trains the stage1 StackGAN.\n","        \"\"\"\n","        x_train, y_train, train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n","        dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(64, 64))\n","\n","        x_test, y_test, test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test, \n","        dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(64, 64))\n","        \n","        x_train, y_train, train_embeds=x_train[:512], y_train[:512], train_embeds[:512]\n","        x_test, y_test, test_embeds=x_test[:128], y_test[:128], test_embeds[:128]\n","        real = np.ones((self.batch_size, 1), dtype='float') * 0.9\n","        fake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n","\n","        for epoch in range(self.epochs):\n","            print(f'Epoch: {epoch}')\n","\n","            gen_loss = []\n","            dis_loss = []\n","\n","            num_batches = int(x_train.shape[0] / self.batch_size)\n","\n","            for i in range(num_batches):\n","\n","                latent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","                embedding_text = train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n","                compressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n","                compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, 128))\n","                compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","                image_batch = x_train[i * self.batch_size:(i+1) * self.batch_size]\n","                image_batch = (image_batch - 127.5) / 127.5\n","\n","                gen_images, _ = self.stage1_generator.predict([embedding_text, latent_space])\n","\n","                discriminator_loss = self.stage1_discriminator.train_on_batch([image_batch, compressed_embedding], \n","                        np.reshape(real, (self.batch_size, 1)))\n","\n","                discriminator_loss_gen = self.stage1_discriminator.train_on_batch([gen_images, compressed_embedding],\n","                        np.reshape(fake, (self.batch_size, 1)))\n","\n","                discriminator_loss_wrong = self.stage1_discriminator.train_on_batch([gen_images[: self.batch_size-1], compressed_embedding[1:]], \n","                        np.reshape(fake[1:], (self.batch_size-1, 1)))\n","\n","                # Discriminator loss\n","                d_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_wrong))\n","                dis_loss.append(d_loss)\n","\n","                print(f'Discriminator Loss: {d_loss}')\n","\n","                # Generator loss\n","                g_loss = self.stage1_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],\n","                        [K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n","\n","                print(f'Generator Loss: {g_loss}')\n","                gen_loss.append(g_loss)\n","\n","                if epoch % 5 == 0:\n","                        latent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","                        embedding_batch = test_embeds[0 : self.batch_size]\n","                        gen_images, _ = self.stage1_generator.predict_on_batch([embedding_batch, latent_space])\n","\n","                        for i, image in enumerate(gen_images[:10]):\n","                            save_image(image, f'/kaggle/working/test/gen_1_{epoch}_{i}')\n","\n","                if epoch % 25 == 0:\n","                    self.stage1_generator.save_weights('/kaggle/working/weights/stage1_gen.h5')\n","                    self.stage1_discriminator.save_weights(\"/kaggle/working/weights/stage1_disc.h5\")\n","                    self.ca_network.save_weights('/kaggle/working/weights/stage1_ca.h5')\n","                    self.embedding_compressor.save_weights('/kaggle/working/weights/stage1_embco.h5')\n","                    self.stage1_adversarial.save_weights('/kaggle/working/weights/stage1_adv.h5')      \n","\n","        self.stage1_generator.save_weights('/kaggle/working/weights/stage1_gen.h5')\n","        self.stage1_discriminator.save_weights(\"/kaggle/working/weights/stage1_disc.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:40:58.292578Z","iopub.status.busy":"2024-06-18T12:40:58.291771Z","iopub.status.idle":"2024-06-18T12:49:32.682861Z","shell.execute_reply":"2024-06-18T12:49:32.681548Z","shell.execute_reply.started":"2024-06-18T12:40:58.292539Z"},"id":"WUDlLFK0xFdV","outputId":"3aec9e0e-cff9-4742-a2d3-ce4161be794e","scrolled":true,"trusted":true},"outputs":[],"source":["stage1 = StackGanStage1()\n","stage1.train_stage1()"]},{"cell_type":"markdown","metadata":{"id":"inj5IwROxFgE"},"source":["## Check test folder for gernerated images from Stage1 Generator"]},{"cell_type":"markdown","metadata":{},"source":["## Let's Implement Stage 2 Generator"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:12.739555Z","iopub.status.busy":"2024-06-18T12:57:12.739088Z","iopub.status.idle":"2024-06-18T12:57:12.760706Z","shell.execute_reply":"2024-06-18T12:57:12.759716Z","shell.execute_reply.started":"2024-06-18T12:57:12.739519Z"},"trusted":true},"outputs":[],"source":["############################################################\n","# Stage 2 Generator Network\n","############################################################\n","\n","def concat_along_dims(inputs):\n","\t\"\"\"Joins the conditioned text with the encoded image along the dimensions.\n","\n","\tArgs:\n","\t\tinputs: consisting of conditioned text and encoded images as [c,x].\n","\n","\tReturns:\n","\t\tJoint block along the dimensions.\n","\t\"\"\"\n","\tc = inputs[0]\n","\tx = inputs[1]\n","\n","\tc = K.expand_dims(c, axis=1)\n","\tc = K.expand_dims(c, axis=1)\n","\tc = K.tile(c, [1, 16, 16, 1])\n","\treturn K.concatenate([c, x], axis = 3)\n","\n","def residual_block(input):\n","\t\"\"\"Residual block with plain identity connections.\n","\n","\tArgs:\n","\t\tinputs: input layer or an encoded layer\n","\n","\tReturns:\n","\t\tLayer with computed identity mapping.\n","\t\"\"\"\n","\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\t\n","\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\t\n","\tx = add([x, input])\n","\tx = ReLU()(x)\n","\n","\treturn x\n","\n","def build_stage2_generator():\n","\t\"\"\"Build the Stage 2 Generator Network using the conditioning text and images from stage 1.\n","\n","\tReturns:\n","\t\tStage 2 Generator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,))\n","\tinput_images = Input(shape=(64, 64, 3))\n","\n","\t# Conditioning Augmentation\n","\tca = Dense(256)(input_layer1)\n","\tmls = LeakyReLU(alpha=0.2)(ca)\n","\tc = Lambda(conditioning_augmentation)(mls)\n","\n","\t# Downsampling block\n","\tx = ZeroPadding2D(padding=(1,1))(input_images)\n","\tx = Conv2D(128, kernel_size=(3,3), strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = ReLU()(x)\n","\n","\tx = ZeroPadding2D(padding=(1,1))(x)\n","\tx = Conv2D(256, kernel_size=(4,4), strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\tx = ZeroPadding2D(padding=(1,1))(x)\n","\tx = Conv2D(512, kernel_size=(4,4), strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\t# Concatenate text conditioning block with the encoded image\n","\tconcat = concat_along_dims([c, x])\n","\n","\t# Residual Blocks\n","\tx = ZeroPadding2D(padding=(1,1))(concat)\n","\tx = Conv2D(512, kernel_size=(3,3), use_bias=False, kernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\n","\t# Upsampling Blocks\n","\tx = UpSamplingBlock(x, 512)\n","\tx = UpSamplingBlock(x, 256)\n","\tx = UpSamplingBlock(x, 128)\n","\tx = UpSamplingBlock(x, 64)\n","\n","\tx = Conv2D(3, kernel_size=(3,3), padding='same', use_bias=False, kernel_initializer='he_uniform')(x)\n","\tx = Activation('tanh')(x)\n","\t\n","\tstage2_gen = Model(inputs=[input_layer1, input_images], outputs=[x, mls])\n","\treturn stage2_gen\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:32.344004Z","iopub.status.busy":"2024-06-18T12:57:32.343067Z","iopub.status.idle":"2024-06-18T12:57:33.059761Z","shell.execute_reply":"2024-06-18T12:57:33.058740Z","shell.execute_reply.started":"2024-06-18T12:57:32.343968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_13\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_27 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n","                                                                                                  \n"," zero_padding2d (ZeroPaddin  (None, 66, 66, 3)            0         ['input_27[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_30 (Conv2D)          (None, 64, 64, 128)          3456      ['zero_padding2d[0][0]']      \n","                                                                                                  \n"," re_lu_17 (ReLU)             (None, 64, 64, 128)          0         ['conv2d_30[0][0]']           \n","                                                                                                  \n"," zero_padding2d_1 (ZeroPadd  (None, 66, 66, 128)          0         ['re_lu_17[0][0]']            \n"," ing2D)                                                                                           \n","                                                                                                  \n"," input_26 (InputLayer)       [(None, 1024)]               0         []                            \n","                                                                                                  \n"," conv2d_31 (Conv2D)          (None, 32, 32, 256)          524288    ['zero_padding2d_1[0][0]']    \n","                                                                                                  \n"," dense_13 (Dense)            (None, 256)                  262400    ['input_26[0][0]']            \n","                                                                                                  \n"," batch_normalization_24 (Ba  (None, 32, 32, 256)          1024      ['conv2d_31[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_20 (LeakyReLU)  (None, 256)                  0         ['dense_13[0][0]']            \n","                                                                                                  \n"," re_lu_18 (ReLU)             (None, 32, 32, 256)          0         ['batch_normalization_24[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," lambda_5 (Lambda)           (None, 128)                  0         ['leaky_re_lu_20[0][0]']      \n","                                                                                                  \n"," zero_padding2d_2 (ZeroPadd  (None, 34, 34, 256)          0         ['re_lu_18[0][0]']            \n"," ing2D)                                                                                           \n","                                                                                                  \n"," tf.expand_dims (TFOpLambda  (None, 1, 128)               0         ['lambda_5[0][0]']            \n"," )                                                                                                \n","                                                                                                  \n"," conv2d_32 (Conv2D)          (None, 16, 16, 512)          2097152   ['zero_padding2d_2[0][0]']    \n","                                                                                                  \n"," tf.expand_dims_1 (TFOpLamb  (None, 1, 1, 128)            0         ['tf.expand_dims[0][0]']      \n"," da)                                                                                              \n","                                                                                                  \n"," batch_normalization_25 (Ba  (None, 16, 16, 512)          2048      ['conv2d_32[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," tf.tile (TFOpLambda)        (None, 16, 16, 128)          0         ['tf.expand_dims_1[0][0]']    \n","                                                                                                  \n"," re_lu_19 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_25[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," tf.concat (TFOpLambda)      (None, 16, 16, 640)          0         ['tf.tile[0][0]',             \n","                                                                     're_lu_19[0][0]']            \n","                                                                                                  \n"," zero_padding2d_3 (ZeroPadd  (None, 18, 18, 640)          0         ['tf.concat[0][0]']           \n"," ing2D)                                                                                           \n","                                                                                                  \n"," conv2d_33 (Conv2D)          (None, 16, 16, 512)          2949120   ['zero_padding2d_3[0][0]']    \n","                                                                                                  \n"," batch_normalization_26 (Ba  (None, 16, 16, 512)          2048      ['conv2d_33[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_20 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_26[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_34 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_20[0][0]']            \n","                                                                                                  \n"," batch_normalization_27 (Ba  (None, 16, 16, 512)          2048      ['conv2d_34[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_21 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_27[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_35 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_21[0][0]']            \n","                                                                                                  \n"," batch_normalization_28 (Ba  (None, 16, 16, 512)          2048      ['conv2d_35[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add (Add)                   (None, 16, 16, 512)          0         ['batch_normalization_28[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_20[0][0]']            \n","                                                                                                  \n"," re_lu_22 (ReLU)             (None, 16, 16, 512)          0         ['add[0][0]']                 \n","                                                                                                  \n"," conv2d_36 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_22[0][0]']            \n","                                                                                                  \n"," batch_normalization_29 (Ba  (None, 16, 16, 512)          2048      ['conv2d_36[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_23 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_29[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_37 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_23[0][0]']            \n","                                                                                                  \n"," batch_normalization_30 (Ba  (None, 16, 16, 512)          2048      ['conv2d_37[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_1 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_30[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_22[0][0]']            \n","                                                                                                  \n"," re_lu_24 (ReLU)             (None, 16, 16, 512)          0         ['add_1[0][0]']               \n","                                                                                                  \n"," conv2d_38 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_24[0][0]']            \n","                                                                                                  \n"," batch_normalization_31 (Ba  (None, 16, 16, 512)          2048      ['conv2d_38[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_25 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_31[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_39 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_25[0][0]']            \n","                                                                                                  \n"," batch_normalization_32 (Ba  (None, 16, 16, 512)          2048      ['conv2d_39[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_2 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_32[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_24[0][0]']            \n","                                                                                                  \n"," re_lu_26 (ReLU)             (None, 16, 16, 512)          0         ['add_2[0][0]']               \n","                                                                                                  \n"," conv2d_40 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_26[0][0]']            \n","                                                                                                  \n"," batch_normalization_33 (Ba  (None, 16, 16, 512)          2048      ['conv2d_40[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_27 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_33[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_41 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_27[0][0]']            \n","                                                                                                  \n"," batch_normalization_34 (Ba  (None, 16, 16, 512)          2048      ['conv2d_41[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_3 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_34[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_26[0][0]']            \n","                                                                                                  \n"," re_lu_28 (ReLU)             (None, 16, 16, 512)          0         ['add_3[0][0]']               \n","                                                                                                  \n"," up_sampling2d_12 (UpSampli  (None, 32, 32, 512)          0         ['re_lu_28[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_42 (Conv2D)          (None, 32, 32, 512)          2359296   ['up_sampling2d_12[0][0]']    \n","                                                                                                  \n"," batch_normalization_35 (Ba  (None, 32, 32, 512)          2048      ['conv2d_42[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_29 (ReLU)             (None, 32, 32, 512)          0         ['batch_normalization_35[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_13 (UpSampli  (None, 64, 64, 512)          0         ['re_lu_29[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_43 (Conv2D)          (None, 64, 64, 256)          1179648   ['up_sampling2d_13[0][0]']    \n","                                                                                                  \n"," batch_normalization_36 (Ba  (None, 64, 64, 256)          1024      ['conv2d_43[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_30 (ReLU)             (None, 64, 64, 256)          0         ['batch_normalization_36[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_14 (UpSampli  (None, 128, 128, 256)        0         ['re_lu_30[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_44 (Conv2D)          (None, 128, 128, 128)        294912    ['up_sampling2d_14[0][0]']    \n","                                                                                                  \n"," batch_normalization_37 (Ba  (None, 128, 128, 128)        512       ['conv2d_44[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_31 (ReLU)             (None, 128, 128, 128)        0         ['batch_normalization_37[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_15 (UpSampli  (None, 256, 256, 128)        0         ['re_lu_31[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_45 (Conv2D)          (None, 256, 256, 64)         73728     ['up_sampling2d_15[0][0]']    \n","                                                                                                  \n"," batch_normalization_38 (Ba  (None, 256, 256, 64)         256       ['conv2d_45[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_32 (ReLU)             (None, 256, 256, 64)         0         ['batch_normalization_38[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_46 (Conv2D)          (None, 256, 256, 3)          1728      ['re_lu_32[0][0]']            \n","                                                                                                  \n"," activation_6 (Activation)   (None, 256, 256, 3)          0         ['conv2d_46[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 28645440 (109.27 MB)\n","Trainable params: 28632768 (109.23 MB)\n","Non-trainable params: 12672 (49.50 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["generator_stage2 = build_stage2_generator()\n","generator_stage2.summary()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:41.193011Z","iopub.status.busy":"2024-06-18T12:57:41.191818Z","iopub.status.idle":"2024-06-18T12:57:41.205642Z","shell.execute_reply":"2024-06-18T12:57:41.204558Z","shell.execute_reply.started":"2024-06-18T12:57:41.192968Z"},"trusted":true},"outputs":[],"source":["\n","############################################################\n","# Stage 2 Discriminator Network\n","############################################################\n","\n","def build_stage2_discriminator():\n","\t\"\"\"Builds the Stage 2 Discriminator that uses the 256x256 resolution images from the generator\n","\tand the compressed and spatially replicated embeddings.\n","\n","\tReturns:\n","\t\tStage 2 Discriminator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(256, 256, 3))\n","\n","\tx = Conv2D(64, kernel_size=(4,4), padding='same', strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n","\tx = LeakyReLU(alpha=0.2)(x)\n","\n","\tx = ConvBlock(x, 128)\n","\tx = ConvBlock(x, 256)\n","\tx = ConvBlock(x, 512)\n","\tx = ConvBlock(x, 1024)\n","\tx = ConvBlock(x, 2048)\n","\tx = ConvBlock(x, 1024, (1,1), 1)\n","\tx = ConvBlock(x, 512, (1,1), 1, False)\n","\n","\tx1 = ConvBlock(x, 128, (1,1), 1)\n","\tx1 = ConvBlock(x1, 128, (3,3), 1)\n","\tx1 = ConvBlock(x1, 512, (3,3), 1, False)\n","\n","\tx2 = add([x, x1])\n","\tx2 = LeakyReLU(alpha=0.2)(x2)\n","\n","\t# Concatenate compressed and spatially replicated embedding\n","\tinput_layer2 = Input(shape=(4, 4, 128))\n","\tconcat = concatenate([x2, input_layer2])\n","\n","\tx3 = Conv2D(512, kernel_size=(1,1), strides=1, padding='same', kernel_initializer='he_uniform')(concat)\n","\tx3 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x3)\n","\tx3 = LeakyReLU(alpha=0.2)(x3)\n","\n","\t# Flatten and add a FC layer\n","\tx3 = Flatten()(x3)\n","\tx3 = Dense(1)(x3)\n","\tx3 = Activation('sigmoid')(x3)\n","\n","\tstage2_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x3])\n","\treturn stage2_dis\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:46.446790Z","iopub.status.busy":"2024-06-18T12:57:46.446421Z","iopub.status.idle":"2024-06-18T12:57:46.890800Z","shell.execute_reply":"2024-06-18T12:57:46.889713Z","shell.execute_reply.started":"2024-06-18T12:57:46.446764Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_14\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_28 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n","                                                                                                  \n"," conv2d_47 (Conv2D)          (None, 128, 128, 64)         3072      ['input_28[0][0]']            \n","                                                                                                  \n"," leaky_re_lu_21 (LeakyReLU)  (None, 128, 128, 64)         0         ['conv2d_47[0][0]']           \n","                                                                                                  \n"," conv2d_48 (Conv2D)          (None, 64, 64, 128)          131072    ['leaky_re_lu_21[0][0]']      \n","                                                                                                  \n"," batch_normalization_39 (Ba  (None, 64, 64, 128)          512       ['conv2d_48[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_22 (LeakyReLU)  (None, 64, 64, 128)          0         ['batch_normalization_39[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_49 (Conv2D)          (None, 32, 32, 256)          524288    ['leaky_re_lu_22[0][0]']      \n","                                                                                                  \n"," batch_normalization_40 (Ba  (None, 32, 32, 256)          1024      ['conv2d_49[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_23 (LeakyReLU)  (None, 32, 32, 256)          0         ['batch_normalization_40[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_50 (Conv2D)          (None, 16, 16, 512)          2097152   ['leaky_re_lu_23[0][0]']      \n","                                                                                                  \n"," batch_normalization_41 (Ba  (None, 16, 16, 512)          2048      ['conv2d_50[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_24 (LeakyReLU)  (None, 16, 16, 512)          0         ['batch_normalization_41[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_51 (Conv2D)          (None, 8, 8, 1024)           8388608   ['leaky_re_lu_24[0][0]']      \n","                                                                                                  \n"," batch_normalization_42 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_51[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_25 (LeakyReLU)  (None, 8, 8, 1024)           0         ['batch_normalization_42[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_52 (Conv2D)          (None, 4, 4, 2048)           3355443   ['leaky_re_lu_25[0][0]']      \n","                                                          2                                       \n","                                                                                                  \n"," batch_normalization_43 (Ba  (None, 4, 4, 2048)           8192      ['conv2d_52[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_26 (LeakyReLU)  (None, 4, 4, 2048)           0         ['batch_normalization_43[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_53 (Conv2D)          (None, 4, 4, 1024)           2097152   ['leaky_re_lu_26[0][0]']      \n","                                                                                                  \n"," batch_normalization_44 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_53[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_27 (LeakyReLU)  (None, 4, 4, 1024)           0         ['batch_normalization_44[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_54 (Conv2D)          (None, 4, 4, 512)            524288    ['leaky_re_lu_27[0][0]']      \n","                                                                                                  \n"," batch_normalization_45 (Ba  (None, 4, 4, 512)            2048      ['conv2d_54[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv2d_55 (Conv2D)          (None, 4, 4, 128)            65536     ['batch_normalization_45[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_46 (Ba  (None, 4, 4, 128)            512       ['conv2d_55[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_28 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_46[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_56 (Conv2D)          (None, 4, 4, 128)            147456    ['leaky_re_lu_28[0][0]']      \n","                                                                                                  \n"," batch_normalization_47 (Ba  (None, 4, 4, 128)            512       ['conv2d_56[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_29 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_47[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_57 (Conv2D)          (None, 4, 4, 512)            589824    ['leaky_re_lu_29[0][0]']      \n","                                                                                                  \n"," batch_normalization_48 (Ba  (None, 4, 4, 512)            2048      ['conv2d_57[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_4 (Add)                 (None, 4, 4, 512)            0         ['batch_normalization_45[0][0]\n","                                                                    ',                            \n","                                                                     'batch_normalization_48[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," leaky_re_lu_30 (LeakyReLU)  (None, 4, 4, 512)            0         ['add_4[0][0]']               \n","                                                                                                  \n"," input_29 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," concatenate_6 (Concatenate  (None, 4, 4, 640)            0         ['leaky_re_lu_30[0][0]',      \n"," )                                                                   'input_29[0][0]']            \n","                                                                                                  \n"," conv2d_58 (Conv2D)          (None, 4, 4, 512)            328192    ['concatenate_6[0][0]']       \n","                                                                                                  \n"," batch_normalization_49 (Ba  (None, 4, 4, 512)            2048      ['conv2d_58[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_31 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_49[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," flatten_3 (Flatten)         (None, 8192)                 0         ['leaky_re_lu_31[0][0]']      \n","                                                                                                  \n"," dense_14 (Dense)            (None, 1)                    8193      ['flatten_3[0][0]']           \n","                                                                                                  \n"," activation_7 (Activation)   (None, 1)                    0         ['dense_14[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 48486401 (184.96 MB)\n","Trainable params: 48472833 (184.91 MB)\n","Non-trainable params: 13568 (53.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["discriminator_stage2 = build_stage2_discriminator()\n","discriminator_stage2.summary()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:52.344413Z","iopub.status.busy":"2024-06-18T12:57:52.344054Z","iopub.status.idle":"2024-06-18T12:57:52.352303Z","shell.execute_reply":"2024-06-18T12:57:52.351171Z","shell.execute_reply.started":"2024-06-18T12:57:52.344384Z"},"trusted":true},"outputs":[],"source":["\n","############################################################\n","# Stage 2 Adversarial Model\n","############################################################\n","\n","def stage2_adversarial_network(stage2_disc, stage2_gen, stage1_gen):\n","\t\"\"\"Stage 2 Adversarial Network.\n","\n","\tArgs:\n","\t\tstage2_disc: Stage 2 Discriminator Model.\n","\t\tstage2_gen: Stage 2 Generator Model.\n","\t\tstage1_gen: Stage 1 Generator Model.\n","\n","\tReturns:\n","\t\tStage 2 Adversarial network.\n","\t\"\"\"\n","\tconditioned_embedding = Input(shape=(1024, ))\n","\tlatent_space = Input(shape=(100, ))\n","\tcompressed_replicated = Input(shape=(4, 4, 128))\n","    \n","\t#the discriminator is trained separately and stage1_gen already trained, and this is the reason why we freeze its layers by setting the property trainable=false\n","\tinput_images, ca = stage1_gen([conditioned_embedding, latent_space])\n","\tstage2_disc.trainable = False\n","\tstage1_gen.trainable = False\n","\n","\timages, ca2 = stage2_gen([conditioned_embedding, input_images])\n","\tprobability = stage2_disc([images, compressed_replicated])\n","\n","\treturn Model(inputs=[conditioned_embedding, latent_space, compressed_replicated],\n","\t\toutputs=[probability, ca2])\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T12:57:55.888391Z","iopub.status.busy":"2024-06-18T12:57:55.887360Z","iopub.status.idle":"2024-06-18T12:57:56.359406Z","shell.execute_reply":"2024-06-18T12:57:56.358397Z","shell.execute_reply.started":"2024-06-18T12:57:55.888353Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_15\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_30 (InputLayer)       [(None, 1024)]               0         []                            \n","                                                                                                  \n"," input_31 (InputLayer)       [(None, 100)]                0         []                            \n","                                                                                                  \n"," model (Functional)          [(None, 64, 64, 3),          1027040   ['input_30[0][0]',            \n","                              (None, 256)]                0          'input_31[0][0]']            \n","                                                                                                  \n"," model_13 (Functional)       [(None, 256, 256, 3),        2864544   ['input_30[0][0]',            \n","                              (None, 256)]                0          'model[1][0]']               \n","                                                                                                  \n"," input_32 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," model_14 (Functional)       (None, 1)                    4848640   ['model_13[0][0]',            \n","                                                          1          'input_32[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 87402241 (333.41 MB)\n","Trainable params: 28632768 (109.23 MB)\n","Non-trainable params: 58769473 (224.19 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["adversarial_stage2 = stage2_adversarial_network(discriminator_stage2, generator_stage2, generator)\n","adversarial_stage2.summary()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:53:55.295476Z","iopub.status.busy":"2024-06-18T13:53:55.294790Z","iopub.status.idle":"2024-06-18T13:53:55.299703Z","shell.execute_reply":"2024-06-18T13:53:55.298731Z","shell.execute_reply.started":"2024-06-18T13:53:55.295431Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:53:55.922146Z","iopub.status.busy":"2024-06-18T13:53:55.921807Z","iopub.status.idle":"2024-06-18T13:53:55.950760Z","shell.execute_reply":"2024-06-18T13:53:55.949868Z","shell.execute_reply.started":"2024-06-18T13:53:55.922121Z"},"trusted":true},"outputs":[],"source":["\n","class StackGanStage2(object):\n","\t\"\"\"StackGAN Stage 2 class.\n","\n","\tArgs:\n","\t\tepochs: Number of epochs\n","\t\tz_dim: Latent space dimensions\n","\t\tbatch_size: Batch Size\n","\t\tenable_function: If True, training function is decorated with tf.function\n","\t\tstage2_generator_lr: Learning rate for stage 2 generator\n","\t\tstage2_discriminator_lr: Learning rate for stage 2 discriminator\n","\t\"\"\"\n","\tdef __init__(self, epochs=10, z_dim=100, batch_size=16, enable_function=True, stage2_generator_lr=0.0002, stage2_discriminator_lr=0.0002):\n","\t\tself.epochs = epochs\n","\t\tself.z_dim = z_dim\n","\t\tself.enable_function = enable_function\n","\t\tself.stage1_generator_lr = stage2_generator_lr\n","\t\tself.stage1_discriminator_lr = stage2_discriminator_lr\n","\t\tself.low_image_size = 64\n","\t\tself.high_image_size = 256\n","\t\tself.conditioning_dim = 128\n","\t\tself.batch_size = batch_size\n","\t\tself.stage2_generator_optimizer = Adam(lr=stage2_generator_lr, beta_1=0.5, beta_2=0.999)\n","\t\tself.stage2_discriminator_optimizer = Adam(lr=stage2_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","\t\tself.stage1_generator = build_stage1_generator()\n","\t\tself.stage1_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n","\t\tself.stage1_generator.load_weights('weights/stage1_gen.h5')\n","\t\tself.stage2_generator = build_stage2_generator()\n","\t\tself.stage2_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n","\n","\t\tself.stage2_discriminator = build_stage2_discriminator()\n","\t\tself.stage2_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage2_discriminator_optimizer)\n","\n","\t\tself.ca_network = build_ca_network()\n","\t\tself.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\tself.embedding_compressor = build_embedding_compressor()\n","\t\tself.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\tself.stage2_adversarial = stage2_adversarial_network(self.stage2_discriminator, self.stage2_generator, self.stage1_generator)\n","\t\tself.stage2_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage2_generator_optimizer)\t\n","\n","\t\tself.checkpoint2 = tf.train.Checkpoint(\n","        \tgenerator_optimizer=self.stage2_generator_optimizer,\n","        \tdiscriminator_optimizer=self.stage2_discriminator_optimizer,\n","        \tgenerator=self.stage2_generator,\n","        \tdiscriminator=self.stage2_discriminator,\n","        \tgenerator1=self.stage1_generator)\n","\n","\tdef visualize_stage2(self):\n","\t\t\"\"\"Running Tensorboard visualizations.\n","\t\t\"\"\"\n","\t\ttb = TensorBoard(log_dir=\"logs/\".format(time.time()))\n","\t\ttb.set_model(self.stage2_generator)\n","\t\ttb.set_model(self.stage2_discriminator)\n","\n","\tdef train_stage2(self):\n","\t\t\"\"\"Trains Stage 2 StackGAN.\n","\t\t\"\"\"\n","\t\tx_high_train, y_high_train, high_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n","      dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(256, 256))\n","\n","\t\tx_high_test, y_high_test, high_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test, \n","      dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(256, 256))\n","\n","\t\tx_low_train, y_low_train, low_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n","      dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(64, 64))\n","\n","\t\tx_low_test, y_low_test, low_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test, \n","      dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(64, 64))\n","\n","\t\treal = np.ones((self.batch_size, 1), dtype='float') * 0.9\n","\t\tfake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n","\n","\t\tfor epoch in tqdm(range(self.epochs)):\n","\t\t\tprint(f'Epoch: {epoch}')\n","\n","\t\t\tgen_loss = []\n","\t\t\tdisc_loss = []\n","\n","\t\t\tnum_batches = int(x_high_train.shape[0] / self.batch_size)\n","\n","\t\t\tfor i in range(num_batches):\n","\n","\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\tembedding_text = high_train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n","\t\t\t\tcompressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n","\t\t\t\tcompressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, self.conditioning_dim))\n","\t\t\t\tcompressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","\t\t\t\timage_batch = x_high_train[i * self.batch_size:(i+1) * self.batch_size]\n","\t\t\t\timage_batch = (image_batch - 127.5) / 127.5\n","\t\t\t\t\n","\t\t\t\tlow_res_fakes, _ = self.stage1_generator.predict([embedding_text, latent_space], verbose=3)\n","\t\t\t\thigh_res_fakes, _ = self.stage2_generator.predict([embedding_text, low_res_fakes], verbose=3)\n","\n","\t\t\t\tdiscriminator_loss = self.stage2_discriminator.train_on_batch([image_batch, compressed_embedding],\n","\t\t\t\t\tnp.reshape(real, (self.batch_size, 1)))\n","\n","\t\t\t\tdiscriminator_loss_gen = self.stage2_discriminator.train_on_batch([high_res_fakes, compressed_embedding],\n","\t\t\t\t\tnp.reshape(fake, (self.batch_size, 1)))\n","\n","\t\t\t\tdiscriminator_loss_fake = self.stage2_discriminator.train_on_batch([image_batch[:(self.batch_size-1)], compressed_embedding[1:]],\n","\t\t\t\t\tnp.reshape(fake[1:], (self.batch_size - 1, 1)))\n","\n","\t\t\t\td_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_fake))\n","\t\t\t\tdisc_loss.append(d_loss)\n","\n","\t\t\t\tprint(f'Discriminator Loss: {d_loss}')\n","\n","\t\t\t\tg_loss = self.stage2_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],\n","\t\t\t\t\t[K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n","\t\t\t\tgen_loss.append(g_loss)\n","\n","\t\t\t\tprint(f'Generator Loss: {g_loss}')\n","\n","\t\t\t\tif epoch % 5 == 0:\n","\t\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\t\tembedding_batch = high_test_embeds[0 : self.batch_size]\n","\n","\t\t\t\t\tlow_fake_images, _ = self.stage1_generator.predict([embedding_batch, latent_space], verbose=3)\n","\t\t\t\t\thigh_fake_images, _ = self.stage2_generator.predict([embedding_batch, low_fake_images], verbose=3)\n","\n","\t\t\t\t\tfor i, image in enumerate(high_fake_images[:10]):\n","\t\t\t\t\t    save_image(image, f'results_stage2/gen_{epoch}_{i}.png')\n","\n","\t\t\t\tif epoch % 10 == 0:\n","\t\t\t\t\tself.stage2_generator.save_weights('weights/stage2_gen.h5')\n","\t\t\t\t\tself.stage2_discriminator.save_weights(\"weights/stage2_disc.h5\")\n","\t\t\t\t\tself.ca_network.save_weights('weights/stage2_ca.h5')\n","\t\t\t\t\tself.embedding_compressor.save_weights('weights/stage2_embco.h5')\n","\t\t\t\t\tself.stage2_adversarial.save_weights('weights/stage2_adv.h5')\n","\n","\t\tself.stage2_generator.save_weights('weights/stage2_gen.h5')\n","\t\tself.stage2_discriminator.save_weights(\"weights/stage2_disc.h5\")\n"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:53:57.393370Z","iopub.status.busy":"2024-06-18T13:53:57.393025Z","iopub.status.idle":"2024-06-18T13:54:05.509835Z","shell.execute_reply":"2024-06-18T13:54:05.508986Z","shell.execute_reply.started":"2024-06-18T13:53:57.393340Z"},"trusted":true},"outputs":[],"source":["import gc\n","import torch\n","def cleanup():\n","    \"\"\"Try to free GPU memory\"\"\"\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","cleanup()"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:54:05.515735Z","iopub.status.busy":"2024-06-18T13:54:05.515449Z","iopub.status.idle":"2024-06-18T13:54:05.971250Z","shell.execute_reply":"2024-06-18T13:54:05.970075Z","shell.execute_reply.started":"2024-06-18T13:54:05.515713Z"},"trusted":true},"outputs":[],"source":["cleanup()"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:14:10.475760Z","iopub.status.busy":"2024-06-18T13:14:10.474923Z","iopub.status.idle":"2024-06-18T13:14:10.480416Z","shell.execute_reply":"2024-06-18T13:14:10.479359Z","shell.execute_reply.started":"2024-06-18T13:14:10.475725Z"},"trusted":true},"outputs":[],"source":["os.mkdir(\"/kaggle/working/results_stage2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T13:54:05.975279Z","iopub.status.busy":"2024-06-18T13:54:05.975004Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0\n","Discriminator Loss: 6.0483264327049255\n","Generator Loss: [120.9015121459961, 116.32646942138672, 2.28752064704895]\n","Discriminator Loss: 3.1258632941171527\n","Generator Loss: [39.70411682128906, 24.542011260986328, 7.581052780151367]\n","Discriminator Loss: 1.7551427688449621\n","Generator Loss: [1264.6466064453125, 1244.7064208984375, 9.97008991241455]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_26/550244555.py:116: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n","  image = plt.figure()\n"]},{"name":"stdout","output_type":"stream","text":["Discriminator Loss: 1.4250288579642074\n","Generator Loss: [1316.5296630859375, 1289.622314453125, 13.453653335571289]\n","Discriminator Loss: 1.2430618990219955\n","Generator Loss: [573.3004760742188, 549.5466918945312, 11.87687873840332]\n","Discriminator Loss: 1.2662344971904531\n","Generator Loss: [456.7267761230469, 430.50244140625, 13.112160682678223]\n","Discriminator Loss: 1.188310402794741\n","Generator Loss: [18.33385467529297, 3.231783866882324, 7.551034927368164]\n","Discriminator Loss: 1.3628358649439178\n","Generator Loss: [7.399942874908447, 0.6011530160903931, 3.399394989013672]\n","Discriminator Loss: 1.2816267049056478\n","Generator Loss: [20.54195785522461, 14.775198936462402, 2.8833792209625244]\n","Discriminator Loss: 1.0623586912406608\n","Generator Loss: [31.49980354309082, 27.027572631835938, 2.2361152172088623]\n","Discriminator Loss: 1.2966096377931535\n","Generator Loss: [78.13349151611328, 73.95524597167969, 2.089121103286743]\n","Discriminator Loss: 1.2710917070508003\n","Generator Loss: [7.316742897033691, 3.9816877841949463, 1.667527675628662]\n","Discriminator Loss: 0.8111760160099948\n","Generator Loss: [11.378839492797852, 7.842198371887207, 1.7683207988739014]\n","Discriminator Loss: 1.2217089446785394\n","Generator Loss: [12.546609878540039, 9.524772644042969, 1.510918378829956]\n","Discriminator Loss: 0.7816320712590823\n","Generator Loss: [10.881546020507812, 7.864348888397217, 1.508598804473877]\n","Discriminator Loss: 0.9664457452308852\n","Generator Loss: [6.882452964782715, 4.369318008422852, 1.2565675973892212]\n","Discriminator Loss: 0.8098618487711065\n","Generator Loss: [6.390017032623291, 4.133000373840332, 1.1285083293914795]\n","Discriminator Loss: 0.9911630275601055\n","Generator Loss: [4.5114569664001465, 2.361361503601074, 1.0750477313995361]\n","Discriminator Loss: 0.7928768328565639\n","Generator Loss: [4.275578498840332, 2.276634693145752, 0.9994717836380005]\n","Discriminator Loss: 0.8533973934681853\n","Generator Loss: [3.569840431213379, 1.737362265586853, 0.9162390232086182]\n","Discriminator Loss: 0.8315372380893677\n","Generator Loss: [5.313261985778809, 3.376565456390381, 0.9683483242988586]\n","Discriminator Loss: 0.8620830072759418\n","Generator Loss: [7.4870219230651855, 5.980946063995361, 0.7530379891395569]\n","Discriminator Loss: 0.8416319397947518\n","Generator Loss: [3.0479929447174072, 1.511031150817871, 0.7684808969497681]\n","Discriminator Loss: 0.8789142114983406\n","Generator Loss: [4.003691673278809, 2.558027982711792, 0.7228317260742188]\n","Discriminator Loss: 0.9641987979994155\n","Generator Loss: [2.767509937286377, 1.3953194618225098, 0.6860951781272888]\n","Discriminator Loss: 0.7194559142226353\n","Generator Loss: [2.990896701812744, 1.727957010269165, 0.6314697861671448]\n","Discriminator Loss: 0.9882394063461106\n","Generator Loss: [2.120774269104004, 0.9502216577529907, 0.5852763056755066]\n","Discriminator Loss: 0.7596251115901396\n","Generator Loss: [2.3472719192504883, 1.3073482513427734, 0.5199618339538574]\n","Discriminator Loss: 0.8657581626903266\n","Generator Loss: [2.169455051422119, 1.04412841796875, 0.5626633167266846]\n","Discriminator Loss: 0.8728568522492424\n","Generator Loss: [2.209432601928711, 1.1845320463180542, 0.5124503374099731]\n","Discriminator Loss: 0.9172499012056505\n","Generator Loss: [2.1494345664978027, 1.1979286670684814, 0.47575294971466064]\n","Discriminator Loss: 0.8055761947798601\n","Generator Loss: [2.486847162246704, 1.4158145189285278, 0.5355163216590881]\n","Discriminator Loss: 0.9112274259823607\n","Generator Loss: [1.6792794466018677, 0.8744926452636719, 0.4023934006690979]\n","Discriminator Loss: 0.8190528879349586\n","Generator Loss: [2.0262463092803955, 1.2547861337661743, 0.3857300877571106]\n","Discriminator Loss: 0.770392150778207\n","Generator Loss: [2.0892865657806396, 1.3389508724212646, 0.3751678168773651]\n","Discriminator Loss: 0.9462639500998193\n","Generator Loss: [1.746451497077942, 1.0177854299545288, 0.36433303356170654]\n","Discriminator Loss: 0.8294704783111229\n","Generator Loss: [2.3398849964141846, 1.3188408613204956, 0.5105220675468445]\n","Discriminator Loss: 0.780571192917705\n","Generator Loss: [1.5676674842834473, 0.8818411827087402, 0.34291312098503113]\n","Discriminator Loss: 0.9285522787176888\n","Generator Loss: [1.7270781993865967, 1.0710318088531494, 0.32802316546440125]\n","Discriminator Loss: 0.8062663478267496\n","Generator Loss: [1.4597653150558472, 0.8907831907272339, 0.28449106216430664]\n","Discriminator Loss: 0.8152078663406428\n","Generator Loss: [1.6436491012573242, 1.1001797914505005, 0.27173465490341187]\n","Discriminator Loss: 0.6308434970851522\n","Generator Loss: [1.4969195127487183, 0.8632547855377197, 0.31683236360549927]\n","Discriminator Loss: 0.73749897594098\n","Generator Loss: [1.2961543798446655, 0.7281969785690308, 0.2839787006378174]\n","Discriminator Loss: 0.8853461546241306\n","Generator Loss: [1.3089039325714111, 0.7758868336677551, 0.2665085792541504]\n","Discriminator Loss: 0.7328438358381391\n","Generator Loss: [1.3017473220825195, 0.7926048636436462, 0.25457119941711426]\n","Discriminator Loss: 0.9300688989460468\n","Generator Loss: [2.09793758392334, 1.5949952602386475, 0.2514711022377014]\n","Discriminator Loss: 0.9565293714404106\n","Generator Loss: [3.6572515964508057, 3.2121834754943848, 0.22253403067588806]\n","Discriminator Loss: 0.7967491971503478\n","Generator Loss: [8.14834213256836, 7.747804641723633, 0.2002686709165573]\n","Discriminator Loss: 0.9171701928280527\n","Generator Loss: [1.4005999565124512, 0.9959508776664734, 0.20232456922531128]\n","Discriminator Loss: 0.7444390350574395\n","Generator Loss: [1.1959130764007568, 0.8124175667762756, 0.1917477250099182]\n","Discriminator Loss: 0.7626322885334957\n","Generator Loss: [1.3063037395477295, 0.8917286992073059, 0.2072874903678894]\n","Discriminator Loss: 0.9130311383050866\n","Generator Loss: [1.1212246417999268, 0.7771056294441223, 0.17205952107906342]\n","Discriminator Loss: 0.7125444081611931\n","Generator Loss: [1.2692930698394775, 0.9205241799354553, 0.1743844747543335]\n","Discriminator Loss: 0.9074303396046162\n","Generator Loss: [1.3818081617355347, 1.004925012588501, 0.18844157457351685]\n","Discriminator Loss: 0.8733035349287093\n","Generator Loss: [1.3979612588882446, 0.9871077537536621, 0.20542675256729126]\n","Discriminator Loss: 0.7190614375867881\n","Generator Loss: [1.252631664276123, 0.7430323362350464, 0.2547996938228607]\n","Discriminator Loss: 0.7442070553661324\n","Generator Loss: [1.0679349899291992, 0.7059710025787354, 0.18098202347755432]\n","Discriminator Loss: 0.7027175417169929\n","Generator Loss: [1.1624852418899536, 0.8599673509597778, 0.15125896036624908]\n","Discriminator Loss: 0.7982386494986713\n","Generator Loss: [1.0258214473724365, 0.7113593816757202, 0.15723103284835815]\n","Discriminator Loss: 0.7348115677013993\n","Generator Loss: [1.4110215902328491, 1.0964843034744263, 0.15726862847805023]\n","Discriminator Loss: 0.8089479990303516\n","Generator Loss: [1.306886911392212, 1.0021162033081055, 0.1523853838443756]\n","Discriminator Loss: 0.8451003204099834\n","Generator Loss: [1.0372915267944336, 0.7528432607650757, 0.14222414791584015]\n","Discriminator Loss: 0.806595504283905\n","Generator Loss: [11.547398567199707, 11.272331237792969, 0.13753357529640198]\n","Discriminator Loss: 0.9540392085909843\n","Generator Loss: [1.812430500984192, 1.5395252704620361, 0.1364526003599167]\n","Discriminator Loss: 1.1877169646322727\n","Generator Loss: [18.449750900268555, 18.21096420288086, 0.1193930134177208]\n","Discriminator Loss: 0.9750066995620728\n","Generator Loss: [57.86140060424805, 57.60931396484375, 0.12604346871376038]\n","Discriminator Loss: 1.1103286245197523\n","Generator Loss: [3.9981095790863037, 3.747352123260498, 0.12537868320941925]\n","Discriminator Loss: 1.2081184043199755\n","Generator Loss: [0.9058574438095093, 0.6790772676467896, 0.11339009553194046]\n","Discriminator Loss: 0.6530315652489662\n","Generator Loss: [5.527276515960693, 5.302347183227539, 0.11246459186077118]\n","Discriminator Loss: 0.791682368144393\n","Generator Loss: [13.81385612487793, 13.582067489624023, 0.11589421331882477]\n","Discriminator Loss: 0.8263088987441733\n","Generator Loss: [6.008779048919678, 5.786988735198975, 0.11089519411325455]\n","Discriminator Loss: 0.875674178590998\n","Generator Loss: [2.5944817066192627, 2.3937606811523438, 0.10036047548055649]\n","Discriminator Loss: 0.7855479009449482\n","Generator Loss: [2.6936044692993164, 2.466163158416748, 0.11372062563896179]\n","Discriminator Loss: 0.7575908526778221\n","Generator Loss: [1.7537609338760376, 1.5303070545196533, 0.11172691732645035]\n","Discriminator Loss: 0.7150256141321734\n","Generator Loss: [2.0848236083984375, 1.8780769109725952, 0.10337333381175995]\n","Discriminator Loss: 0.9762136640492827\n","Generator Loss: [1.0988119840621948, 0.8950772881507874, 0.10186737030744553]\n","Discriminator Loss: 0.7343663137871772\n","Generator Loss: [1.1495534181594849, 0.968826174736023, 0.09036360681056976]\n","Discriminator Loss: 0.7598755192011595\n","Generator Loss: [1.2110633850097656, 1.009330153465271, 0.10086659342050552]\n","Discriminator Loss: 0.825818944722414\n","Generator Loss: [1.3260329961776733, 1.1229771375656128, 0.10152791440486908]\n","Discriminator Loss: 0.7809450306231156\n","Generator Loss: [1.4870423078536987, 1.1237878799438477, 0.18162722885608673]\n","Discriminator Loss: 0.7583970244158991\n","Generator Loss: [1.5897787809371948, 1.3980897665023804, 0.09584452956914902]\n","Discriminator Loss: 0.7180690774694085\n","Generator Loss: [1.648752212524414, 1.4762771129608154, 0.08623752743005753]\n","Discriminator Loss: 0.9412646571872756\n","Generator Loss: [1.2364951372146606, 1.0157358646392822, 0.1103796511888504]\n","Discriminator Loss: 0.7234658664092422\n","Generator Loss: [1.4019834995269775, 1.109815001487732, 0.14608421921730042]\n","Discriminator Loss: 0.6699915577773936\n","Generator Loss: [1.3155906200408936, 1.056732416152954, 0.12942913174629211]\n","Discriminator Loss: 0.8116641792003065\n","Generator Loss: [1.259650468826294, 1.084388256072998, 0.08763110637664795]\n","Discriminator Loss: 0.6340429231058806\n","Generator Loss: [1.0320637226104736, 0.8709657192230225, 0.08054903149604797]\n","Discriminator Loss: 0.7432742977980524\n","Generator Loss: [1.2343709468841553, 1.0576682090759277, 0.08835136145353317]\n","Discriminator Loss: 0.6691118349554017\n","Generator Loss: [1.1794732809066772, 1.0345432758331299, 0.07246500253677368]\n","Discriminator Loss: 0.6998952534631826\n","Generator Loss: [1.2812443971633911, 1.1363604068756104, 0.07244199514389038]\n","Discriminator Loss: 0.7248128228238784\n","Generator Loss: [1.1950130462646484, 1.0204181671142578, 0.08729743957519531]\n","Discriminator Loss: 0.7477821041829884\n","Generator Loss: [1.0947593450546265, 0.9432576298713684, 0.07575085759162903]\n","Discriminator Loss: 0.6991201164200902\n","Generator Loss: [1.1992918252944946, 0.9761322140693665, 0.11157980561256409]\n","Discriminator Loss: 0.7405791393830441\n","Generator Loss: [1.3011395931243896, 1.1087260246276855, 0.09620679914951324]\n","Discriminator Loss: 0.6635600792942569\n","Generator Loss: [1.2606189250946045, 1.1010956764221191, 0.07976159453392029]\n","Discriminator Loss: 0.7650650182331447\n","Generator Loss: [1.0812164545059204, 0.9435767531394958, 0.06881987303495407]\n","Discriminator Loss: 0.7230891918879934\n","Generator Loss: [1.1811410188674927, 1.0263316631317139, 0.0774046927690506]\n","Discriminator Loss: 0.6981585361645557\n","Generator Loss: [1.297278642654419, 1.1480199098587036, 0.07462938129901886]\n","Discriminator Loss: 0.9410336784785613\n","Generator Loss: [1.1481705904006958, 1.0077626705169678, 0.07020395994186401]\n","Discriminator Loss: 0.7815493631642312\n","Generator Loss: [1.2051252126693726, 1.0810697078704834, 0.062027741223573685]\n","Discriminator Loss: 0.65621944173472\n","Generator Loss: [1.2460908889770508, 1.1287952661514282, 0.05864780396223068]\n","Discriminator Loss: 0.6773680055484874\n","Generator Loss: [1.1600313186645508, 1.0522737503051758, 0.053878795355558395]\n","Discriminator Loss: 0.6548103675886523\n","Generator Loss: [1.0660585165023804, 0.9477357864379883, 0.059161387383937836]\n","Discriminator Loss: 0.7073909023893066\n","Generator Loss: [1.0261279344558716, 0.8965218663215637, 0.06480302661657333]\n","Discriminator Loss: 0.7172623403021134\n","Generator Loss: [1.0511081218719482, 0.9271811246871948, 0.0619635283946991]\n","Discriminator Loss: 0.7175446476903744\n","Generator Loss: [1.1126878261566162, 1.0080831050872803, 0.05230238288640976]\n","Discriminator Loss: 0.7032519735075766\n","Generator Loss: [1.1290571689605713, 1.0219900608062744, 0.05353358015418053]\n","Discriminator Loss: 0.6505511633586138\n","Generator Loss: [1.2387325763702393, 1.1238820552825928, 0.057425230741500854]\n","Discriminator Loss: 0.6898811478749849\n","Generator Loss: [1.182756781578064, 0.9917328357696533, 0.09551195055246353]\n","Discriminator Loss: 0.7814088567974977\n","Generator Loss: [1.0171270370483398, 0.9197155833244324, 0.04870573431253433]\n","Discriminator Loss: 0.7099992888106499\n","Generator Loss: [1.1561739444732666, 1.0618722438812256, 0.04715085029602051]\n","Discriminator Loss: 0.6623550252988935\n","Generator Loss: [1.2652924060821533, 1.141548991203308, 0.061871714890003204]\n","Discriminator Loss: 0.7602874271688052\n","Generator Loss: [0.8951762914657593, 0.7968113422393799, 0.0491824746131897]\n","Discriminator Loss: 0.7004303462454118\n","Generator Loss: [1.0325469970703125, 0.924993097782135, 0.053776923567056656]\n","Discriminator Loss: 0.7058724429807626\n","Generator Loss: [1.0900781154632568, 0.9887670278549194, 0.0506555400788784]\n","Discriminator Loss: 0.8338126897579059\n","Generator Loss: [1.0563089847564697, 0.9347397089004517, 0.060784656554460526]\n","Discriminator Loss: 0.7757334858179092\n","Generator Loss: [3.575896978378296, 3.4671173095703125, 0.05438978224992752]\n","Discriminator Loss: 0.8832227494567633\n","Generator Loss: [3.4250643253326416, 3.3282551765441895, 0.04840454086661339]\n","Discriminator Loss: 0.6939269287395291\n","Generator Loss: [3.158694267272949, 3.0608696937561035, 0.04891224950551987]\n","Discriminator Loss: 0.739171689201612\n","Generator Loss: [3.0467066764831543, 2.895491361618042, 0.07560759782791138]\n","Discriminator Loss: 0.627612183772726\n","Generator Loss: [2.959597110748291, 2.853076696395874, 0.05326015502214432]\n","Discriminator Loss: 0.7045026070991298\n","Generator Loss: [2.5345072746276855, 2.3869569301605225, 0.07377517223358154]\n","Discriminator Loss: 0.6911078764824197\n","Generator Loss: [1.395384430885315, 1.1684869527816772, 0.11344873160123825]\n","Discriminator Loss: 0.6824823068454862\n","Generator Loss: [2.6015563011169434, 2.4719486236572266, 0.06480387598276138]\n","Discriminator Loss: 0.6612408100627363\n","Generator Loss: [2.7230985164642334, 2.617202043533325, 0.05294829234480858]\n","Discriminator Loss: 0.7626362310547847\n","Generator Loss: [1.9621732234954834, 1.8556915521621704, 0.05324084311723709]\n","Discriminator Loss: 0.6939873729133978\n","Generator Loss: [1.5144312381744385, 1.4271738529205322, 0.04362866282463074]\n","Discriminator Loss: 0.6432637041434646\n","Generator Loss: [3.248936176300049, 3.1663200855255127, 0.041308097541332245]\n","Discriminator Loss: 0.7201744777412387\n","Generator Loss: [2.1294443607330322, 2.0468554496765137, 0.041294440627098083]\n","Discriminator Loss: 0.7660484898806317\n","Generator Loss: [2.5010135173797607, 2.391407012939453, 0.054803237318992615]\n","Discriminator Loss: 0.713106036739191\n","Generator Loss: [2.227121353149414, 2.142747402191162, 0.04218701273202896]\n","Discriminator Loss: 0.7071193481970113\n","Generator Loss: [2.418834924697876, 2.344326972961426, 0.037254031747579575]\n","Discriminator Loss: 0.7589963198406622\n","Generator Loss: [2.582935094833374, 2.4702258110046387, 0.05635463818907738]\n","Discriminator Loss: 0.6712086647748947\n","Generator Loss: [2.8386402130126953, 2.681764602661133, 0.07843774557113647]\n","Discriminator Loss: 0.7214480422844645\n","Generator Loss: [2.41763973236084, 2.3287501335144043, 0.044444739818573]\n","Discriminator Loss: 0.7311941859734361\n","Generator Loss: [2.117351770401001, 2.0207693576812744, 0.04829125106334686]\n","Discriminator Loss: 0.7112440951168537\n","Generator Loss: [2.6078522205352783, 2.5338170528411865, 0.037017568945884705]\n","Discriminator Loss: 0.6667737367097288\n","Generator Loss: [1.8535410165786743, 1.7939000129699707, 0.029820479452610016]\n","Discriminator Loss: 0.8003787890193053\n","Generator Loss: [2.5952718257904053, 2.5180792808532715, 0.03859627991914749]\n","Discriminator Loss: 0.8124345535761677\n","Generator Loss: [1.5500125885009766, 1.4820597171783447, 0.03397640958428383]\n","Discriminator Loss: 0.6833881088823546\n","Generator Loss: [1.5316076278686523, 1.4646689891815186, 0.033469296991825104]\n","Discriminator Loss: 0.6998409534571692\n","Generator Loss: [1.1437058448791504, 1.0760693550109863, 0.033818233758211136]\n","Discriminator Loss: 0.6579208722978365\n","Generator Loss: [1.4792888164520264, 1.4149751663208008, 0.03215683996677399]\n","Discriminator Loss: 0.637465360108763\n","Generator Loss: [1.4274643659591675, 1.3534252643585205, 0.037019528448581696]\n","Discriminator Loss: 1.0312332131434232\n","Generator Loss: [1.1370729207992554, 1.0731308460235596, 0.03197101131081581]\n","Discriminator Loss: 0.6137736104428768\n","Generator Loss: [1.5634005069732666, 1.4862234592437744, 0.038588497787714005]\n","Discriminator Loss: 0.8843096822092775\n","Generator Loss: [1.333815097808838, 1.2702009677886963, 0.03180709481239319]\n","Discriminator Loss: 0.7065086886286736\n","Generator Loss: [1.1056294441223145, 0.9965473413467407, 0.054541029036045074]\n","Discriminator Loss: 0.7667343337088823\n","Generator Loss: [1.2934625148773193, 1.194831132888794, 0.049315664917230606]\n","Discriminator Loss: 0.6801863135769963\n","Generator Loss: [1.6825181245803833, 1.5448009967803955, 0.06885858625173569]\n","Discriminator Loss: 0.679389802418882\n","Generator Loss: [1.1025769710540771, 1.0364630222320557, 0.033056989312171936]\n","Discriminator Loss: 0.6854550593416207\n","Generator Loss: [0.8505476713180542, 0.7961902618408203, 0.027178702875971794]\n","Discriminator Loss: 0.6875387295149267\n","Generator Loss: [0.9303898811340332, 0.8777724504470825, 0.026308711618185043]\n","Discriminator Loss: 0.6394323032000102\n","Generator Loss: [1.2210105657577515, 1.1296916007995605, 0.04565950110554695]\n","Discriminator Loss: 0.7976331706158817\n","Generator Loss: [0.9383597373962402, 0.879130482673645, 0.029614636674523354]\n","Discriminator Loss: 0.581863448722288\n","Generator Loss: [0.9831438660621643, 0.9246816039085388, 0.029231136664748192]\n","Discriminator Loss: 0.8198955976549769\n","Generator Loss: [0.9090613722801208, 0.8527213335037231, 0.0281700287014246]\n","Discriminator Loss: 0.6464466515462846\n","Generator Loss: [0.9836795926094055, 0.9269202351570129, 0.028379682451486588]\n","Discriminator Loss: 0.7251095394603908\n","Generator Loss: [0.874770998954773, 0.8278704881668091, 0.023450244218111038]\n","Discriminator Loss: 0.697940798883792\n","Generator Loss: [1.0275764465332031, 0.9700405597686768, 0.028767943382263184]\n","Discriminator Loss: 0.6806434923491906\n","Generator Loss: [0.8929386734962463, 0.8398826122283936, 0.026528028771281242]\n","Discriminator Loss: 0.6511905673833098\n","Generator Loss: [1.1294846534729004, 1.04768967628479, 0.04089749976992607]\n","Discriminator Loss: 0.7962445050943643\n","Generator Loss: [1.1736233234405518, 1.0033828020095825, 0.08512025326490402]\n","Discriminator Loss: 0.630137256346643\n","Generator Loss: [1.3645793199539185, 1.195390224456787, 0.08459454774856567]\n","Discriminator Loss: 0.7178967047366314\n","Generator Loss: [1.4122533798217773, 1.3379418849945068, 0.037155769765377045]\n","Discriminator Loss: 0.7476682903943583\n","Generator Loss: [1.6309884786605835, 1.5814343690872192, 0.024777058511972427]\n","Discriminator Loss: 0.6434563697694102\n","Generator Loss: [1.2397432327270508, 1.1779673099517822, 0.030887972563505173]\n","Discriminator Loss: 0.6750217803346459\n","Generator Loss: [1.1061691045761108, 1.0333480834960938, 0.03641052171587944]\n","Discriminator Loss: 0.6690873556071892\n","Generator Loss: [1.056275486946106, 1.0027236938476562, 0.026775898411870003]\n","Discriminator Loss: 0.6145700681809103\n","Generator Loss: [1.1687771081924438, 1.124529480934143, 0.02212381735444069]\n","Discriminator Loss: 0.6689659505063901\n","Generator Loss: [1.144352674484253, 1.0984188318252563, 0.02296695113182068]\n","Discriminator Loss: 0.6788096360396594\n","Generator Loss: [1.0328818559646606, 0.9767802953720093, 0.028050782158970833]\n","Discriminator Loss: 0.649978635716252\n","Generator Loss: [1.0683046579360962, 1.0084097385406494, 0.02994748204946518]\n","Discriminator Loss: 0.7247859355411492\n","Generator Loss: [1.1842668056488037, 1.1229668855667114, 0.030649933964014053]\n","Discriminator Loss: 0.6560757021652535\n","Generator Loss: [1.1998919248580933, 1.1280560493469238, 0.03591792657971382]\n","Discriminator Loss: 0.754095136129763\n","Generator Loss: [1.037232518196106, 0.983769416809082, 0.02673155628144741]\n","Discriminator Loss: 0.6983303133456502\n","Generator Loss: [1.0149818658828735, 0.9613691568374634, 0.026806354522705078]\n","Discriminator Loss: 0.6221729608514579\n","Generator Loss: [1.1869994401931763, 1.1356829404830933, 0.025658274069428444]\n","Discriminator Loss: 0.7522928882390261\n","Generator Loss: [0.9954565763473511, 0.9514275789260864, 0.02201450988650322]\n","Discriminator Loss: 0.664026384241879\n","Generator Loss: [0.9259554147720337, 0.8248738050460815, 0.05054079368710518]\n","Discriminator Loss: 0.655979199858848\n","Generator Loss: [0.9576217532157898, 0.900435745716095, 0.028592996299266815]\n","Discriminator Loss: 0.8059517122164834\n","Generator Loss: [1.0653080940246582, 1.0163999795913696, 0.02445404790341854]\n","Discriminator Loss: 0.697659820318222\n","Generator Loss: [1.1911380290985107, 1.1537381410598755, 0.018699917942285538]\n","Discriminator Loss: 0.7098288055858575\n","Generator Loss: [0.8954634070396423, 0.8588898181915283, 0.018286781385540962]\n","Discriminator Loss: 0.7489405238156905\n","Generator Loss: [0.8872144818305969, 0.8344764113426208, 0.026369037106633186]\n","Discriminator Loss: 0.6484407060779631\n","Generator Loss: [1.0335679054260254, 0.9915202856063843, 0.021023796871304512]\n","Discriminator Loss: 0.7272440981469117\n","Generator Loss: [0.9395356774330139, 0.8920649290084839, 0.02373538166284561]\n","Discriminator Loss: 0.8182537876418792\n","Generator Loss: [0.8898318409919739, 0.8339448571205139, 0.027943486347794533]\n","Discriminator Loss: 0.7685465922113508\n","Generator Loss: [1.0264277458190918, 0.9317035675048828, 0.047362107783555984]\n","Discriminator Loss: 0.8010245104087517\n","Generator Loss: [1.0115243196487427, 0.9721848964691162, 0.019669698551297188]\n","Discriminator Loss: 0.6939248075359501\n","Generator Loss: [1.1875934600830078, 1.1432849168777466, 0.02215428091585636]\n","Discriminator Loss: 0.6792308103758842\n","Generator Loss: [1.2783095836639404, 1.2256604433059692, 0.026324590668082237]\n","Discriminator Loss: 0.6687121746363118\n","Generator Loss: [1.2746318578720093, 1.230271816253662, 0.022180043160915375]\n","Discriminator Loss: 0.6840910704850103\n","Generator Loss: [1.060605525970459, 1.0138380527496338, 0.023383725434541702]\n","Discriminator Loss: 0.6189819032442756\n","Generator Loss: [1.2182222604751587, 1.1764823198318481, 0.020869962871074677]\n","Discriminator Loss: 0.6641114558733534\n","Generator Loss: [1.1372764110565186, 1.100633144378662, 0.018321620300412178]\n","Discriminator Loss: 0.6340315398629173\n","Generator Loss: [1.0931806564331055, 1.051461935043335, 0.020859356969594955]\n","Discriminator Loss: 0.6527535894201719\n","Generator Loss: [1.090062141418457, 1.0501612424850464, 0.019950436428189278]\n","Discriminator Loss: 0.6386891720176209\n","Generator Loss: [1.30838942527771, 1.2692134380340576, 0.019587969407439232]\n","Discriminator Loss: 0.7411677227064501\n","Generator Loss: [1.1113805770874023, 1.079653024673462, 0.01586376503109932]\n","Discriminator Loss: 0.7143946290161693\n","Generator Loss: [0.9542214274406433, 0.9233034253120422, 0.015458992682397366]\n","Discriminator Loss: 0.6234532401940669\n","Generator Loss: [1.0444257259368896, 0.9956474304199219, 0.02438916638493538]\n","Discriminator Loss: 0.7076020466338377\n","Generator Loss: [1.000934362411499, 0.9323369264602661, 0.03429868817329407]\n","Discriminator Loss: 0.6483611089934129\n","Generator Loss: [1.0448822975158691, 0.9958729147911072, 0.024504683911800385]\n","Discriminator Loss: 0.6480588677804917\n","Generator Loss: [1.1016935110092163, 1.0331437587738037, 0.03427489474415779]\n","Discriminator Loss: 0.7129254599567503\n","Generator Loss: [0.9755667448043823, 0.9324283599853516, 0.021569181233644485]\n","Discriminator Loss: 0.6723696584522258\n","Generator Loss: [1.134487509727478, 1.0859622955322266, 0.024262621998786926]\n","Discriminator Loss: 0.6599188655673061\n","Generator Loss: [1.0993009805679321, 1.0674188137054443, 0.015941105782985687]\n","Discriminator Loss: 0.7819030960672535\n","Generator Loss: [0.8904436230659485, 0.8553029298782349, 0.017570337280631065]\n","Discriminator Loss: 0.6404000301845372\n","Generator Loss: [1.0499597787857056, 0.926802933216095, 0.06157839670777321]\n","Discriminator Loss: 0.6347541563445702\n","Generator Loss: [1.1210241317749023, 1.0625743865966797, 0.029224855825304985]\n","Discriminator Loss: 0.6860632243624423\n","Generator Loss: [0.9512780904769897, 0.8905990123748779, 0.030339544638991356]\n","Discriminator Loss: 0.6405148658668622\n","Generator Loss: [0.8793971538543701, 0.8117640018463135, 0.033816561102867126]\n","Discriminator Loss: 0.8198459898703732\n","Generator Loss: [1.01127028465271, 0.8685532808303833, 0.07135853171348572]\n","Discriminator Loss: 0.714858240360627\n","Generator Loss: [0.9550393223762512, 0.9146993160247803, 0.020169991999864578]\n","Discriminator Loss: 0.7844926733814646\n","Generator Loss: [0.9810723662376404, 0.9473336338996887, 0.01686936803162098]\n","Discriminator Loss: 0.6606800222361926\n","Generator Loss: [1.0339694023132324, 0.995664119720459, 0.019152622669935226]\n","Discriminator Loss: 0.7414640169590712\n","Generator Loss: [0.9600126147270203, 0.8934189081192017, 0.033296842128038406]\n","Discriminator Loss: 0.6878308795276098\n","Generator Loss: [0.9746797680854797, 0.9451819658279419, 0.014748914167284966]\n","Discriminator Loss: 0.7099714395590127\n","Generator Loss: [0.9793669581413269, 0.873609721660614, 0.052878618240356445]\n","Discriminator Loss: 0.629367120040115\n","Generator Loss: [0.9839454293251038, 0.9075512290000916, 0.0381971076130867]\n","Discriminator Loss: 0.6078212374122813\n","Generator Loss: [1.0150620937347412, 0.9633868932723999, 0.025837626308202744]\n","Discriminator Loss: 0.6766519326483831\n","Generator Loss: [0.856217622756958, 0.8253064155578613, 0.015455592423677444]\n","Discriminator Loss: 0.5991614545055199\n","Generator Loss: [0.8753480911254883, 0.8434159755706787, 0.015966061502695084]\n","Discriminator Loss: 0.6910849480555044\n","Generator Loss: [0.9491701126098633, 0.9116227626800537, 0.018773680552840233]\n","Discriminator Loss: 0.6684357953781728\n","Generator Loss: [0.9160662889480591, 0.8875063061714172, 0.01427999883890152]\n","Discriminator Loss: 0.6329299789722427\n","Generator Loss: [0.9290432929992676, 0.8928031921386719, 0.018120063468813896]\n","Discriminator Loss: 0.6630199030041695\n","Generator Loss: [0.9523988962173462, 0.8850529789924622, 0.03367296978831291]\n","Discriminator Loss: 0.6385839441500138\n","Generator Loss: [0.9083636403083801, 0.877758800983429, 0.015302412211894989]\n","Discriminator Loss: 0.7042890797602013\n","Generator Loss: [0.817884624004364, 0.7856912612915039, 0.01609667018055916]\n","Discriminator Loss: 0.641267359835183\n","Generator Loss: [1.0056145191192627, 0.9754619002342224, 0.01507630292326212]\n","Discriminator Loss: 0.700390745914774\n","Generator Loss: [0.8086518049240112, 0.7573342323303223, 0.025658776983618736]\n","Discriminator Loss: 0.5910969291071524\n","Generator Loss: [0.9029442667961121, 0.8656304478645325, 0.018656913191080093]\n","Discriminator Loss: 0.6158728387672454\n","Generator Loss: [0.8797619342803955, 0.8472228050231934, 0.016269555315375328]\n","Discriminator Loss: 0.7234436901489971\n","Generator Loss: [0.8516150116920471, 0.8077802062034607, 0.021917404606938362]\n","Discriminator Loss: 0.6270880754418613\n","Generator Loss: [0.9867268800735474, 0.9608852863311768, 0.012920800596475601]\n","Discriminator Loss: 0.7553441474592546\n","Generator Loss: [0.8628281354904175, 0.8268107175827026, 0.018008707091212273]\n","Discriminator Loss: 0.7093033791461494\n","Generator Loss: [0.9936051368713379, 0.9666135907173157, 0.013495782390236855]\n","Discriminator Loss: 0.662871267151786\n","Generator Loss: [0.8615369200706482, 0.837762713432312, 0.011887099593877792]\n","Discriminator Loss: 0.6519696815958014\n","Generator Loss: [1.0121899843215942, 0.9592009782791138, 0.02649448625743389]\n","Discriminator Loss: 0.6682521229377016\n","Generator Loss: [1.0937321186065674, 1.070460319519043, 0.011635886505246162]\n","Discriminator Loss: 0.722773893197882\n","Generator Loss: [0.8149695992469788, 0.7815461158752441, 0.016711749136447906]\n","Discriminator Loss: 0.6193103421246633\n","Generator Loss: [0.9569445848464966, 0.9313111305236816, 0.012816722504794598]\n","Discriminator Loss: 0.7205589996883646\n","Generator Loss: [0.7966164350509644, 0.7697767615318298, 0.013419851660728455]\n","Discriminator Loss: 0.6071817185584223\n","Generator Loss: [1.1264703273773193, 1.103628396987915, 0.011420994997024536]\n","Discriminator Loss: 0.6910543602425605\n","Generator Loss: [0.8854250311851501, 0.8536482453346252, 0.015888385474681854]\n","Discriminator Loss: 0.6518727858492639\n","Generator Loss: [0.9871479272842407, 0.9063013792037964, 0.04042327031493187]\n","Discriminator Loss: 0.7450153244717512\n","Generator Loss: [0.8430039286613464, 0.8064441680908203, 0.018279891461133957]\n","Discriminator Loss: 0.6501317416186794\n","Generator Loss: [1.066758394241333, 1.0296590328216553, 0.018549706786870956]\n","Discriminator Loss: 0.6499341545277275\n","Generator Loss: [1.0651589632034302, 1.0332554578781128, 0.01595173589885235]\n","Discriminator Loss: 0.7336731910181697\n","Generator Loss: [0.8568076491355896, 0.8078960180282593, 0.02445581555366516]\n","Discriminator Loss: 0.639082218403928\n","Generator Loss: [1.0006239414215088, 0.9604490995407104, 0.020087415352463722]\n","Discriminator Loss: 0.6735296227998333\n","Generator Loss: [0.916527271270752, 0.8862885236740112, 0.015119366347789764]\n","Discriminator Loss: 0.701324834022671\n","Generator Loss: [0.9240014553070068, 0.891448974609375, 0.016276249662041664]\n","Discriminator Loss: 0.6273442585297744\n","Generator Loss: [1.0685640573501587, 0.9925134181976318, 0.03802533820271492]\n","Discriminator Loss: 0.6174158643552801\n","Generator Loss: [1.0435237884521484, 1.0034763813018799, 0.020023727789521217]\n","Discriminator Loss: 0.6378458419094386\n","Generator Loss: [0.9837828278541565, 0.9571346044540405, 0.013324118219316006]\n","Discriminator Loss: 0.7218627835973166\n","Generator Loss: [0.822090208530426, 0.7822571992874146, 0.01991649717092514]\n","Discriminator Loss: 0.6812966629950097\n","Generator Loss: [0.8851262331008911, 0.8370051980018616, 0.024060508236289024]\n","Discriminator Loss: 0.6608863070141524\n","Generator Loss: [0.8903878927230835, 0.8406903147697449, 0.024848777800798416]\n","Discriminator Loss: 0.6221885294507956\n","Generator Loss: [0.9491201043128967, 0.9005632400512695, 0.0242784321308136]\n","Discriminator Loss: 0.6854546171671245\n","Generator Loss: [0.8571532964706421, 0.8348233699798584, 0.011164974421262741]\n","Discriminator Loss: 0.7029657387465704\n","Generator Loss: [0.7640613317489624, 0.7401905059814453, 0.01193542405962944]\n","Discriminator Loss: 0.6682252367900219\n","Generator Loss: [1.0080457925796509, 0.980928897857666, 0.013558421283960342]\n","Discriminator Loss: 0.7027015782368835\n","Generator Loss: [0.954362690448761, 0.9169012308120728, 0.01873071864247322]\n","Discriminator Loss: 0.6250032528478187\n","Generator Loss: [0.9753971695899963, 0.9540835618972778, 0.010656807571649551]\n","Discriminator Loss: 0.6571507949847728\n","Generator Loss: [0.946848452091217, 0.9202187061309814, 0.013314886018633842]\n","Discriminator Loss: 0.6460410579165909\n","Generator Loss: [0.8412760496139526, 0.807864785194397, 0.016705621033906937]\n","Discriminator Loss: 0.628823371429462\n","Generator Loss: [0.8957486152648926, 0.8631550669670105, 0.01629677414894104]\n","Discriminator Loss: 0.6156137358339038\n","Generator Loss: [1.0547904968261719, 0.9501703977584839, 0.0523100346326828]\n","Discriminator Loss: 0.6933571496629156\n","Generator Loss: [1.0302562713623047, 1.0011205673217773, 0.014567873440682888]\n","Discriminator Loss: 0.772588826570427\n","Generator Loss: [0.860167920589447, 0.8253453969955444, 0.01741124875843525]\n","Discriminator Loss: 0.6317379678512225\n","Generator Loss: [0.9429655075073242, 0.9202814102172852, 0.011342049576342106]\n","Discriminator Loss: 0.6091881924767222\n","Generator Loss: [0.8745256662368774, 0.855243444442749, 0.009641112759709358]\n","Discriminator Loss: 0.6400138835888356\n","Generator Loss: [1.0190315246582031, 0.9929343461990356, 0.013048600405454636]\n","Discriminator Loss: 0.6906758586555952\n","Generator Loss: [0.8673208951950073, 0.8133729696273804, 0.026973966509103775]\n","Discriminator Loss: 0.6058025397505844\n","Generator Loss: [0.906363844871521, 0.8875957727432251, 0.00938403233885765]\n","Discriminator Loss: 0.7128843942045933\n","Generator Loss: [0.8942173719406128, 0.8577061891555786, 0.01825559139251709]\n","Discriminator Loss: 0.673930183344055\n","Generator Loss: [0.8158395886421204, 0.7937659025192261, 0.01103683840483427]\n","Discriminator Loss: 0.6275466599618085\n","Generator Loss: [1.072891354560852, 1.0511317253112793, 0.010879822075366974]\n","Discriminator Loss: 0.6874630296370015\n","Generator Loss: [0.8588421940803528, 0.8402045965194702, 0.009318807162344456]\n","Discriminator Loss: 0.6237460169068072\n","Generator Loss: [0.8553444743156433, 0.829723060131073, 0.012810716405510902]\n","Discriminator Loss: 0.6585044625389855\n","Generator Loss: [0.8311640024185181, 0.803683876991272, 0.013740051537752151]\n","Discriminator Loss: 0.6025826379045611\n","Generator Loss: [0.9243788719177246, 0.9041985273361206, 0.010090173222124577]\n","Discriminator Loss: 0.7044157159689348\n","Generator Loss: [0.9067312479019165, 0.8798096179962158, 0.01346080657094717]\n","Discriminator Loss: 0.64286715099297\n","Generator Loss: [0.76784747838974, 0.7464338541030884, 0.010706798173487186]\n","Discriminator Loss: 0.6248856313177384\n","Generator Loss: [0.8312081098556519, 0.7616856694221497, 0.034761227667331696]\n","Discriminator Loss: 0.6429262574383756\n","Generator Loss: [1.0429458618164062, 1.0198918581008911, 0.011527030728757381]\n","Discriminator Loss: 0.7090663793787826\n","Generator Loss: [0.9000353813171387, 0.8728377819061279, 0.013598799705505371]\n","Discriminator Loss: 0.6453379103913903\n","Generator Loss: [0.8884141445159912, 0.8582415580749512, 0.015086278319358826]\n","Discriminator Loss: 0.6735080186626874\n","Generator Loss: [0.9539655447006226, 0.8771814107894897, 0.0383920818567276]\n","Discriminator Loss: 0.6478764618077548\n","Generator Loss: [1.0985407829284668, 1.0716748237609863, 0.013432950712740421]\n","Discriminator Loss: 0.6272919875045773\n","Generator Loss: [0.8890947103500366, 0.8586715459823608, 0.015211569145321846]\n","Discriminator Loss: 0.6392242002038984\n","Generator Loss: [0.8180637955665588, 0.7953886985778809, 0.011337537318468094]\n","Discriminator Loss: 0.6370653424528427\n","Generator Loss: [0.9074422121047974, 0.8175618648529053, 0.04494015872478485]\n","Discriminator Loss: 0.6665195397144998\n","Generator Loss: [0.9439136385917664, 0.8752824068069458, 0.034315623342990875]\n","Discriminator Loss: 0.6335944931634003\n","Generator Loss: [0.8951908946037292, 0.8752579689025879, 0.00996647123247385]\n","Discriminator Loss: 0.6807651984709082\n","Generator Loss: [0.8655084371566772, 0.8423478603363037, 0.011580273509025574]\n","Discriminator Loss: 0.6289600256714039\n","Generator Loss: [0.9850369691848755, 0.9663639068603516, 0.009336527436971664]\n","Discriminator Loss: 0.6096572167734848\n","Generator Loss: [1.0380911827087402, 1.0173161029815674, 0.010387558490037918]\n","Discriminator Loss: 0.689698603709985\n","Generator Loss: [0.8494516015052795, 0.8123764991760254, 0.01853753812611103]\n","Discriminator Loss: 0.6166559463526937\n","Generator Loss: [0.8726576566696167, 0.8421837091445923, 0.015236982144415379]\n","Discriminator Loss: 0.6563956264872104\n","Generator Loss: [0.9253408312797546, 0.8692505359649658, 0.02804514952003956]\n","Discriminator Loss: 0.6331326421932317\n","Generator Loss: [0.8947249054908752, 0.8718745708465576, 0.011425153352320194]\n","Discriminator Loss: 0.6224765472725267\n","Generator Loss: [1.0056431293487549, 0.9217705130577087, 0.04193628206849098]\n","Discriminator Loss: 0.7430261129920837\n","Generator Loss: [0.8825465440750122, 0.8580127358436584, 0.012266909703612328]\n","Discriminator Loss: 0.6008112897106912\n","Generator Loss: [1.0630426406860352, 1.0430017709732056, 0.010020424611866474]\n","Discriminator Loss: 0.6400173836882459\n","Generator Loss: [0.8681390285491943, 0.8491384983062744, 0.009500253014266491]\n","Discriminator Loss: 0.6564519751118496\n","Generator Loss: [0.8057359457015991, 0.7755560874938965, 0.015089939348399639]\n","Discriminator Loss: 0.6117131488281302\n","Generator Loss: [0.9298480749130249, 0.8944992423057556, 0.017674416303634644]\n","Discriminator Loss: 0.592425812035799\n","Generator Loss: [1.8697518110275269, 1.8469820022583008, 0.01138488668948412]\n","Discriminator Loss: 0.7202550587244332\n","Generator Loss: [1.7864797115325928, 1.709806203842163, 0.038336776196956635]\n","Discriminator Loss: 0.6201341843698174\n","Generator Loss: [0.9863182902336121, 0.9690194129943848, 0.008649427443742752]\n","Discriminator Loss: 0.6218449824373238\n","Generator Loss: [0.9735543727874756, 0.9319104552268982, 0.02082196995615959]\n","Discriminator Loss: 0.615195516118547\n","Generator Loss: [0.8918900489807129, 0.872848391532898, 0.009520834311842918]\n","Discriminator Loss: 0.6037673605023883\n","Generator Loss: [0.963375985622406, 0.9425587058067322, 0.01040862686932087]\n","Discriminator Loss: 0.6794643821194768\n","Generator Loss: [0.8781036734580994, 0.8617326021194458, 0.008185525424778461]\n","Discriminator Loss: 0.6549926449861232\n","Generator Loss: [0.8209163546562195, 0.790643036365509, 0.015136664733290672]\n","Discriminator Loss: 0.6275125673419097\n","Generator Loss: [0.8858796954154968, 0.8622955083847046, 0.011792106553912163]\n","Discriminator Loss: 0.6618053861529916\n","Generator Loss: [0.9381710290908813, 0.9174001216888428, 0.010385456494987011]\n","Discriminator Loss: 0.6803852543307585\n","Generator Loss: [0.856393039226532, 0.8343303203582764, 0.01103135384619236]\n","Discriminator Loss: 0.6026367172889877\n","Generator Loss: [0.9970094561576843, 0.9082539081573486, 0.04437777027487755]\n","Discriminator Loss: 0.6809200221468927\n","Generator Loss: [0.7622162699699402, 0.7442322373390198, 0.008992007002234459]\n","Discriminator Loss: 0.5850146654556738\n","Generator Loss: [1.0642671585083008, 1.0298898220062256, 0.017188696190714836]\n","Discriminator Loss: 0.6081784844718641\n","Generator Loss: [1.056748390197754, 1.0263737440109253, 0.015187302604317665]\n","Discriminator Loss: 0.6831871182948817\n","Generator Loss: [1.0682510137557983, 0.9947696924209595, 0.036740656942129135]\n","Discriminator Loss: 0.6310383455711417\n","Generator Loss: [0.9536973237991333, 0.9338192939758301, 0.009939029812812805]\n","Discriminator Loss: 0.6129698418080807\n","Generator Loss: [0.9654965400695801, 0.921925961971283, 0.02178528718650341]\n","Discriminator Loss: 0.6423040133086033\n","Generator Loss: [0.8421424031257629, 0.8148744106292725, 0.01363399252295494]\n","Discriminator Loss: 0.634011425281642\n","Generator Loss: [0.8156067728996277, 0.7815393209457397, 0.017033716663718224]\n","Discriminator Loss: 0.6292259897600161\n","Generator Loss: [0.9249180555343628, 0.8947262763977051, 0.015095904469490051]\n","Discriminator Loss: 0.6414701818721369\n","Generator Loss: [1.0071582794189453, 0.988821268081665, 0.009168526157736778]\n","Discriminator Loss: 0.6437111426639603\n","Generator Loss: [0.8914003968238831, 0.8735830783843994, 0.008908647112548351]\n","Discriminator Loss: 0.6006014869053615\n","Generator Loss: [0.9383614659309387, 0.8847850561141968, 0.02678820677101612]\n","Discriminator Loss: 0.5983567422881606\n","Generator Loss: [0.9962310194969177, 0.9467852711677551, 0.024722883477807045]\n","Discriminator Loss: 0.6275084119697567\n","Generator Loss: [1.0493834018707275, 1.0183254480361938, 0.015528996475040913]\n","Discriminator Loss: 0.6244592563598417\n","Generator Loss: [1.0102485418319702, 0.952269434928894, 0.028989577665925026]\n","Discriminator Loss: 0.6624265892314725\n","Generator Loss: [1.000686526298523, 0.9741036295890808, 0.013291427865624428]\n","Discriminator Loss: 0.6476708578993566\n","Generator Loss: [0.9287247657775879, 0.8567017316818237, 0.03601152077317238]\n","Discriminator Loss: 0.5983207444660366\n","Generator Loss: [0.909550666809082, 0.8709240555763245, 0.01931331306695938]\n","Discriminator Loss: 0.6146371519316745\n","Generator Loss: [0.9117985367774963, 0.8891323804855347, 0.011333081871271133]\n","Discriminator Loss: 0.6463961582703632\n","Generator Loss: [0.9568904638290405, 0.9271652698516846, 0.014862582087516785]\n","Discriminator Loss: 0.6497303725627717\n","Generator Loss: [0.9474095702171326, 0.928933322429657, 0.009238135069608688]\n","Discriminator Loss: 0.6156051309226314\n","Generator Loss: [1.0200780630111694, 0.9699031710624695, 0.02508745715022087]\n","Discriminator Loss: 0.6482618312547856\n","Generator Loss: [0.8993814587593079, 0.8789438009262085, 0.010218826122581959]\n","Discriminator Loss: 0.698912394218496\n","Generator Loss: [0.8903771638870239, 0.8684784173965454, 0.010949361138045788]\n","Discriminator Loss: 0.6212308598187519\n","Generator Loss: [0.9779201745986938, 0.9263339638710022, 0.025793112814426422]\n","Discriminator Loss: 0.6789483558386564\n","Generator Loss: [1.135350227355957, 1.1057400703430176, 0.014805056154727936]\n","Discriminator Loss: 0.6234343401447404\n","Generator Loss: [1.3956438302993774, 1.3741002082824707, 0.010771790519356728]\n","Discriminator Loss: 0.6731570561387343\n","Generator Loss: [1.2044392824172974, 1.1877442598342896, 0.008347518742084503]\n","Discriminator Loss: 0.6683491156436503\n","Generator Loss: [1.1773895025253296, 1.153794288635254, 0.01179759856313467]\n","Discriminator Loss: 0.6425845306366682\n","Generator Loss: [1.8636268377304077, 1.8413660526275635, 0.0111303785815835]\n","Discriminator Loss: 0.6030498668988002\n","Generator Loss: [1.4790644645690918, 1.4600346088409424, 0.009514919482171535]\n","Discriminator Loss: 0.6526813009986654\n","Generator Loss: [1.8118513822555542, 1.7928155660629272, 0.009517887607216835]\n","Discriminator Loss: 0.627722972050833\n","Generator Loss: [0.9541285037994385, 0.9361985921859741, 0.008964959532022476]\n","Discriminator Loss: 0.691237504070159\n","Generator Loss: [1.7166247367858887, 1.701541781425476, 0.00754146184772253]\n","Discriminator Loss: 0.6569810180517379\n","Generator Loss: [1.1190590858459473, 1.1038919687271118, 0.007583570200949907]\n","Discriminator Loss: 0.6404953279125039\n","Generator Loss: [1.186594843864441, 1.1684646606445312, 0.009065109305083752]\n","Discriminator Loss: 0.6175077861989848\n","Generator Loss: [1.204005479812622, 1.161135196685791, 0.021435171365737915]\n","Discriminator Loss: 0.6298108361588675\n","Generator Loss: [1.0989948511123657, 1.0728200674057007, 0.013087375089526176]\n","Discriminator Loss: 0.6416062765420065\n","Generator Loss: [1.0981993675231934, 1.036055564880371, 0.031071875244379044]\n","Discriminator Loss: 0.6538978619209956\n","Generator Loss: [0.9393963813781738, 0.9211798906326294, 0.00910824816673994]\n","Discriminator Loss: 0.631661112573056\n","Generator Loss: [1.0050352811813354, 0.9775984287261963, 0.013718447647988796]\n","Discriminator Loss: 0.6511525513487868\n","Generator Loss: [0.9354393482208252, 0.8762927055358887, 0.029573310166597366]\n","Discriminator Loss: 0.6229588464775588\n","Generator Loss: [0.9810197353363037, 0.9602174162864685, 0.010401161387562752]\n","Discriminator Loss: 0.6303176608234935\n","Generator Loss: [1.0485237836837769, 0.9880362153053284, 0.03024376928806305]\n","Discriminator Loss: 0.6076224730059039\n","Generator Loss: [0.9968969821929932, 0.9653636813163757, 0.015766657888889313]\n","Discriminator Loss: 0.5947506747033913\n","Generator Loss: [1.0115325450897217, 0.9729368686676025, 0.019297868013381958]\n","Discriminator Loss: 0.6298681010230212\n","Generator Loss: [0.8688725233078003, 0.8533201217651367, 0.007776211015880108]\n","Discriminator Loss: 0.6113453853176907\n","Generator Loss: [0.9127029776573181, 0.8564050197601318, 0.028148988261818886]\n","Discriminator Loss: 0.6664395624393364\n","Generator Loss: [0.866466760635376, 0.840652585029602, 0.012907098978757858]\n","Discriminator Loss: 0.6216892462398391\n","Generator Loss: [1.0957517623901367, 1.0658210515975952, 0.014965347945690155]\n","Discriminator Loss: 0.6183030548854731\n","Generator Loss: [1.0944583415985107, 1.0543146133422852, 0.02007186785340309]\n","Discriminator Loss: 0.6482162799948128\n","Generator Loss: [1.00332772731781, 0.972277820110321, 0.015524961985647678]\n","Discriminator Loss: 0.789437140658265\n","Generator Loss: [0.869019091129303, 0.8486324548721313, 0.010193318128585815]\n","Discriminator Loss: 0.6182818114757538\n","Generator Loss: [1.925244927406311, 1.8805887699127197, 0.022328095510601997]\n","Discriminator Loss: 2.755372077226639\n","Generator Loss: [0.9064921736717224, 0.8876815438270569, 0.009405309334397316]\n","Discriminator Loss: 0.9553944617509842\n","Generator Loss: [27.81183433532715, 27.7152156829834, 0.048309143632650375]\n","Discriminator Loss: 2.246098560281098\n","Generator Loss: [5.268202304840088, 0.3615706264972687, 2.4533157348632812]\n","Discriminator Loss: 1.288011210039258\n","Generator Loss: [2.3749423027038574, 2.3320980072021484, 0.02142217382788658]\n","Discriminator Loss: 0.8888330925256014\n","Generator Loss: [0.4790008068084717, 0.40980660915374756, 0.03459710627794266]\n","Discriminator Loss: 0.8186681873630732\n","Generator Loss: [30.203027725219727, 30.143232345581055, 0.02989768795669079]\n","Discriminator Loss: 0.8550994154065847\n","Generator Loss: [11.436293601989746, 11.227619171142578, 0.10433740168809891]\n","Discriminator Loss: 0.7979628343891818\n","Generator Loss: [6.812670707702637, 6.668702125549316, 0.0719841718673706]\n","Discriminator Loss: 0.7946019473019987\n","Generator Loss: [2.2765326499938965, 2.1574482917785645, 0.05954219400882721]\n","Discriminator Loss: 0.7349405487548211\n","Generator Loss: [3.4001846313476562, 3.297037363052368, 0.05157363787293434]\n","Discriminator Loss: 0.7909456757188309\n","Generator Loss: [35.82832336425781, 35.734954833984375, 0.04668436199426651]\n","Discriminator Loss: 0.8337502884678543\n","Generator Loss: [14.06209945678711, 13.968328475952148, 0.04688534140586853]\n","Discriminator Loss: 0.6457247785583604\n","Generator Loss: [89.82154846191406, 84.9524917602539, 2.4345295429229736]\n","Discriminator Loss: 0.6632620953023434\n","Generator Loss: [85.78327941894531, 85.43243408203125, 0.17542403936386108]\n","Discriminator Loss: 0.6978891351900529\n","Generator Loss: [52.220542907714844, 51.956912994384766, 0.13181498646736145]\n","Discriminator Loss: 0.8723091781139374\n","Generator Loss: [103.9879150390625, 103.77438354492188, 0.10676727443933487]\n","Discriminator Loss: 0.9655067808926105\n","Generator Loss: [1.8956233263015747, 1.6632869243621826, 0.11616820842027664]\n","Discriminator Loss: 1.1295631006360054\n","Generator Loss: [24.11717987060547, 23.90448760986328, 0.10634639859199524]\n","Discriminator Loss: 1.013623581151478\n","Generator Loss: [2.462916374206543, 2.2566614151000977, 0.10312746465206146]\n","Discriminator Loss: 0.6122071277350187\n","Generator Loss: [13.52617359161377, 13.360126495361328, 0.08302348852157593]\n","Discriminator Loss: 0.7067587431520224\n","Generator Loss: [3.0370144844055176, 2.750171184539795, 0.1434216946363449]\n","Discriminator Loss: 0.706314392387867\n","Generator Loss: [0.6144748330116272, 0.4433633089065552, 0.08555576205253601]\n","Discriminator Loss: 0.6972688153837225\n","Generator Loss: [1.3316720724105835, 1.1776070594787598, 0.07703249156475067]\n","Discriminator Loss: 0.8010571857448667\n","Generator Loss: [0.6080945730209351, 0.4595223069190979, 0.07428613305091858]\n","Discriminator Loss: 0.678315126337111\n","Generator Loss: [0.7762974500656128, 0.5975973010063171, 0.08935006707906723]\n","Discriminator Loss: 0.7001089896075428\n","Generator Loss: [0.6635586619377136, 0.5139368772506714, 0.07481089234352112]\n","Discriminator Loss: 0.7296459393110126\n","Generator Loss: [0.6717650890350342, 0.5206618309020996, 0.07555162906646729]\n","Discriminator Loss: 0.7703965961700305\n","Generator Loss: [0.8107671141624451, 0.6760287284851074, 0.06736918538808823]\n","Discriminator Loss: 0.7293486106791534\n","Generator Loss: [0.8147280812263489, 0.6807070970535278, 0.06701049953699112]\n","Discriminator Loss: 0.6999132307246327\n","Generator Loss: [10.021686553955078, 9.886363983154297, 0.06766129285097122]\n","Discriminator Loss: 0.7474874095059931\n","Generator Loss: [1.917143702507019, 1.7836835384368896, 0.0667300820350647]\n","Discriminator Loss: 0.6281474238057854\n","Generator Loss: [2.930518627166748, 2.7991394996643066, 0.06568959355354309]\n","Discriminator Loss: 0.6623667702078819\n","Generator Loss: [2.2220466136932373, 2.094590663909912, 0.06372794508934021]\n","Discriminator Loss: 0.8450460448420927\n","Generator Loss: [0.9888426065444946, 0.8611927628517151, 0.06382492184638977]\n","Discriminator Loss: 0.6444586987054208\n","Generator Loss: [1.0003035068511963, 0.8810484409332275, 0.059627532958984375]\n","Discriminator Loss: 0.7177905924763763\n","Generator Loss: [1.5894341468811035, 1.4695041179656982, 0.05996503308415413]\n","Discriminator Loss: 0.589306702328031\n","Generator Loss: [1.3567410707473755, 1.23716402053833, 0.05978850647807121]\n","Discriminator Loss: 0.7053691144810728\n","Generator Loss: [2.2101078033447266, 2.0920352935791016, 0.05903621390461922]\n","Discriminator Loss: 0.8103136071522385\n","Generator Loss: [1.2349023818969727, 1.1156072616577148, 0.05964755639433861]\n","Discriminator Loss: 0.7574214760097675\n","Generator Loss: [1.0243568420410156, 0.9057806730270386, 0.059288058429956436]\n","Discriminator Loss: 0.6510871997452341\n","Generator Loss: [1.4534543752670288, 1.3353838920593262, 0.05903524532914162]\n","Discriminator Loss: 0.7273814413056243\n","Generator Loss: [1.2402548789978027, 1.1245307922363281, 0.05786207318305969]\n","Discriminator Loss: 0.6735815064748749\n","Generator Loss: [1.1046397686004639, 0.9874957799911499, 0.05857197567820549]\n","Discriminator Loss: 0.6696569950363482\n","Generator Loss: [1.2018264532089233, 1.08292555809021, 0.059450432658195496]\n","Discriminator Loss: 0.7180094636350987\n","Generator Loss: [0.9797354340553284, 0.8622359037399292, 0.058749761432409286]\n","Discriminator Loss: 0.6147570291213924\n","Generator Loss: [0.9999843239784241, 0.8827774524688721, 0.058603435754776]\n","Discriminator Loss: 0.6693844808178255\n","Generator Loss: [1.153790831565857, 1.039805293083191, 0.05699274688959122]\n","Discriminator Loss: 0.6399467650844599\n","Generator Loss: [1.0619826316833496, 0.9445328712463379, 0.05872487276792526]\n","Discriminator Loss: 0.7017569420277141\n","Generator Loss: [1.0097944736480713, 0.8783670663833618, 0.06571370363235474]\n","Discriminator Loss: 0.63246903505933\n","Generator Loss: [1.0196473598480225, 0.9067315459251404, 0.05645790696144104]\n","Discriminator Loss: 0.6756175228547363\n","Generator Loss: [1.0417612791061401, 0.7906022071838379, 0.12557955086231232]\n","Discriminator Loss: 0.6833335535957303\n","Generator Loss: [1.0126447677612305, 0.9004548192024231, 0.05609497055411339]\n","Discriminator Loss: 0.7598016967094736\n","Generator Loss: [1.075985312461853, 0.963882327079773, 0.05605148896574974]\n","Discriminator Loss: 0.6318146074190736\n","Generator Loss: [1.3887814283370972, 1.2780570983886719, 0.05536218360066414]\n","Discriminator Loss: 0.7844131707097404\n","Generator Loss: [1.1028558015823364, 0.9750653505325317, 0.06389520317316055]\n","Discriminator Loss: 0.7091442564269528\n","Generator Loss: [0.9885660409927368, 0.8807575702667236, 0.05390423908829689]\n","Discriminator Loss: 0.5983795942738652\n","Generator Loss: [1.155078649520874, 1.043968915939331, 0.055554844439029694]\n","Discriminator Loss: 0.6836713908996899\n","Generator Loss: [0.9825413823127747, 0.8729208111763, 0.054810281842947006]\n","Discriminator Loss: 0.6093074436612369\n","Generator Loss: [0.9988059997558594, 0.8852429986000061, 0.05678151547908783]\n","Discriminator Loss: 0.6940459487814223\n","Generator Loss: [0.8965763449668884, 0.7824638485908508, 0.0570562481880188]\n","Discriminator Loss: 0.6398432933056029\n","Generator Loss: [0.9842130541801453, 0.8737139701843262, 0.05524953827261925]\n","Discriminator Loss: 0.6206896797666559\n","Generator Loss: [1.0723806619644165, 0.9520273208618164, 0.060176678001880646]\n","Discriminator Loss: 0.7506609284318984\n","Generator Loss: [1.0290508270263672, 0.9178239107131958, 0.05561348423361778]\n","Discriminator Loss: 0.6084303967945743\n","Generator Loss: [1.3213846683502197, 0.9514752626419067, 0.1849546730518341]\n","Discriminator Loss: 0.6863243219268043\n","Generator Loss: [0.9291481971740723, 0.823459804058075, 0.05284421145915985]\n","Discriminator Loss: 0.6357028733036714\n","Generator Loss: [1.154717206954956, 1.0507744550704956, 0.051971375942230225]\n","Discriminator Loss: 0.7950496738776565\n","Generator Loss: [1.1776245832443237, 1.070713996887207, 0.053455278277397156]\n","Discriminator Loss: 0.5692743796571449\n","Generator Loss: [1.1619012355804443, 1.0374794006347656, 0.06221092492341995]\n","Discriminator Loss: 0.7698211219540099\n","Generator Loss: [1.1125085353851318, 1.0039095878601074, 0.0542994849383831]\n","Discriminator Loss: 0.6151360767180449\n","Generator Loss: [1.3025176525115967, 1.1929371356964111, 0.054790232330560684]\n","Discriminator Loss: 0.615766130746124\n","Generator Loss: [1.2320356369018555, 1.1290404796600342, 0.05149760842323303]\n","Discriminator Loss: 0.7853829432278872\n","Generator Loss: [0.998916745185852, 0.8930806517601013, 0.05291803181171417]\n","Discriminator Loss: 0.6206449686724227\n","Generator Loss: [1.0139025449752808, 0.9065139293670654, 0.053694333881139755]\n","Discriminator Loss: 0.618483416212257\n","Generator Loss: [0.9055445194244385, 0.7951952219009399, 0.05517464876174927]\n","Discriminator Loss: 0.635562268551439\n","Generator Loss: [0.9591894149780273, 0.8571446537971497, 0.05102238804101944]\n","Discriminator Loss: 0.634152632963378\n","Generator Loss: [0.8997262716293335, 0.792631983757019, 0.05354713276028633]\n","Discriminator Loss: 0.6152518204180524\n","Generator Loss: [1.0797679424285889, 0.9739565849304199, 0.05290565639734268]\n","Discriminator Loss: 0.7415563436225057\n","Generator Loss: [325.6181945800781, 0.8745889067649841, 162.37179565429688]\n","Discriminator Loss: 0.5858318429673091\n","Generator Loss: [0.9671227931976318, 0.904278039932251, 0.03142237290740013]\n","Discriminator Loss: 0.6255834512412548\n","Generator Loss: [0.9658506512641907, 0.8812894821166992, 0.04228057712316513]\n","Discriminator Loss: 0.6301860467647202\n","Generator Loss: [0.9475730061531067, 0.9203784465789795, 0.013597270473837852]\n","Discriminator Loss: 0.6343121958088886\n","Generator Loss: [1.0258644819259644, 0.9649649262428284, 0.030449766665697098]\n","Discriminator Loss: 0.7075334303081036\n","Generator Loss: [0.7970342636108398, 0.7192869186401367, 0.038873668760061264]\n","Discriminator Loss: 0.6429241806035861\n","Generator Loss: [0.9563761949539185, 0.8146687150001526, 0.07085372507572174]\n","Discriminator Loss: 0.6904024798423052\n","Generator Loss: [0.8786967992782593, 0.8496737480163574, 0.014511511661112309]\n","Discriminator Loss: 0.5954894834012521\n","Generator Loss: [1.6989660263061523, 0.9171900749206543, 0.3908880054950714]\n","Discriminator Loss: 0.6372193536462873\n","Generator Loss: [4.860491752624512, 0.9569919109344482, 1.9517499208450317]\n","Discriminator Loss: 0.6214368533401284\n","Generator Loss: [1.066061019897461, 0.8534714579582214, 0.10629476606845856]\n","Discriminator Loss: 0.6377882044762373\n","Generator Loss: [2.784285306930542, 0.8407315015792847, 0.9717769026756287]\n","Discriminator Loss: 0.6064822444677702\n","Generator Loss: [0.9739775061607361, 0.9075976610183716, 0.03318992629647255]\n","Discriminator Loss: 0.6634349541272968\n","Generator Loss: [0.9587832689285278, 0.9036093950271606, 0.02758694440126419]\n","Discriminator Loss: 0.6003962228278397\n","Generator Loss: [1.3198254108428955, 0.9213981628417969, 0.1992136538028717]\n","Discriminator Loss: 0.6097168501764827\n","Generator Loss: [1.3056511878967285, 0.9495832920074463, 0.1780339479446411]\n","Discriminator Loss: 0.641274548273941\n","Generator Loss: [1.2694282531738281, 0.8884751796722412, 0.19047652184963226]\n","Discriminator Loss: 0.6114973469084362\n","Generator Loss: [1.0143449306488037, 0.8879110813140869, 0.0632169172167778]\n","Discriminator Loss: 0.7154722660779953\n","Generator Loss: [4.183703422546387, 0.9327192306518555, 1.6254920959472656]\n","Discriminator Loss: 0.9368462012280361\n","Generator Loss: [0.6002218127250671, 0.5803937911987305, 0.00991402380168438]\n","Discriminator Loss: 0.5698106456547976\n","Generator Loss: [1.5325958728790283, 0.932792067527771, 0.29990193247795105]\n","Discriminator Loss: 0.5858810300123878\n","Generator Loss: [1.2359386682510376, 1.2116236686706543, 0.012157472781836987]\n","Discriminator Loss: 0.6496294128592126\n","Generator Loss: [1.196272373199463, 1.156262993812561, 0.02000468783080578]\n","Discriminator Loss: 0.7453269055113196\n","Generator Loss: [1.3552000522613525, 1.30195152759552, 0.02662423998117447]\n","Discriminator Loss: 0.6116771220695227\n","Generator Loss: [1.0972548723220825, 1.0656859874725342, 0.01578444615006447]\n","Discriminator Loss: 0.7161958847136702\n","Generator Loss: [1.1009299755096436, 1.0730829238891602, 0.013923521153628826]\n","Discriminator Loss: 0.6451743501966121\n","Generator Loss: [1.0829722881317139, 1.0388927459716797, 0.02203979343175888]\n","Discriminator Loss: 0.7500439840368927\n","Generator Loss: [0.8746532797813416, 0.8348374962806702, 0.01990787871181965]\n","Discriminator Loss: 0.6185984506737441\n","Generator Loss: [1.0640404224395752, 0.9634970426559448, 0.05027170479297638]\n","Discriminator Loss: 0.5835959839314455\n","Generator Loss: [1.1495428085327148, 1.1295592784881592, 0.009991751983761787]\n","Discriminator Loss: 0.7087843995541334\n","Generator Loss: [1.023826003074646, 0.9734606742858887, 0.025182677432894707]\n","Discriminator Loss: 0.6201972397611826\n","Generator Loss: [1.093800663948059, 1.0094501972198486, 0.04217521473765373]\n","Discriminator Loss: 0.7677170399983879\n","Generator Loss: [0.9824985861778259, 0.939345121383667, 0.021576741710305214]\n","Discriminator Loss: 0.6287303764256649\n","Generator Loss: [0.8553292155265808, 0.8224512934684753, 0.016438961029052734]\n","Discriminator Loss: 0.655456284614047\n","Generator Loss: [0.8577836155891418, 0.8252356052398682, 0.01627400331199169]\n","Discriminator Loss: 0.6335686976090074\n","Generator Loss: [0.9315326809883118, 0.852857232093811, 0.03933773189783096]\n","Discriminator Loss: 0.6501214390227688\n","Generator Loss: [0.8796353936195374, 0.8581428527832031, 0.010746270418167114]\n","Discriminator Loss: 0.6169543036085088\n","Generator Loss: [1.0164753198623657, 0.9883208274841309, 0.014077248051762581]\n","Discriminator Loss: 0.5761996860965155\n","Generator Loss: [1.0950149297714233, 0.9881412982940674, 0.05343681201338768]\n","Discriminator Loss: 0.6156000452829176\n","Generator Loss: [0.9982469081878662, 0.9721351861953735, 0.013055864721536636]\n","Discriminator Loss: 0.723575316122151\n","Generator Loss: [0.8961610794067383, 0.846582293510437, 0.02478940784931183]\n","Discriminator Loss: 0.6205109789443668\n","Generator Loss: [1.018797755241394, 0.9549928307533264, 0.0319024883210659]\n","Discriminator Loss: 0.7327104369178414\n","Generator Loss: [0.8963425755500793, 0.8669250011444092, 0.01470879651606083]\n","Discriminator Loss: 0.6251544340630062\n","Generator Loss: [1.0009580850601196, 0.9205646514892578, 0.04019670560956001]\n","Discriminator Loss: 0.6266864854842424\n","Generator Loss: [0.9837014079093933, 0.9613392353057861, 0.01118107233196497]\n","Discriminator Loss: 0.6520032322441693\n","Generator Loss: [1.037419319152832, 1.0004527568817139, 0.01848331093788147]\n","Discriminator Loss: 0.6688671615556814\n","Generator Loss: [1.094736099243164, 0.9409356117248535, 0.07690027356147766]\n","Discriminator Loss: 0.6749447566689923\n","Generator Loss: [0.9220660924911499, 0.8539876937866211, 0.03403919190168381]\n","Discriminator Loss: 0.643506433640141\n","Generator Loss: [2.2680468559265137, 0.895261824131012, 0.6863924860954285]\n","Discriminator Loss: 0.6019613385433331\n","Generator Loss: [1.052567481994629, 0.9758161306381226, 0.038375698029994965]\n","Discriminator Loss: 0.593181974007166\n","Generator Loss: [1.0291197299957275, 0.9628562331199646, 0.03313172981142998]\n","Discriminator Loss: 0.7302701907319715\n","Generator Loss: [0.8926491737365723, 0.8735381960868835, 0.009555480442941189]\n","Discriminator Loss: 0.6602994884597138\n","Generator Loss: [0.9616650938987732, 0.9087568521499634, 0.026454132050275803]\n","Discriminator Loss: 0.6258907201117836\n","Generator Loss: [0.9410187602043152, 0.9219392538070679, 0.009539756923913956]\n","Discriminator Loss: 0.6052750817325432\n","Generator Loss: [1.2696317434310913, 1.0018391609191895, 0.13389627635478973]\n","Discriminator Loss: 0.7196958319109399\n","Generator Loss: [0.777256190776825, 0.7419853210449219, 0.017635434865951538]\n","Discriminator Loss: 0.5962785044685006\n","Generator Loss: [0.8075748085975647, 0.7853901386260986, 0.01109232660382986]\n","Discriminator Loss: 0.6052139294333756\n","Generator Loss: [1.9263287782669067, 0.7596606016159058, 0.5833340883255005]\n","Discriminator Loss: 0.6256611079443246\n","Generator Loss: [1.1335868835449219, 0.8059169054031372, 0.16383495926856995]\n","Discriminator Loss: 0.6305285348789766\n","Generator Loss: [0.8685697913169861, 0.840681254863739, 0.013944266363978386]\n","Discriminator Loss: 0.6567407596012345\n","Generator Loss: [7.706530570983887, 0.898713231086731, 3.4039087295532227]\n","Discriminator Loss: 0.6201819332200103\n","Generator Loss: [0.9527288675308228, 0.9276294708251953, 0.01254970207810402]\n","Discriminator Loss: 0.6553789221215993\n","Generator Loss: [0.8790699243545532, 0.8458335399627686, 0.016618182882666588]\n","Discriminator Loss: 0.6760638170526363\n","Generator Loss: [0.9806843400001526, 0.838196873664856, 0.07124373316764832]\n","Discriminator Loss: 0.5811061401618645\n","Generator Loss: [1.022976040840149, 0.9263540506362915, 0.0483110211789608]\n","Discriminator Loss: 0.6555160589632578\n","Generator Loss: [0.9958887100219727, 0.9668605923652649, 0.014514068141579628]\n","Discriminator Loss: 0.6867127340810839\n","Generator Loss: [0.9789040088653564, 0.9527758359909058, 0.013064092956483364]\n","Discriminator Loss: 0.6931311876978725\n","Generator Loss: [0.7036364078521729, 0.6827998161315918, 0.010418307967483997]\n","Discriminator Loss: 0.6284167432459071\n","Generator Loss: [0.7794944643974304, 0.7494652271270752, 0.015014623291790485]\n","Discriminator Loss: 0.5953830147045664\n","Generator Loss: [0.9908229112625122, 0.9590011239051819, 0.015910906717181206]\n","Discriminator Loss: 0.6348441285081208\n","Generator Loss: [0.9028214812278748, 0.8800164461135864, 0.011402517557144165]\n","Discriminator Loss: 0.6378271151334047\n","Generator Loss: [0.8825907707214355, 0.852238655090332, 0.015176046639680862]\n","Discriminator Loss: 0.6399481756961904\n","Generator Loss: [1.131026268005371, 0.9476630687713623, 0.09168161451816559]\n","Discriminator Loss: 0.5955874813371338\n","Generator Loss: [0.9295908212661743, 0.8896962404251099, 0.019947290420532227]\n","Discriminator Loss: 0.6194643883063691\n","Generator Loss: [0.9160913228988647, 0.8866702318191528, 0.01471055205911398]\n","Discriminator Loss: 0.6275248637539335\n","Generator Loss: [0.8361901044845581, 0.8022959232330322, 0.016947075724601746]\n","Discriminator Loss: 0.6603053139988333\n","Generator Loss: [0.8412517309188843, 0.783397376537323, 0.02892717905342579]\n","Discriminator Loss: 0.5748473847197602\n","Generator Loss: [1.0031144618988037, 0.945582389831543, 0.028766024857759476]\n","Discriminator Loss: 0.6283286365141976\n","Generator Loss: [0.8893803954124451, 0.8663410544395447, 0.011519675143063068]\n","Discriminator Loss: 0.6123476289212704\n","Generator Loss: [0.9021674394607544, 0.8729351758956909, 0.014616137370467186]\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 1/10 [50:38<7:35:45, 3038.39s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Discriminator Loss: 0.6699061396066099\n","Generator Loss: [0.8560432195663452, 0.8179519772529602, 0.0190456360578537]\n","Discriminator Loss: 0.590950701807742\n","Generator Loss: [0.9400069713592529, 0.9017723202705383, 0.019117338582873344]\n","Discriminator Loss: 0.627752439075266\n","Generator Loss: [0.7928162217140198, 0.7687901258468628, 0.012013040482997894]\n","Discriminator Loss: 0.6274358600494452\n","Generator Loss: [0.8165943622589111, 0.77372807264328, 0.021433139219880104]\n","Discriminator Loss: 0.6196593944623601\n","Generator Loss: [0.936297595500946, 0.8097710013389587, 0.06326329708099365]\n","Discriminator Loss: 0.615243110791198\n","Generator Loss: [1.093414068222046, 0.8686404824256897, 0.1123867854475975]\n","Discriminator Loss: 0.6093275891107623\n","Generator Loss: [0.9347540736198425, 0.909001350402832, 0.012876355089247227]\n","Discriminator Loss: 0.676439259244944\n","Generator Loss: [0.843091607093811, 0.8010311126708984, 0.021030254662036896]\n","Discriminator Loss: 0.6088858496077592\n","Generator Loss: [1.0759495496749878, 0.9784778356552124, 0.04873586446046829]\n","Discriminator Loss: 0.6670141703216359\n","Generator Loss: [0.9412947297096252, 0.8806458115577698, 0.030324451625347137]\n","Discriminator Loss: 0.6032220742781647\n","Generator Loss: [0.9705623388290405, 0.9295368790626526, 0.020512741059064865]\n","Discriminator Loss: 0.6750323464511894\n","Generator Loss: [0.851588785648346, 0.8190836906433105, 0.0162525437772274]\n","Discriminator Loss: 0.6385834083193913\n","Generator Loss: [0.9200281500816345, 0.8580941557884216, 0.030967000871896744]\n","Discriminator Loss: 0.6213409119518474\n","Generator Loss: [0.8833919763565063, 0.8545839786529541, 0.014404000714421272]\n","Discriminator Loss: 0.5935477122548036\n","Generator Loss: [0.9914782047271729, 0.9404450058937073, 0.025516606867313385]\n","Discriminator Loss: 0.6668759626336396\n","Generator Loss: [0.8887316584587097, 0.850862443447113, 0.01893460564315319]\n","Discriminator Loss: 0.5906436045916053\n","Generator Loss: [0.9472565650939941, 0.8945803046226501, 0.02633814327418804]\n","Discriminator Loss: 0.6259585981024429\n","Generator Loss: [0.8585794568061829, 0.8376699686050415, 0.01045475248247385]\n","Discriminator Loss: 0.5923217519011814\n","Generator Loss: [0.9611876606941223, 0.9207998514175415, 0.02019389532506466]\n","Discriminator Loss: 0.5968524885902298\n","Generator Loss: [0.9820724129676819, 0.9529402256011963, 0.014566089026629925]\n","Discriminator Loss: 0.6068891712930053\n","Generator Loss: [0.9860175251960754, 0.9281024932861328, 0.028957506641745567]\n","Discriminator Loss: 0.6097050489624962\n","Generator Loss: [0.9269168376922607, 0.8959448337554932, 0.015485996380448341]\n","Discriminator Loss: 0.6248034905584063\n","Generator Loss: [0.8859437108039856, 0.8291972875595093, 0.028373198583722115]\n","Discriminator Loss: 0.5850514506746549\n","Generator Loss: [0.941495954990387, 0.9048572778701782, 0.018319327384233475]\n","Discriminator Loss: 0.6418491620570421\n","Generator Loss: [0.8945760130882263, 0.8692102432250977, 0.012682880274951458]\n","Discriminator Loss: 0.636365885118721\n","Generator Loss: [0.9265495538711548, 0.8631601333618164, 0.03169470652937889]\n","Discriminator Loss: 0.5850115626817569\n","Generator Loss: [1.0245351791381836, 0.9984147548675537, 0.013060232624411583]\n","Discriminator Loss: 0.6040761529293377\n","Generator Loss: [1.0258491039276123, 1.0039104223251343, 0.010969329625368118]\n","Discriminator Loss: 0.6001298268529354\n","Generator Loss: [1.0012075901031494, 0.9808826446533203, 0.010162481106817722]\n","Discriminator Loss: 0.6054026879137382\n","Generator Loss: [0.9535209536552429, 0.9334633946418762, 0.01002878975123167]\n","Discriminator Loss: 0.608015947858803\n","Generator Loss: [1.0337157249450684, 1.0061333179473877, 0.013791225850582123]\n","Discriminator Loss: 0.6033502118079923\n","Generator Loss: [0.971349835395813, 0.9199296832084656, 0.025710079818964005]\n","Discriminator Loss: 0.8819474009796977\n","Generator Loss: [0.7747697830200195, 0.7454210519790649, 0.014674375765025616]\n","Discriminator Loss: 0.6059362479718402\n","Generator Loss: [0.9341246485710144, 0.9133150577545166, 0.010404802858829498]\n","Discriminator Loss: 0.652370915748179\n","Generator Loss: [1.017146348953247, 0.9897855520248413, 0.013680418953299522]\n","Discriminator Loss: 0.6015218339452986\n","Generator Loss: [1.0103875398635864, 0.9763143062591553, 0.017036641016602516]\n","Discriminator Loss: 0.63189550943207\n","Generator Loss: [1.0094916820526123, 0.9576753377914429, 0.025908157229423523]\n","Discriminator Loss: 0.663681085978169\n","Generator Loss: [0.927794337272644, 0.9093177318572998, 0.009238293394446373]\n","Discriminator Loss: 0.6834305972588481\n","Generator Loss: [1.1393893957138062, 1.0660731792449951, 0.03665809705853462]\n","Discriminator Loss: 0.6458895512187155\n","Generator Loss: [0.9087470769882202, 0.885924220085144, 0.011411433108150959]\n","Discriminator Loss: 0.6474453457631171\n","Generator Loss: [0.892981231212616, 0.8625114560127258, 0.015234873630106449]\n","Discriminator Loss: 0.651811097515747\n","Generator Loss: [0.8473522663116455, 0.8125186562538147, 0.017416799440979958]\n","Discriminator Loss: 0.5973718405875843\n","Generator Loss: [0.8127184510231018, 0.7888055443763733, 0.011956444010138512]\n","Discriminator Loss: 0.7200930459075607\n","Generator Loss: [0.8904282450675964, 0.867567241191864, 0.011430507525801659]\n","Discriminator Loss: 0.6036313802469522\n","Generator Loss: [0.9320855140686035, 0.9109972715377808, 0.010544116608798504]\n","Discriminator Loss: 0.6539934334577993\n","Generator Loss: [1.0111610889434814, 0.8968111872673035, 0.057174935936927795]\n","Discriminator Loss: 0.6173511850647628\n","Generator Loss: [1.0527870655059814, 0.9989535808563232, 0.02691672183573246]\n","Discriminator Loss: 0.7979413848370314\n","Generator Loss: [0.7078887820243835, 0.6858310699462891, 0.011028842069208622]\n","Discriminator Loss: 0.5820153867825866\n","Generator Loss: [0.6955065727233887, 0.6619728803634644, 0.016766851767897606]\n","Discriminator Loss: 0.6039368564961478\n","Generator Loss: [0.7510676383972168, 0.7243322134017944, 0.013367713429033756]\n","Discriminator Loss: 0.6241077323502395\n","Generator Loss: [0.832720160484314, 0.8062649369239807, 0.013227619230747223]\n","Discriminator Loss: 0.6012788540683687\n","Generator Loss: [0.8842792510986328, 0.8640744686126709, 0.010102404281497002]\n","Discriminator Loss: 0.6297917910269462\n","Generator Loss: [0.9311789274215698, 0.8995767831802368, 0.015801068395376205]\n","Discriminator Loss: 0.6157098151161335\n","Generator Loss: [0.8773117065429688, 0.8520320653915405, 0.012639818713068962]\n","Discriminator Loss: 0.668539305916056\n","Generator Loss: [0.8493316769599915, 0.8060304522514343, 0.021650614216923714]\n","Discriminator Loss: 0.6128171297023073\n","Generator Loss: [0.9637486338615417, 0.8709050416946411, 0.04642179608345032]\n","Discriminator Loss: 0.6225971527746879\n","Generator Loss: [1.0701481103897095, 1.0164440870285034, 0.02685200609266758]\n","Discriminator Loss: 0.6964957921300083\n","Generator Loss: [0.8335865139961243, 0.8116427659988403, 0.010971861891448498]\n","Discriminator Loss: 0.624234126182273\n","Generator Loss: [0.8748446702957153, 0.8519089221954346, 0.011467883363366127]\n","Discriminator Loss: 0.6046281706367154\n","Generator Loss: [0.9611550569534302, 0.8884140849113464, 0.03637048974633217]\n","Discriminator Loss: 0.5962440787698142\n","Generator Loss: [0.9157914519309998, 0.8867639899253845, 0.014513744041323662]\n","Discriminator Loss: 0.6636176275787875\n","Generator Loss: [0.8144453167915344, 0.7944961190223694, 0.009974591434001923]\n","Discriminator Loss: 0.6148357178608421\n","Generator Loss: [0.7981822490692139, 0.776692807674408, 0.010744721628725529]\n","Discriminator Loss: 0.5862563413393218\n","Generator Loss: [0.9139459729194641, 0.8520463705062866, 0.03094979003071785]\n","Discriminator Loss: 0.5852998594637029\n","Generator Loss: [0.9236144423484802, 0.8912631273269653, 0.016175666823983192]\n","Discriminator Loss: 0.5849233210610691\n","Generator Loss: [0.8876409530639648, 0.8560706377029419, 0.01578514836728573]\n","Discriminator Loss: 0.6773879341199063\n","Generator Loss: [0.9036902785301208, 0.8678260445594788, 0.01793213002383709]\n","Discriminator Loss: 0.5689882741426118\n","Generator Loss: [0.8997640013694763, 0.8776860237121582, 0.011038996279239655]\n","Discriminator Loss: 0.7021583557216218\n","Generator Loss: [0.8659395575523376, 0.835641622543335, 0.01514896284788847]\n","Discriminator Loss: 0.6091550138080493\n","Generator Loss: [0.8259836435317993, 0.8032779693603516, 0.011352826841175556]\n","Discriminator Loss: 0.5903888754692161\n","Generator Loss: [0.84696364402771, 0.8288741707801819, 0.009044722653925419]\n","Discriminator Loss: 0.5958648383675609\n","Generator Loss: [0.7736763954162598, 0.7536266446113586, 0.010024873539805412]\n","Discriminator Loss: 0.629249462072039\n","Generator Loss: [1.1223047971725464, 1.090038537979126, 0.016133124008774757]\n","Discriminator Loss: 0.6232606420380762\n","Generator Loss: [1.137856125831604, 1.0776034593582153, 0.03012632578611374]\n","Discriminator Loss: 0.6328259267211251\n","Generator Loss: [1.1711684465408325, 1.1087678670883179, 0.031200289726257324]\n","Discriminator Loss: 0.7387035983119858\n","Generator Loss: [0.8505511283874512, 0.8318713307380676, 0.009339910931885242]\n","Discriminator Loss: 0.5964457490481436\n","Generator Loss: [0.8769721388816833, 0.8576993942260742, 0.009636358357965946]\n","Discriminator Loss: 0.5810875431488967\n","Generator Loss: [0.9103772044181824, 0.8878965377807617, 0.011240321211516857]\n","Discriminator Loss: 0.630762998451246\n","Generator Loss: [0.873205840587616, 0.8466001749038696, 0.013302827253937721]\n","Discriminator Loss: 0.686381184932543\n","Generator Loss: [0.9344045519828796, 0.8699885606765747, 0.03220799192786217]\n","Discriminator Loss: 0.6012388153467327\n","Generator Loss: [0.9377541542053223, 0.9104436039924622, 0.013655267655849457]\n","Discriminator Loss: 0.5826082611456513\n","Generator Loss: [0.9525769352912903, 0.9245221614837646, 0.014027383178472519]\n","Discriminator Loss: 0.6012362664623652\n","Generator Loss: [0.9845147728919983, 0.9319963455200195, 0.026259208098053932]\n","Discriminator Loss: 0.7005434176535346\n","Generator Loss: [0.8401410579681396, 0.7862222194671631, 0.026959411799907684]\n","Discriminator Loss: 0.652587944699917\n","Generator Loss: [0.8382182717323303, 0.7850269079208374, 0.02659567818045616]\n","Discriminator Loss: 0.6382139805646148\n","Generator Loss: [1.1132440567016602, 0.9061014652252197, 0.10357126593589783]\n","Discriminator Loss: 0.6776102865114808\n","Generator Loss: [0.809131920337677, 0.7857263088226318, 0.011702807620167732]\n","Discriminator Loss: 0.6383171096094884\n","Generator Loss: [0.8463085293769836, 0.8004780411720276, 0.02291523851454258]\n","Discriminator Loss: 0.6002567284740508\n","Generator Loss: [0.9710806608200073, 0.9507772922515869, 0.010151684284210205]\n","Discriminator Loss: 0.6788743025099393\n","Generator Loss: [0.8258652091026306, 0.8062195181846619, 0.009822845458984375]\n","Discriminator Loss: 0.5926640060642967\n","Generator Loss: [0.9367529153823853, 0.8481801748275757, 0.04428638517856598]\n","Discriminator Loss: 0.6239796735608252\n","Generator Loss: [0.8302620053291321, 0.8080086708068848, 0.011126663535833359]\n","Discriminator Loss: 0.5851172887487337\n","Generator Loss: [0.8769682049751282, 0.840050458908081, 0.018458880484104156]\n","Discriminator Loss: 0.628916411777027\n","Generator Loss: [0.8608746528625488, 0.8091976642608643, 0.02583850361406803]\n","Discriminator Loss: 0.5796427084860625\n","Generator Loss: [0.9242548942565918, 0.8839237689971924, 0.020165549591183662]\n","Discriminator Loss: 0.6118037214328069\n","Generator Loss: [0.898550271987915, 0.8719288110733032, 0.013310741633176804]\n","Discriminator Loss: 0.5828483388759196\n","Generator Loss: [0.905439555644989, 0.876082181930542, 0.014678684063255787]\n","Discriminator Loss: 0.6352066668914631\n","Generator Loss: [0.91140216588974, 0.874531626701355, 0.01843528263270855]\n","Discriminator Loss: 0.6328688116627745\n","Generator Loss: [0.8090633153915405, 0.7668308615684509, 0.02111622504889965]\n","Discriminator Loss: 0.5753995293343905\n","Generator Loss: [0.9240763187408447, 0.8733558654785156, 0.025360221043229103]\n","Discriminator Loss: 0.6269327358459122\n","Generator Loss: [0.828579843044281, 0.809609055519104, 0.009485404007136822]\n","Discriminator Loss: 0.6029203229409177\n","Generator Loss: [0.8172252178192139, 0.7960208654403687, 0.010602178983390331]\n","Discriminator Loss: 0.6250302719563479\n","Generator Loss: [0.8426816463470459, 0.8169158697128296, 0.012882895767688751]\n","Discriminator Loss: 0.5990437506698072\n","Generator Loss: [0.9246922135353088, 0.8693891167640686, 0.02765154466032982]\n","Discriminator Loss: 0.601819924719166\n","Generator Loss: [0.8938637971878052, 0.8675921559333801, 0.013135815970599651]\n","Discriminator Loss: 0.585548724156979\n","Generator Loss: [0.9082975387573242, 0.8876745700836182, 0.010311481542885303]\n","Discriminator Loss: 0.6061055989994202\n","Generator Loss: [0.8949440121650696, 0.8734785318374634, 0.010732733644545078]\n","Discriminator Loss: 0.5897433949576225\n","Generator Loss: [0.9117329716682434, 0.8878213167190552, 0.011955833062529564]\n","Discriminator Loss: 0.6466229601937812\n","Generator Loss: [0.802821159362793, 0.7597211003303528, 0.021550022065639496]\n","Discriminator Loss: 0.6192618156783283\n","Generator Loss: [0.7953857183456421, 0.7719842195510864, 0.011700746603310108]\n","Discriminator Loss: 0.5539439058629796\n","Generator Loss: [0.8476079106330872, 0.8258029222488403, 0.010902492329478264]\n","Discriminator Loss: 0.5907006788474973\n","Generator Loss: [0.8538107872009277, 0.8275447487831116, 0.01313302293419838]\n","Discriminator Loss: 0.6175682645261986\n","Generator Loss: [0.7836390137672424, 0.7529357671737671, 0.015351631678640842]\n","Discriminator Loss: 0.68077086948324\n","Generator Loss: [0.8725061416625977, 0.8466441035270691, 0.012931028380990028]\n","Discriminator Loss: 0.5806463651824743\n","Generator Loss: [0.8958654403686523, 0.8726791739463806, 0.011593126691877842]\n","Discriminator Loss: 0.6142151225358248\n","Generator Loss: [0.857414960861206, 0.8218203783035278, 0.01779729686677456]\n","Discriminator Loss: 0.6033601829549298\n","Generator Loss: [0.8809117674827576, 0.850249707698822, 0.015331029891967773]\n","Discriminator Loss: 0.6191798032523366\n","Generator Loss: [0.8415132761001587, 0.8171887993812561, 0.01216222532093525]\n","Discriminator Loss: 0.6109119476168416\n","Generator Loss: [0.8029419779777527, 0.7817708253860474, 0.010585564188659191]\n","Discriminator Loss: 0.5742775700346101\n","Generator Loss: [1.1801998615264893, 0.8472710847854614, 0.16646438837051392]\n","Discriminator Loss: 0.5865608119929675\n","Generator Loss: [0.8790298700332642, 0.8536841869354248, 0.012672838754951954]\n","Discriminator Loss: 0.6048742093262263\n","Generator Loss: [0.8196782469749451, 0.7818671464920044, 0.01890553906559944]\n","Discriminator Loss: 0.5842705618706532\n","Generator Loss: [1.5542315244674683, 0.7871564626693726, 0.38353753089904785]\n","Discriminator Loss: 0.8387966128793778\n","Generator Loss: [0.7166693806648254, 0.6813969612121582, 0.01763620600104332]\n","Discriminator Loss: 0.7727248402079567\n","Generator Loss: [0.8624837398529053, 0.8367328643798828, 0.012875447049736977]\n","Discriminator Loss: 0.8224134218180552\n","Generator Loss: [0.7637190222740173, 0.724773645401001, 0.019472695887088776]\n","Discriminator Loss: 0.8073301791737322\n","Generator Loss: [0.7441962361335754, 0.7248207330703735, 0.009687752462923527]\n","Discriminator Loss: 0.7201528940349817\n","Generator Loss: [0.7122138142585754, 0.6898033618927002, 0.011205220595002174]\n","Discriminator Loss: 0.7838062553782947\n","Generator Loss: [0.8778210282325745, 0.8499209880828857, 0.013950010761618614]\n","Discriminator Loss: 0.7432830706238747\n","Generator Loss: [0.8739985227584839, 0.8370223045349121, 0.018488114699721336]\n","Discriminator Loss: 0.6244539584149607\n","Generator Loss: [0.8310511708259583, 0.7968423366546631, 0.01710442826151848]\n","Discriminator Loss: 0.6189229101146339\n","Generator Loss: [0.9584823846817017, 0.9393512606620789, 0.00956556387245655]\n","Discriminator Loss: 0.5878954628133215\n","Generator Loss: [1.0284680128097534, 0.895465075969696, 0.06650145351886749]\n","Discriminator Loss: 0.6247069884848315\n","Generator Loss: [0.9660406112670898, 0.8936184644699097, 0.03621106594800949]\n","Discriminator Loss: 0.6458709831931628\n","Generator Loss: [0.7832077145576477, 0.7541137337684631, 0.01454697735607624]\n","Discriminator Loss: 0.6350263436179375\n","Generator Loss: [0.8213118314743042, 0.7935935854911804, 0.013859114609658718]\n","Discriminator Loss: 0.724055966391461\n","Generator Loss: [0.8465381860733032, 0.8212394714355469, 0.012649348005652428]\n","Discriminator Loss: 0.6374307519872673\n","Generator Loss: [0.8125145435333252, 0.7772429585456848, 0.01763579063117504]\n","Discriminator Loss: 0.6118562435731292\n","Generator Loss: [0.9570677876472473, 0.9394311904907227, 0.008818293921649456]\n","Discriminator Loss: 0.6156129473965848\n","Generator Loss: [0.931373119354248, 0.8946114182472229, 0.018380852416157722]\n","Discriminator Loss: 0.6509057758667041\n","Generator Loss: [0.783735454082489, 0.7133196592330933, 0.035207897424697876]\n","Discriminator Loss: 0.6080018279171782\n","Generator Loss: [0.9600846767425537, 0.9039396643638611, 0.028072519227862358]\n","Discriminator Loss: 0.6116011422127485\n","Generator Loss: [0.842786967754364, 0.820315957069397, 0.011235492303967476]\n","Discriminator Loss: 0.6264820999640506\n","Generator Loss: [0.9063248634338379, 0.8634117245674133, 0.02145656943321228]\n","Discriminator Loss: 0.6114836796768941\n","Generator Loss: [0.8679111003875732, 0.8469806909561157, 0.010465198196470737]\n","Discriminator Loss: 0.616950191630167\n","Generator Loss: [0.8154284954071045, 0.7871648073196411, 0.014131855219602585]\n","Discriminator Loss: 0.5832356516402797\n","Generator Loss: [0.8844334483146667, 0.8567808270454407, 0.013826305978000164]\n","Discriminator Loss: 0.6346283900202252\n","Generator Loss: [0.774350106716156, 0.7531876564025879, 0.010581213980913162]\n","Discriminator Loss: 0.640180000293185\n","Generator Loss: [0.724737823009491, 0.6993303298950195, 0.012703755870461464]\n","Discriminator Loss: 0.6479758174391463\n","Generator Loss: [0.7808788418769836, 0.7595329284667969, 0.01067295204848051]\n","Discriminator Loss: 0.5769443432654953\n","Generator Loss: [0.9089520573616028, 0.8787925243377686, 0.015079756267368793]\n","Discriminator Loss: 0.6193114776251605\n","Generator Loss: [0.8390882015228271, 0.81825190782547, 0.01041814312338829]\n","Discriminator Loss: 0.6183885810969514\n","Generator Loss: [0.912492036819458, 0.8744131326675415, 0.019039437174797058]\n","Discriminator Loss: 0.5871783078255248\n","Generator Loss: [0.8170428276062012, 0.7911494970321655, 0.012946659699082375]\n","Discriminator Loss: 0.6343602827109862\n","Generator Loss: [0.7515885829925537, 0.7123326659202576, 0.019627943634986877]\n","Discriminator Loss: 0.6071144150118926\n","Generator Loss: [0.830947756767273, 0.6820679903030396, 0.0744398832321167]\n","Discriminator Loss: 0.5763588021800388\n","Generator Loss: [0.85024493932724, 0.7953680753707886, 0.027438441291451454]\n","Discriminator Loss: 0.6030185007548425\n","Generator Loss: [0.8619956374168396, 0.8343861103057861, 0.013804765418171883]\n","Discriminator Loss: 0.5929325095494278\n","Generator Loss: [0.8408970832824707, 0.8204591870307922, 0.010218942537903786]\n","Discriminator Loss: 0.6131002277543303\n","Generator Loss: [0.8106178641319275, 0.7931926846504211, 0.008712583221495152]\n","Discriminator Loss: 0.6160157206177246\n","Generator Loss: [0.9670077562332153, 0.7724243998527527, 0.09729169309139252]\n","Discriminator Loss: 0.6091383800230687\n","Generator Loss: [0.8281792402267456, 0.8055449724197388, 0.011317124590277672]\n","Discriminator Loss: 0.604659102667938\n","Generator Loss: [0.8911550045013428, 0.8715904951095581, 0.009782252833247185]\n","Discriminator Loss: 0.6267378250340698\n","Generator Loss: [0.7837144136428833, 0.7598385810852051, 0.011937919072806835]\n","Discriminator Loss: 0.6065503984573297\n","Generator Loss: [0.8785036206245422, 0.8555002808570862, 0.011501677334308624]\n","Discriminator Loss: 0.661696684663184\n","Generator Loss: [0.8435473442077637, 0.822733998298645, 0.010406658984720707]\n","Discriminator Loss: 0.6632091432402376\n","Generator Loss: [0.8139064311981201, 0.7903544306755066, 0.011775990948081017]\n","Discriminator Loss: 0.5853452311421279\n","Generator Loss: [0.7807697057723999, 0.7596416473388672, 0.010564015246927738]\n","Discriminator Loss: 0.5713353181781713\n","Generator Loss: [0.8456922769546509, 0.7974156141281128, 0.02413833513855934]\n","Discriminator Loss: 0.6427247048268327\n","Generator Loss: [0.7430530786514282, 0.6913276314735413, 0.025862716138362885]\n","Discriminator Loss: 0.6124361261026934\n","Generator Loss: [0.7100958824157715, 0.6480801105499268, 0.03100789710879326]\n","Discriminator Loss: 0.6483147108228877\n","Generator Loss: [0.806136965751648, 0.7807329893112183, 0.012701991014182568]\n","Discriminator Loss: 0.5748268204624765\n","Generator Loss: [0.7301834225654602, 0.7097384929656982, 0.010222451761364937]\n","Discriminator Loss: 0.5877086282271193\n","Generator Loss: [0.7748630046844482, 0.7477293014526367, 0.013566852547228336]\n","Discriminator Loss: 0.5914527652930701\n","Generator Loss: [0.7943854928016663, 0.7689508199691772, 0.01271732896566391]\n","Discriminator Loss: 0.5871882545397966\n","Generator Loss: [0.8023924231529236, 0.7812190055847168, 0.01058671623468399]\n","Discriminator Loss: 0.6191941461001989\n","Generator Loss: [0.7433176636695862, 0.7241659164428711, 0.00957588292658329]\n","Discriminator Loss: 0.6031096909573535\n","Generator Loss: [0.8624803423881531, 0.8383016586303711, 0.012089328840374947]\n","Discriminator Loss: 0.652731117712392\n","Generator Loss: [0.7594736218452454, 0.7088552713394165, 0.025309162214398384]\n","Discriminator Loss: 0.608731698259362\n","Generator Loss: [0.8414036631584167, 0.8138311505317688, 0.013786247000098228]\n","Discriminator Loss: 0.6276237434794893\n","Generator Loss: [0.8302158713340759, 0.7737178802490234, 0.028248993679881096]\n","Discriminator Loss: 0.6672682629432529\n","Generator Loss: [1.2416019439697266, 0.8301988840103149, 0.20570150017738342]\n","Discriminator Loss: 0.595579491026001\n","Generator Loss: [1.0550005435943604, 0.8707044124603271, 0.09214808791875839]\n","Discriminator Loss: 0.6137377587874653\n","Generator Loss: [0.8686505556106567, 0.8422844409942627, 0.01318305917084217]\n","Discriminator Loss: 0.5775803584838286\n","Generator Loss: [0.8122777342796326, 0.7875005006790161, 0.012388614937663078]\n","Discriminator Loss: 0.5902443713130197\n","Generator Loss: [0.8668256998062134, 0.8472948670387268, 0.009765421971678734]\n","Discriminator Loss: 0.6740717533102725\n","Generator Loss: [0.7883985042572021, 0.7491209506988525, 0.019638771191239357]\n","Discriminator Loss: 0.5728491854533786\n","Generator Loss: [0.8016074299812317, 0.7774609327316284, 0.012073254212737083]\n","Discriminator Loss: 0.6229632911708904\n","Generator Loss: [0.8954182863235474, 0.8025046586990356, 0.04645681381225586]\n","Discriminator Loss: 0.6480999854393303\n","Generator Loss: [0.7769822478294373, 0.759141206741333, 0.00892051961272955]\n","Discriminator Loss: 0.5739337198901922\n","Generator Loss: [0.9400436878204346, 0.9199779033660889, 0.010032888501882553]\n","Discriminator Loss: 0.6692143241234589\n","Generator Loss: [0.8077521324157715, 0.7803613543510437, 0.013695391826331615]\n","Discriminator Loss: 0.6149752251803875\n","Generator Loss: [0.9153976440429688, 0.8949103951454163, 0.010243615135550499]\n","Discriminator Loss: 0.625634662079392\n","Generator Loss: [0.8892765045166016, 0.8535398244857788, 0.01786833070218563]\n","Discriminator Loss: 0.6172644567777752\n","Generator Loss: [0.8476526737213135, 0.8201820254325867, 0.013735324144363403]\n","Discriminator Loss: 0.6153754832339473\n","Generator Loss: [0.8809440732002258, 0.7778068780899048, 0.051568593829870224]\n","Discriminator Loss: 0.6743802894779947\n","Generator Loss: [0.8399078845977783, 0.8190531730651855, 0.010427359491586685]\n","Discriminator Loss: 0.598523907159688\n","Generator Loss: [0.8505807518959045, 0.8282883763313293, 0.011146187782287598]\n","Discriminator Loss: 0.5825619309325702\n","Generator Loss: [0.9012356400489807, 0.8773248791694641, 0.011955386027693748]\n","Discriminator Loss: 0.6309605067726807\n","Generator Loss: [0.7690484523773193, 0.7163615226745605, 0.02634347416460514]\n","Discriminator Loss: 0.6128912891726941\n","Generator Loss: [0.7390984296798706, 0.7189007997512817, 0.010098825208842754]\n","Discriminator Loss: 0.581936108705122\n","Generator Loss: [0.8188247084617615, 0.7964949607849121, 0.011164872907102108]\n","Discriminator Loss: 0.6499218477401882\n","Generator Loss: [0.7980287671089172, 0.7800369262695312, 0.008995921351015568]\n","Discriminator Loss: 0.5870337367668981\n","Generator Loss: [0.9436917304992676, 0.9196957349777222, 0.01199801079928875]\n","Discriminator Loss: 0.623915750416927\n","Generator Loss: [0.8652440309524536, 0.8408117294311523, 0.012216143310070038]\n","Discriminator Loss: 0.6405920468969271\n","Generator Loss: [0.8015048503875732, 0.7800953388214111, 0.010704759508371353]\n","Discriminator Loss: 0.6484376721637091\n","Generator Loss: [0.8985547423362732, 0.8802847862243652, 0.009134968742728233]\n","Discriminator Loss: 0.5948272378882393\n","Generator Loss: [1.0318318605422974, 1.0134837627410889, 0.009174049831926823]\n","Discriminator Loss: 0.6108555544778937\n","Generator Loss: [1.105189561843872, 1.0783460140228271, 0.01342178788036108]\n","Discriminator Loss: 0.6943934226583224\n","Generator Loss: [0.821013867855072, 0.765338659286499, 0.027837608009576797]\n","Discriminator Loss: 0.6913262269226834\n","Generator Loss: [0.7941064834594727, 0.7678876519203186, 0.013109422288835049]\n","Discriminator Loss: 0.6026363968267106\n","Generator Loss: [0.8475714325904846, 0.8186562061309814, 0.014457623474299908]\n","Discriminator Loss: 0.6135018535424024\n","Generator Loss: [0.8447900414466858, 0.81882244348526, 0.012983785942196846]\n","Discriminator Loss: 0.591620443505235\n","Generator Loss: [0.8457087278366089, 0.8187973499298096, 0.013455696403980255]\n","Discriminator Loss: 0.6219763100962155\n","Generator Loss: [0.8192879557609558, 0.8001143336296082, 0.009586808271706104]\n","Discriminator Loss: 0.5786906092835125\n","Generator Loss: [0.911693811416626, 0.8889040946960449, 0.011394866742193699]\n","Discriminator Loss: 0.600439958536299\n","Generator Loss: [0.9305808544158936, 0.8571836948394775, 0.036698564887046814]\n","Discriminator Loss: 0.6307216729328502\n","Generator Loss: [0.8112780451774597, 0.7817888259887695, 0.014744620770215988]\n","Discriminator Loss: 0.5808250361587852\n","Generator Loss: [0.7902283668518066, 0.7640681266784668, 0.013080116361379623]\n","Discriminator Loss: 0.5756037347891834\n","Generator Loss: [0.8690573573112488, 0.8390247821807861, 0.015016297809779644]\n","Discriminator Loss: 0.6019176105473889\n","Generator Loss: [0.8504053354263306, 0.8003522157669067, 0.02502657100558281]\n","Discriminator Loss: 0.597732679976616\n","Generator Loss: [0.8692477345466614, 0.8475919961929321, 0.010827858000993729]\n","Discriminator Loss: 0.6486552052374464\n","Generator Loss: [0.7713042497634888, 0.7498963475227356, 0.010703950189054012]\n","Discriminator Loss: 0.586280909483321\n","Generator Loss: [0.9546735286712646, 0.9294004440307617, 0.012636538594961166]\n","Discriminator Loss: 0.5777997067780234\n","Generator Loss: [1.0543222427368164, 1.0208818912506104, 0.01672019623219967]\n","Discriminator Loss: 0.624898481532\n","Generator Loss: [0.9654004573822021, 0.9460668563842773, 0.009666810743510723]\n","Discriminator Loss: 0.6008054961421294\n","Generator Loss: [0.9940633177757263, 0.9468823671340942, 0.023590482771396637]\n","Discriminator Loss: 0.5878182016895153\n","Generator Loss: [0.93013596534729, 0.8963301181793213, 0.01690291427075863]\n","Discriminator Loss: 0.5973565754538868\n","Generator Loss: [1.0124597549438477, 0.9839035868644714, 0.014278100803494453]\n","Discriminator Loss: 0.6177688912139274\n","Generator Loss: [0.8497803211212158, 0.8267186880111694, 0.011530802585184574]\n","Discriminator Loss: 0.5937150067911716\n","Generator Loss: [0.7902814745903015, 0.7608642578125, 0.014708607457578182]\n","Discriminator Loss: 0.6136519348656293\n","Generator Loss: [0.864575207233429, 0.8187145590782166, 0.022930337116122246]\n","Discriminator Loss: 0.5561118854238885\n","Generator Loss: [0.920745849609375, 0.9017585515975952, 0.009493649937212467]\n","Discriminator Loss: 0.6502291746728588\n","Generator Loss: [0.8477076292037964, 0.8161611557006836, 0.015773244202136993]\n","Discriminator Loss: 0.5549193857295904\n","Generator Loss: [1.084078311920166, 1.053820252418518, 0.015129020437598228]\n","Discriminator Loss: 0.6222553819534369\n","Generator Loss: [1.0246286392211914, 1.0077872276306152, 0.008420727215707302]\n","Discriminator Loss: 0.610941199818626\n","Generator Loss: [1.044765830039978, 1.0242607593536377, 0.010252553969621658]\n","Discriminator Loss: 0.6072403052785376\n","Generator Loss: [1.0500227212905884, 1.0106221437454224, 0.019700277596712112]\n","Discriminator Loss: 0.6098613935828325\n","Generator Loss: [1.010543704032898, 0.9803131818771362, 0.015115258283913136]\n","Discriminator Loss: 0.602851761970669\n","Generator Loss: [1.0019047260284424, 0.9665372967720032, 0.017683720216155052]\n","Discriminator Loss: 0.6025681929313578\n","Generator Loss: [1.0254383087158203, 1.0036680698394775, 0.010885132476687431]\n","Discriminator Loss: 0.6478964325942798\n","Generator Loss: [0.8718512058258057, 0.8429558277130127, 0.014447682537138462]\n","Discriminator Loss: 0.5812636243063025\n","Generator Loss: [0.9743220806121826, 0.9562015533447266, 0.009060273878276348]\n","Discriminator Loss: 0.6954728812270332\n","Generator Loss: [0.8397153615951538, 0.7967889308929443, 0.02146320231258869]\n","Discriminator Loss: 0.6080525116994977\n","Generator Loss: [0.8339670300483704, 0.8155696988105774, 0.009198660030961037]\n","Discriminator Loss: 0.5873588467547961\n","Generator Loss: [0.7475398182868958, 0.7284654378890991, 0.009537179954349995]\n","Discriminator Loss: 0.6076651924449834\n","Generator Loss: [0.8580247163772583, 0.8287252187728882, 0.014649752527475357]\n","Discriminator Loss: 0.5317851962317945\n","Generator Loss: [1.0654852390289307, 1.0481219291687012, 0.008681651204824448]\n","Discriminator Loss: 0.6872846322949044\n","Generator Loss: [0.9487466812133789, 0.9263505935668945, 0.011198054999113083]\n","Discriminator Loss: 0.5680055355333025\n","Generator Loss: [0.8661617040634155, 0.8482708930969238, 0.008945410139858723]\n","Discriminator Loss: 0.6742810914292932\n","Generator Loss: [0.8743698596954346, 0.8560602068901062, 0.009154831059277058]\n","Discriminator Loss: 0.5502062055747956\n","Generator Loss: [0.8839126229286194, 0.8655819892883301, 0.009165319614112377]\n","Discriminator Loss: 0.6307590391952544\n","Generator Loss: [0.8382946848869324, 0.8173243999481201, 0.010485141538083553]\n","Discriminator Loss: 0.6040882603847422\n","Generator Loss: [0.7732855677604675, 0.7385597229003906, 0.0173629280179739]\n","Discriminator Loss: 0.7274925262026954\n","Generator Loss: [0.6936288475990295, 0.668134331703186, 0.012747254222631454]\n","Discriminator Loss: 0.6249135357211344\n","Generator Loss: [1.0056852102279663, 0.9638370275497437, 0.020924119278788567]\n","Discriminator Loss: 0.6269892053678632\n","Generator Loss: [0.7090492844581604, 0.6882326602935791, 0.010408300906419754]\n","Discriminator Loss: 0.6266899739857763\n","Generator Loss: [0.9312652349472046, 0.9012026190757751, 0.015031302347779274]\n","Discriminator Loss: 0.5888585981228971\n","Generator Loss: [0.9012700915336609, 0.8743502497673035, 0.013459914363920689]\n","Discriminator Loss: 0.5958075188827934\n","Generator Loss: [0.9197038412094116, 0.8953675627708435, 0.012168153189122677]\n","Discriminator Loss: 0.6027447210144601\n","Generator Loss: [0.9114601612091064, 0.871375560760498, 0.020042311400175095]\n","Discriminator Loss: 0.5967735105296015\n","Generator Loss: [0.8744550943374634, 0.8373182415962219, 0.018568433821201324]\n","Discriminator Loss: 0.6060421187721658\n","Generator Loss: [0.7784414887428284, 0.7368167042732239, 0.02081240527331829]\n","Discriminator Loss: 0.5924775680541643\n","Generator Loss: [0.8052688241004944, 0.784940779209137, 0.010164021514356136]\n","Discriminator Loss: 0.5793373718552175\n","Generator Loss: [0.8093108534812927, 0.7856168150901794, 0.011847026646137238]\n","Discriminator Loss: 0.5901258652447723\n","Generator Loss: [0.8197335600852966, 0.7902418971061707, 0.014745824970304966]\n","Discriminator Loss: 0.6080154480878264\n","Generator Loss: [0.8940039873123169, 0.8147831559181213, 0.039610423147678375]\n","Discriminator Loss: 0.5934024565503933\n","Generator Loss: [0.7833147048950195, 0.7547781467437744, 0.01426828932017088]\n","Discriminator Loss: 0.6167898129642708\n","Generator Loss: [0.7235715389251709, 0.7058420181274414, 0.008864748291671276]\n","Discriminator Loss: 0.5565165881998837\n","Generator Loss: [0.7503721117973328, 0.7321895956993103, 0.009091252461075783]\n","Discriminator Loss: 0.5838660676672589\n","Generator Loss: [0.7810094952583313, 0.761529266834259, 0.009740118868649006]\n","Discriminator Loss: 0.6043697572822566\n","Generator Loss: [0.7753849625587463, 0.7478978633880615, 0.01374355610460043]\n","Discriminator Loss: 0.5728260428004432\n","Generator Loss: [0.7879602909088135, 0.7725115418434143, 0.00772436847910285]\n","Discriminator Loss: 0.6360044245811878\n","Generator Loss: [0.6968826055526733, 0.6707061529159546, 0.013088220730423927]\n","Discriminator Loss: 0.6332152849790873\n","Generator Loss: [0.7573428750038147, 0.7220080494880676, 0.017667409032583237]\n","Discriminator Loss: 0.608379147706728\n","Generator Loss: [0.7410209774971008, 0.7207096815109253, 0.010155645199120045]\n","Discriminator Loss: 0.5944980810745619\n","Generator Loss: [0.8261658549308777, 0.778136670589447, 0.02401459589600563]\n","Discriminator Loss: 0.5996856410056353\n","Generator Loss: [0.8805358409881592, 0.850501537322998, 0.015017136931419373]\n","Discriminator Loss: 0.6243512872460997\n","Generator Loss: [0.7929499745368958, 0.7637765407562256, 0.014586703851819038]\n","Discriminator Loss: 0.6615653912303969\n","Generator Loss: [0.659843921661377, 0.6423083543777466, 0.00876779854297638]\n","Discriminator Loss: 0.5935076073074015\n","Generator Loss: [0.7058742642402649, 0.6870739459991455, 0.009400153532624245]\n","Discriminator Loss: 0.6325363702344475\n","Generator Loss: [0.7373412251472473, 0.719605028629303, 0.008868099190294743]\n","Discriminator Loss: 0.6062906757360906\n","Generator Loss: [0.8424797654151917, 0.7620099782943726, 0.04023490101099014]\n","Discriminator Loss: 0.6302921747374057\n","Generator Loss: [0.9603562355041504, 0.9417484998703003, 0.00930385384708643]\n","Discriminator Loss: 0.6078291827288922\n","Generator Loss: [0.8686637282371521, 0.8416355848312378, 0.013514082878828049]\n","Discriminator Loss: 0.5902328919764841\n","Generator Loss: [0.8512260913848877, 0.8337646722793579, 0.008730708621442318]\n","Discriminator Loss: 0.627468410457368\n","Generator Loss: [0.7903271913528442, 0.7713468074798584, 0.009490184485912323]\n","Discriminator Loss: 0.5870776217561797\n","Generator Loss: [0.888455331325531, 0.8724589943885803, 0.00799816194921732]\n","Discriminator Loss: 0.6222564276977209\n","Generator Loss: [0.8654200434684753, 0.8465935587882996, 0.009413250721991062]\n","Discriminator Loss: 0.5770194320648443\n","Generator Loss: [0.9077596664428711, 0.8845904469490051, 0.011584606021642685]\n","Discriminator Loss: 0.6107777748984518\n","Generator Loss: [0.8112865686416626, 0.7934383749961853, 0.008924086578190327]\n","Discriminator Loss: 0.6078318913423573\n","Generator Loss: [0.9284921288490295, 0.9066013097763062, 0.010945415124297142]\n","Discriminator Loss: 0.5941912313865032\n","Generator Loss: [0.8428844213485718, 0.8227908611297607, 0.010046789422631264]\n","Discriminator Loss: 0.593861104833195\n","Generator Loss: [0.9183974862098694, 0.8848400712013245, 0.016778701916337013]\n","Discriminator Loss: 0.6227128427854041\n","Generator Loss: [0.9019814133644104, 0.8845608234405518, 0.00871029682457447]\n","Discriminator Loss: 0.595913958575693\n","Generator Loss: [0.9466543793678284, 0.924117922782898, 0.01126823853701353]\n","Discriminator Loss: 0.5832827965859906\n","Generator Loss: [0.9060162305831909, 0.8791712522506714, 0.013422478921711445]\n","Discriminator Loss: 0.5925710127921775\n","Generator Loss: [0.9044280648231506, 0.8686558604240417, 0.0178860891610384]\n","Discriminator Loss: 0.5897233551586396\n","Generator Loss: [0.800586462020874, 0.7769567966461182, 0.011814823374152184]\n","Discriminator Loss: 0.5886454860738013\n","Generator Loss: [0.880630612373352, 0.8577947616577148, 0.011417914181947708]\n","Discriminator Loss: 0.6678055704032886\n","Generator Loss: [0.8067654371261597, 0.7803769111633301, 0.013194261118769646]\n","Discriminator Loss: 0.5670566697372124\n","Generator Loss: [0.8879348039627075, 0.8030524253845215, 0.042441196739673615]\n","Discriminator Loss: 0.5817292013671249\n","Generator Loss: [0.8163625597953796, 0.7766169309616089, 0.01987280696630478]\n","Discriminator Loss: 0.647011180393747\n","Generator Loss: [0.837369978427887, 0.817462682723999, 0.009953661821782589]\n","Discriminator Loss: 0.5909298482583836\n","Generator Loss: [0.6908488869667053, 0.6701834797859192, 0.010332701727747917]\n","Discriminator Loss: 0.6255540398415178\n","Generator Loss: [0.8626697659492493, 0.8455421924591064, 0.00856377836316824]\n","Discriminator Loss: 0.5846382483869093\n","Generator Loss: [0.834811806678772, 0.8147046566009521, 0.010053586214780807]\n","Discriminator Loss: 0.6083191413781606\n","Generator Loss: [1.1145007610321045, 0.884473443031311, 0.11501364409923553]\n","Discriminator Loss: 0.6255672523111571\n","Generator Loss: [0.9374718070030212, 0.9126836061477661, 0.01239410787820816]\n","Discriminator Loss: 0.5613414438412292\n","Generator Loss: [0.8757391571998596, 0.8447110652923584, 0.015514044091105461]\n","Discriminator Loss: 0.6310856782947667\n","Generator Loss: [0.8513022065162659, 0.8309968709945679, 0.010152663104236126]\n","Discriminator Loss: 0.5780061363184359\n","Generator Loss: [0.9012505412101746, 0.8593012094497681, 0.020974673330783844]\n","Discriminator Loss: 0.5893296642461792\n","Generator Loss: [0.9391092658042908, 0.9054776430130005, 0.01681581325829029]\n","Discriminator Loss: 0.5809189438732574\n","Generator Loss: [0.9075874090194702, 0.8890345096588135, 0.009276440367102623]\n","Discriminator Loss: 0.6217108211531013\n","Generator Loss: [0.7890127301216125, 0.7708922624588013, 0.009060225449502468]\n","Discriminator Loss: 0.5956083956116345\n","Generator Loss: [0.7710316181182861, 0.7441001534461975, 0.013465723022818565]\n","Discriminator Loss: 0.5719496379024349\n","Generator Loss: [0.8454731106758118, 0.8065217733383179, 0.019475670531392097]\n","Discriminator Loss: 0.5769991400520667\n","Generator Loss: [0.8643926382064819, 0.8441295623779297, 0.010131552815437317]\n","Discriminator Loss: 0.5968316745420452\n","Generator Loss: [0.8635066151618958, 0.8215816617012024, 0.020962484180927277]\n","Discriminator Loss: 0.5645439464060473\n","Generator Loss: [0.8758480548858643, 0.8573180437088013, 0.009265003725886345]\n","Discriminator Loss: 0.6033436840953073\n","Generator Loss: [0.8443138003349304, 0.8124684691429138, 0.015922676771879196]\n","Discriminator Loss: 0.5979184107345645\n","Generator Loss: [0.9291602373123169, 0.9084072113037109, 0.01037651114165783]\n","Discriminator Loss: 0.5813560025271727\n","Generator Loss: [0.9082185626029968, 0.8879308700561523, 0.010143836960196495]\n","Discriminator Loss: 0.6069126077272813\n","Generator Loss: [0.8390123248100281, 0.819405198097229, 0.009803570806980133]\n","Discriminator Loss: 0.5949925967579475\n","Generator Loss: [0.744429349899292, 0.7146136164665222, 0.014907879754900932]\n","Discriminator Loss: 0.6375288100243779\n","Generator Loss: [0.8665223121643066, 0.8431504964828491, 0.011685904115438461]\n","Discriminator Loss: 0.5847616044193273\n","Generator Loss: [0.897631049156189, 0.8326999545097351, 0.03246553614735603]\n","Discriminator Loss: 0.5975380275922362\n","Generator Loss: [0.9005424380302429, 0.8810012340545654, 0.009770598262548447]\n","Discriminator Loss: 0.5954221754218452\n","Generator Loss: [0.8392806053161621, 0.8061435222625732, 0.016568537801504135]\n","Discriminator Loss: 0.5878944727883209\n","Generator Loss: [0.905646026134491, 0.8723047971725464, 0.016670627519488335]\n","Discriminator Loss: 0.5840064451185754\n","Generator Loss: [0.8948643803596497, 0.8761482238769531, 0.009358074516057968]\n","Discriminator Loss: 0.6308718738000607\n","Generator Loss: [0.7434806823730469, 0.7262452840805054, 0.008617701940238476]\n","Discriminator Loss: 0.5821995430160314\n","Generator Loss: [0.7426273226737976, 0.7136096954345703, 0.014508809894323349]\n","Discriminator Loss: 0.6060343757853843\n","Generator Loss: [0.7383537888526917, 0.720109760761261, 0.009122010320425034]\n","Discriminator Loss: 0.6233898800637689\n","Generator Loss: [0.8174264430999756, 0.8000948429107666, 0.008665790781378746]\n","Discriminator Loss: 0.6109459512808826\n","Generator Loss: [0.9637545347213745, 0.9363291263580322, 0.01371269952505827]\n","Discriminator Loss: 0.6039513374562375\n","Generator Loss: [0.8756505250930786, 0.8514269590377808, 0.012111795134842396]\n","Discriminator Loss: 0.5976569543781807\n","Generator Loss: [0.8816087245941162, 0.8461495637893677, 0.017729593440890312]\n","Discriminator Loss: 0.6324943260260625\n","Generator Loss: [0.7994346618652344, 0.7756121158599854, 0.011911263689398766]\n","Discriminator Loss: 0.5972348933282774\n","Generator Loss: [0.7223092317581177, 0.6999958753585815, 0.011156689375638962]\n","Discriminator Loss: 0.5686578592139995\n","Generator Loss: [0.8504843711853027, 0.8275744915008545, 0.011454925872385502]\n","Discriminator Loss: 0.5716031473129988\n","Generator Loss: [1.0509343147277832, 0.9999828338623047, 0.025475716218352318]\n","Discriminator Loss: 0.6077627414201743\n","Generator Loss: [0.8973137140274048, 0.8782329559326172, 0.009540380910038948]\n","Discriminator Loss: 0.6654192329006037\n","Generator Loss: [0.9091084003448486, 0.880125105381012, 0.01449163444340229]\n","Discriminator Loss: 0.596210082370817\n","Generator Loss: [0.9413041472434998, 0.9169644117355347, 0.012169877998530865]\n","Discriminator Loss: 0.6514506616949802\n","Generator Loss: [0.8658801317214966, 0.816887617111206, 0.024496259167790413]\n","Discriminator Loss: 0.598653446406388\n","Generator Loss: [0.8990896344184875, 0.8768311738967896, 0.011129236780107021]\n","Discriminator Loss: 0.5776642273922334\n","Generator Loss: [0.8934623003005981, 0.8571077585220337, 0.018177257850766182]\n","Discriminator Loss: 0.6392486535114585\n","Generator Loss: [0.8123866319656372, 0.7852405309677124, 0.013573051430284977]\n","Discriminator Loss: 0.6241755949467915\n","Generator Loss: [0.9283108115196228, 0.9046652913093567, 0.011822748929262161]\n","Discriminator Loss: 0.5918378967362514\n","Generator Loss: [0.8758412003517151, 0.8186119794845581, 0.028614608570933342]\n","Discriminator Loss: 0.6510903761300142\n","Generator Loss: [0.8629822731018066, 0.8416817784309387, 0.010650239884853363]\n","Discriminator Loss: 0.5820188348079682\n","Generator Loss: [0.9084970355033875, 0.890277624130249, 0.00910971686244011]\n","Discriminator Loss: 0.5929473587020766\n","Generator Loss: [0.9632174372673035, 0.9324118494987488, 0.015402800403535366]\n","Discriminator Loss: 0.6284148852791986\n","Generator Loss: [0.8904953598976135, 0.8614454865455627, 0.014524935744702816]\n","Discriminator Loss: 0.6339360576239415\n","Generator Loss: [0.7483194470405579, 0.6903253793716431, 0.02899702452123165]\n","Discriminator Loss: 0.6562943982826255\n","Generator Loss: [0.8075371980667114, 0.7734408974647522, 0.017048165202140808]\n","Discriminator Loss: 0.606059665333305\n","Generator Loss: [0.8081023097038269, 0.7848193645477295, 0.011641481891274452]\n","Discriminator Loss: 0.5887301973816648\n","Generator Loss: [0.8679975271224976, 0.8236720561981201, 0.02216273732483387]\n","Discriminator Loss: 0.5612046659807675\n","Generator Loss: [0.8188911080360413, 0.7919293642044067, 0.01348086167126894]\n","Discriminator Loss: 0.5902409863410867\n","Generator Loss: [0.8272839188575745, 0.8068733811378479, 0.010205265134572983]\n","Discriminator Loss: 0.5891284015742713\n","Generator Loss: [0.789262056350708, 0.7663255929946899, 0.011468242853879929]\n","Discriminator Loss: 0.5944358213819214\n","Generator Loss: [0.7854753732681274, 0.7656928896903992, 0.00989125482738018]\n","Discriminator Loss: 0.5834798735158984\n","Generator Loss: [0.9234181642532349, 0.8873109817504883, 0.018053606152534485]\n","Discriminator Loss: 0.6125533708691364\n","Generator Loss: [0.8406015038490295, 0.8215271234512329, 0.009537199512124062]\n","Discriminator Loss: 0.6025892974648741\n","Generator Loss: [0.7911904454231262, 0.7470236420631409, 0.02208341285586357]\n","Discriminator Loss: 0.5687422418850474\n","Generator Loss: [0.8326610326766968, 0.8021647930145264, 0.015248110517859459]\n","Discriminator Loss: 0.6043715729974792\n","Generator Loss: [0.8071646094322205, 0.7528589963912964, 0.027152802795171738]\n","Discriminator Loss: 0.5729054505391105\n","Generator Loss: [0.7633169889450073, 0.7432408332824707, 0.010038089007139206]\n","Discriminator Loss: 0.5767055750475265\n","Generator Loss: [0.795890212059021, 0.7780990600585938, 0.008895562961697578]\n","Discriminator Loss: 0.5741948229260743\n","Generator Loss: [0.8089870810508728, 0.7855433225631714, 0.011721877381205559]\n","Discriminator Loss: 0.5738240956925438\n","Generator Loss: [0.8336753249168396, 0.8064360618591309, 0.01361964549869299]\n","Discriminator Loss: 0.5976848709433398\n","Generator Loss: [0.7782927751541138, 0.7591835260391235, 0.009554624557495117]\n","Discriminator Loss: 0.5771401206402516\n","Generator Loss: [0.8095510005950928, 0.7901952862739563, 0.009677862748503685]\n","Discriminator Loss: 0.6104250995213079\n","Generator Loss: [0.8071375489234924, 0.7869594097137451, 0.010089067742228508]\n","Discriminator Loss: 0.5626779279627954\n","Generator Loss: [0.85182124376297, 0.8343964219093323, 0.008712399750947952]\n","Discriminator Loss: 0.6093532303821121\n","Generator Loss: [0.8373638987541199, 0.8203620314598083, 0.008500944823026657]\n","Discriminator Loss: 0.5687863775474398\n","Generator Loss: [0.8387104868888855, 0.8173823356628418, 0.010664082132279873]\n","Discriminator Loss: 0.574596162616217\n","Generator Loss: [0.8849517107009888, 0.8565568327903748, 0.014197451062500477]\n","Discriminator Loss: 0.5809177075934713\n","Generator Loss: [0.8754264712333679, 0.8334152698516846, 0.02100558765232563]\n","Discriminator Loss: 0.6170842197498132\n","Generator Loss: [0.8329660296440125, 0.7869614958763123, 0.023002278059720993]\n","Discriminator Loss: 0.5566979583454668\n","Generator Loss: [0.817720353603363, 0.7950843572616577, 0.011318005621433258]\n","Discriminator Loss: 0.5713491629794589\n","Generator Loss: [0.8665437698364258, 0.8378475904464722, 0.014348089694976807]\n","Discriminator Loss: 0.5685031874854758\n","Generator Loss: [0.8695204257965088, 0.8323326110839844, 0.01859389990568161]\n","Discriminator Loss: 0.5789410462894011\n","Generator Loss: [0.9951541423797607, 0.826194167137146, 0.08447997272014618]\n","Discriminator Loss: 0.5683724975751829\n","Generator Loss: [0.9397429823875427, 0.9027845859527588, 0.018479205667972565]\n","Discriminator Loss: 0.643255249116919\n","Generator Loss: [0.7908240556716919, 0.7639850378036499, 0.0134194977581501]\n","Discriminator Loss: 0.5861703853588551\n","Generator Loss: [0.7939956784248352, 0.7678802013397217, 0.013057724572718143]\n","Discriminator Loss: 0.5887578339315951\n","Generator Loss: [0.8399360179901123, 0.8215356469154358, 0.009200191125273705]\n","Discriminator Loss: 0.6064197575906292\n","Generator Loss: [0.9630075097084045, 0.9268173575401306, 0.018095076084136963]\n","Discriminator Loss: 0.5655994478438515\n","Generator Loss: [0.9725756645202637, 0.9254360198974609, 0.02356981858611107]\n","Discriminator Loss: 0.5780431057974056\n","Generator Loss: [0.896886944770813, 0.8725526332855225, 0.01216716319322586]\n","Discriminator Loss: 0.6245579849892238\n","Generator Loss: [0.8582653999328613, 0.8292047381401062, 0.014530343934893608]\n","Discriminator Loss: 0.5681231807138829\n","Generator Loss: [1.0102720260620117, 0.912580132484436, 0.04884594678878784]\n","Discriminator Loss: 0.5795641509466805\n","Generator Loss: [0.9025323390960693, 0.8790208697319031, 0.011755727231502533]\n","Discriminator Loss: 0.5909850902062317\n","Generator Loss: [0.9284839034080505, 0.8925465941429138, 0.017968647181987762]\n","Discriminator Loss: 0.5746654997492442\n","Generator Loss: [0.9258328080177307, 0.9044027924537659, 0.010715004988014698]\n","Discriminator Loss: 0.6071878335023939\n","Generator Loss: [0.9953590631484985, 0.9381937384605408, 0.028582649305462837]\n","Discriminator Loss: 0.5810670885039144\n","Generator Loss: [1.0014909505844116, 0.9810649156570435, 0.010213029570877552]\n","Discriminator Loss: 0.6271979020693834\n","Generator Loss: [0.8791028261184692, 0.8599522113800049, 0.009575296193361282]\n","Discriminator Loss: 0.5745985726825893\n","Generator Loss: [0.8872556686401367, 0.8212413787841797, 0.03300715237855911]\n","Discriminator Loss: 0.6015619522004272\n","Generator Loss: [0.8263159394264221, 0.8017292618751526, 0.012293338775634766]\n","Discriminator Loss: 0.6044334527759929\n","Generator Loss: [0.7990761399269104, 0.7775985598564148, 0.0107387974858284]\n","Discriminator Loss: 0.6155800685010036\n","Generator Loss: [0.8456090688705444, 0.8191521167755127, 0.01322847232222557]\n","Discriminator Loss: 0.5629684880768764\n","Generator Loss: [0.8437772393226624, 0.8244451880455017, 0.0096660191193223]\n","Discriminator Loss: 0.5816488548152847\n","Generator Loss: [0.8267536163330078, 0.8077929615974426, 0.009480338543653488]\n","Discriminator Loss: 0.6042375611705211\n","Generator Loss: [0.8076320886611938, 0.7883666753768921, 0.009632718749344349]\n","Discriminator Loss: 0.5885807337108417\n","Generator Loss: [0.8536790609359741, 0.8328655958175659, 0.010406735353171825]\n","Discriminator Loss: 0.575420559009217\n","Generator Loss: [0.8365538716316223, 0.8200876712799072, 0.008233104832470417]\n","Discriminator Loss: 0.5822294855170185\n","Generator Loss: [0.8636025786399841, 0.8463269472122192, 0.008637811057269573]\n","Discriminator Loss: 0.5687090208848531\n","Generator Loss: [0.8128293752670288, 0.7951112985610962, 0.008859041146934032]\n","Discriminator Loss: 0.5851142718092888\n","Generator Loss: [0.8103063702583313, 0.790690541267395, 0.00980791449546814]\n","Discriminator Loss: 0.5702998483211559\n","Generator Loss: [0.9405567646026611, 0.9164744019508362, 0.012041181325912476]\n","Discriminator Loss: 0.5554654555871821\n","Generator Loss: [0.957952082157135, 0.9342164993286133, 0.01186777837574482]\n","Discriminator Loss: 0.5974002548318822\n","Generator Loss: [0.8427395820617676, 0.8153482675552368, 0.01369566936045885]\n","Discriminator Loss: 0.6048264272976667\n","Generator Loss: [0.8467814326286316, 0.8234416246414185, 0.011669901199638844]\n","Discriminator Loss: 0.5679703140631318\n","Generator Loss: [0.9243940711021423, 0.862288236618042, 0.031052928417921066]\n","Discriminator Loss: 0.6106510991885443\n","Generator Loss: [0.7469116449356079, 0.7068597078323364, 0.02002597413957119]\n","Discriminator Loss: 0.6018557729330496\n","Generator Loss: [0.776689887046814, 0.7396833896636963, 0.018503235653042793]\n","Discriminator Loss: 0.5792156645657087\n","Generator Loss: [0.7225592732429504, 0.6988518834114075, 0.011853707022964954]\n","Discriminator Loss: 0.5970619417694252\n","Generator Loss: [0.944817841053009, 0.8972315192222595, 0.023793162778019905]\n","Discriminator Loss: 0.5593168521863845\n","Generator Loss: [0.8000458478927612, 0.7726978063583374, 0.013674023561179638]\n","Discriminator Loss: 0.6057441771918093\n","Generator Loss: [0.8066878914833069, 0.7780323028564453, 0.01432779710739851]\n","Discriminator Loss: 0.5652179832395632\n","Generator Loss: [0.8295369148254395, 0.8085209131240845, 0.010508009232580662]\n","Discriminator Loss: 0.7174986599275144\n","Generator Loss: [0.4144679009914398, 0.38827913999557495, 0.01309437956660986]\n","Discriminator Loss: 0.6070861729967874\n","Generator Loss: [0.5121792554855347, 0.490774005651474, 0.010702637955546379]\n","Discriminator Loss: 0.578682579951419\n","Generator Loss: [0.5846641063690186, 0.5595674514770508, 0.012548316270112991]\n","Discriminator Loss: 0.5869417141511803\n","Generator Loss: [0.6819387674331665, 0.6588577628135681, 0.011540498584508896]\n","Discriminator Loss: 0.590635668442701\n","Generator Loss: [0.7523195147514343, 0.7343242764472961, 0.008997620083391666]\n","Discriminator Loss: 0.5889353182574268\n","Generator Loss: [0.6069795489311218, 0.5864946842193604, 0.010242429561913013]\n","Discriminator Loss: 0.6310374385793693\n","Generator Loss: [0.6928487420082092, 0.672641396522522, 0.010103675536811352]\n","Discriminator Loss: 0.6042493920540437\n","Generator Loss: [0.7471247911453247, 0.7278566956520081, 0.009634044021368027]\n","Discriminator Loss: 0.6177327091718325\n","Generator Loss: [0.902580976486206, 0.8854781985282898, 0.0085514010861516]\n","Discriminator Loss: 0.5877871299744584\n","Generator Loss: [0.8834792971611023, 0.866295337677002, 0.008591972291469574]\n","Discriminator Loss: 0.5999590571445879\n","Generator Loss: [0.8289477229118347, 0.811206579208374, 0.008870585821568966]\n","Discriminator Loss: 0.654487612657249\n","Generator Loss: [0.8360435962677002, 0.815280020236969, 0.010381781496107578]\n","Discriminator Loss: 0.5945216566033196\n","Generator Loss: [0.7288647294044495, 0.7082439661026001, 0.010310370475053787]\n","Discriminator Loss: 0.6026091193489265\n","Generator Loss: [0.7562540173530579, 0.7379470467567444, 0.009153475053608418]\n","Discriminator Loss: 0.6157954536611214\n","Generator Loss: [0.7336093187332153, 0.7141727209091187, 0.009718300774693489]\n","Discriminator Loss: 0.5812273030023789\n","Generator Loss: [0.7302494049072266, 0.7031722664833069, 0.013538576662540436]\n","Discriminator Loss: 0.5921139113706886\n","Generator Loss: [0.7755839228630066, 0.752953290939331, 0.01131532434374094]\n","Discriminator Loss: 0.592658444904373\n","Generator Loss: [0.7337305545806885, 0.7165929079055786, 0.008568813093006611]\n","Discriminator Loss: 0.5984259332763031\n","Generator Loss: [0.6852084398269653, 0.6616457104682922, 0.011781361885368824]\n","Discriminator Loss: 0.5995651393095613\n","Generator Loss: [0.6428490281105042, 0.6214867830276489, 0.010681108571588993]\n","Discriminator Loss: 0.6125977946794592\n","Generator Loss: [0.7396916747093201, 0.7008899450302124, 0.019400864839553833]\n","Discriminator Loss: 0.5714947444685095\n","Generator Loss: [0.9348387718200684, 0.918723464012146, 0.008057666011154652]\n","Discriminator Loss: 0.5774972857852845\n","Generator Loss: [1.0251308679580688, 0.9948989152908325, 0.015115986578166485]\n","Discriminator Loss: 0.6013216767787526\n","Generator Loss: [1.0127702951431274, 0.9935455918312073, 0.009612363763153553]\n","Discriminator Loss: 0.6676090832406771\n","Generator Loss: [0.8392386436462402, 0.8189243674278259, 0.01015714555978775]\n","Discriminator Loss: 0.6056391528618406\n","Generator Loss: [0.8975123763084412, 0.8795120716094971, 0.009000143967568874]\n","Discriminator Loss: 0.6102634866947483\n","Generator Loss: [0.9279036521911621, 0.8768548965454102, 0.02552439272403717]\n","Discriminator Loss: 0.620797909097746\n","Generator Loss: [0.7390590310096741, 0.7184103727340698, 0.010324319824576378]\n","Discriminator Loss: 0.5853288571088342\n","Generator Loss: [0.77604740858078, 0.7437484264373779, 0.016149481758475304]\n","Discriminator Loss: 0.6153534385666717\n","Generator Loss: [0.7358523011207581, 0.7150270938873291, 0.01041259802877903]\n","Discriminator Loss: 0.6044112263843999\n","Generator Loss: [0.6938976049423218, 0.6746203899383545, 0.009638603776693344]\n","Discriminator Loss: 0.6267672725371085\n","Generator Loss: [0.7078244686126709, 0.6874940395355225, 0.010165218263864517]\n","Discriminator Loss: 0.5966314224679081\n","Generator Loss: [0.7129366993904114, 0.687982439994812, 0.012477143667638302]\n","Discriminator Loss: 0.5854584892367711\n","Generator Loss: [0.8241405487060547, 0.7512565851211548, 0.036441970616579056]\n","Discriminator Loss: 0.5918401931594417\n","Generator Loss: [0.7311882972717285, 0.7051963806152344, 0.012995961122214794]\n","Discriminator Loss: 0.5711423342727358\n","Generator Loss: [1.0075898170471191, 0.9784421920776367, 0.014573784545063972]\n","Discriminator Loss: 0.603288996295305\n","Generator Loss: [0.811492383480072, 0.793597936630249, 0.008947232738137245]\n","Discriminator Loss: 0.6609378409448254\n","Generator Loss: [0.6582704782485962, 0.6409326791763306, 0.008668893948197365]\n","Discriminator Loss: 0.6071944151699427\n","Generator Loss: [0.5376629829406738, 0.5200153589248657, 0.008823826909065247]\n","Discriminator Loss: 0.5598509956907947\n","Generator Loss: [1.5736677646636963, 1.2710583209991455, 0.15130473673343658]\n","Discriminator Loss: 0.5656405637637363\n","Generator Loss: [0.787373960018158, 0.7543240785598755, 0.01652492955327034]\n","Discriminator Loss: 0.5647174370242283\n","Generator Loss: [0.8270076513290405, 0.7176960706710815, 0.054655805230140686]\n","Discriminator Loss: 0.5744157035442186\n","Generator Loss: [0.7257040739059448, 0.6810835599899292, 0.02231026068329811]\n","Discriminator Loss: 0.6387957186670974\n","Generator Loss: [0.5146764516830444, 0.4533524513244629, 0.03066200762987137]\n","Discriminator Loss: 0.5734184871398611\n","Generator Loss: [6.757632255554199, 6.68887186050415, 0.03438008949160576]\n","Discriminator Loss: 0.5568956819479354\n","Generator Loss: [10.030113220214844, 9.955533027648926, 0.037290219217538834]\n","Discriminator Loss: 0.6270687356591225\n","Generator Loss: [0.7505946159362793, 0.6811239123344421, 0.034735340625047684]\n","Discriminator Loss: 0.7692264357829117\n","Generator Loss: [0.8596670627593994, 0.7920038104057312, 0.0338316410779953]\n","Discriminator Loss: 0.6053816750645638\n","Generator Loss: [1.0856153964996338, 1.0085155963897705, 0.03854990005493164]\n","Discriminator Loss: 0.720888416050002\n","Generator Loss: [516033.25, 1.9568665027618408, 258015.640625]\n","Discriminator Loss: 0.5725200937654336\n","Generator Loss: [0.9626044631004333, 0.9035693407058716, 0.02951757423579693]\n","Discriminator Loss: 0.6305375235360771\n","Generator Loss: [1.3646382093429565, 0.9550670385360718, 0.2047855705022812]\n","Discriminator Loss: 0.576035623403186\n","Generator Loss: [1.0162138938903809, 0.9900681972503662, 0.013072826899588108]\n","Discriminator Loss: 0.636239588450735\n","Generator Loss: [0.9817922115325928, 0.9474436640739441, 0.01717427186667919]\n","Discriminator Loss: 0.6072105182329324\n","Generator Loss: [0.73929762840271, 0.7156379222869873, 0.011829855851829052]\n","Discriminator Loss: 0.703368761769525\n","Generator Loss: [0.8914015889167786, 0.868063747882843, 0.011668926104903221]\n","Discriminator Loss: 0.6076696924737917\n","Generator Loss: [0.9069849252700806, 0.8803235292434692, 0.01333069708198309]\n","Discriminator Loss: 0.5876964577655599\n","Generator Loss: [0.8596832752227783, 0.824728786945343, 0.017477253451943398]\n","Discriminator Loss: 0.5823286107661261\n","Generator Loss: [0.9071077108383179, 0.7865990996360779, 0.060254305601119995]\n","Discriminator Loss: 0.5985256701569597\n","Generator Loss: [0.8073904514312744, 0.7731475830078125, 0.017121421173214912]\n","Discriminator Loss: 0.576088940983027\n","Generator Loss: [0.919979989528656, 0.6838088035583496, 0.1180856004357338]\n","Discriminator Loss: 0.6203111654449458\n","Generator Loss: [0.8720553517341614, 0.8496403694152832, 0.011207498610019684]\n","Discriminator Loss: 0.5872671665492817\n","Generator Loss: [0.8211178779602051, 0.7958006262779236, 0.012658635154366493]\n","Discriminator Loss: 0.5932241253285611\n","Generator Loss: [0.9936048984527588, 0.9020045399665833, 0.04580019414424896]\n","Discriminator Loss: 0.6200004970887676\n","Generator Loss: [0.880660355091095, 0.8231089115142822, 0.028775708749890327]\n","Discriminator Loss: 0.6497866203135345\n","Generator Loss: [0.752769410610199, 0.6545708179473877, 0.04909930005669594]\n","Discriminator Loss: 0.7193648845495773\n","Generator Loss: [0.7279466986656189, 0.6869425177574158, 0.020502086728811264]\n","Discriminator Loss: 0.6804062245064415\n","Generator Loss: [1.3932933807373047, 0.8650671243667603, 0.2641131281852722]\n","Discriminator Loss: 0.5987578883286915\n","Generator Loss: [0.9080674052238464, 0.8870461583137512, 0.010510637424886227]\n","Discriminator Loss: 0.5831209290627157\n","Generator Loss: [1.3748130798339844, 1.1205956935882568, 0.12710870802402496]\n","Discriminator Loss: 0.6277096768171759\n","Generator Loss: [1.2204772233963013, 1.1975080966949463, 0.011484585702419281]\n","Discriminator Loss: 0.7575252655442455\n","Generator Loss: [0.9189135432243347, 0.8855808973312378, 0.01666632853448391]\n","Discriminator Loss: 0.5731839809504891\n","Generator Loss: [0.8968633413314819, 0.8695306181907654, 0.013666350394487381]\n","Discriminator Loss: 0.6026576915064652\n","Generator Loss: [0.9143664836883545, 0.8844965696334839, 0.014934955164790154]\n","Discriminator Loss: 0.6166481034451863\n","Generator Loss: [0.8619634509086609, 0.8342821598052979, 0.013840645551681519]\n","Discriminator Loss: 0.582865363718156\n","Generator Loss: [0.9133789539337158, 0.8843926787376404, 0.014493124559521675]\n","Discriminator Loss: 0.6589859571540728\n","Generator Loss: [0.8620752096176147, 0.8284324407577515, 0.016821378841996193]\n","Discriminator Loss: 0.6081853278265044\n","Generator Loss: [0.9763779640197754, 0.9428139925003052, 0.016781970858573914]\n","Discriminator Loss: 0.5561102269639377\n","Generator Loss: [1.0292534828186035, 1.0093059539794922, 0.00997374951839447]\n","Discriminator Loss: 0.5934600669424981\n","Generator Loss: [0.8192813396453857, 0.7818245887756348, 0.018728386610746384]\n","Discriminator Loss: 0.591671339563618\n","Generator Loss: [0.7917004823684692, 0.7556308507919312, 0.01803482510149479]\n","Discriminator Loss: 0.589748099388089\n","Generator Loss: [0.706301748752594, 0.6813974380493164, 0.012452146038413048]\n","Discriminator Loss: 0.6120607899138122\n","Generator Loss: [0.7572993040084839, 0.728952944278717, 0.014173180796205997]\n","Discriminator Loss: 0.6401921994474833\n","Generator Loss: [0.8802585601806641, 0.8524744510650635, 0.013892052695155144]\n","Discriminator Loss: 0.6319261984535842\n","Generator Loss: [1.0153188705444336, 0.9752068519592285, 0.0200559813529253]\n","Discriminator Loss: 0.6021810384336277\n","Generator Loss: [0.9354354739189148, 0.9116042256355286, 0.011915613897144794]\n","Discriminator Loss: 0.5964017133737798\n","Generator Loss: [0.9465444087982178, 0.9253605604171753, 0.010591921396553516]\n","Discriminator Loss: 0.6014042810202227\n","Generator Loss: [0.8746861219406128, 0.8212020993232727, 0.02674201875925064]\n","Discriminator Loss: 0.6076200420793612\n","Generator Loss: [0.73685622215271, 0.7099676728248596, 0.013444279320538044]\n","Discriminator Loss: 0.6213296331115998\n","Generator Loss: [0.6912752985954285, 0.6633294224739075, 0.01397294458001852]\n","Discriminator Loss: 0.5996435401903\n","Generator Loss: [0.7842442393302917, 0.747738242149353, 0.018252987414598465]\n","Discriminator Loss: 0.5814825642228243\n","Generator Loss: [0.8360536694526672, 0.8077131509780884, 0.014170246198773384]\n","Discriminator Loss: 0.6709065726972767\n","Generator Loss: [0.6994337439537048, 0.6603536605834961, 0.01954003795981407]\n","Discriminator Loss: 0.5966743938079162\n","Generator Loss: [0.7213082313537598, 0.6986966133117676, 0.011305799707770348]\n","Discriminator Loss: 0.6118972944968846\n","Generator Loss: [0.7589316368103027, 0.7348364591598511, 0.01204759068787098]\n","Discriminator Loss: 0.6197972825029865\n","Generator Loss: [0.8606327176094055, 0.7908351421356201, 0.0348987802863121]\n","Discriminator Loss: 0.5637313192128204\n","Generator Loss: [0.7689949870109558, 0.7375775575637817, 0.01570870727300644]\n","Discriminator Loss: 0.5998926552601915\n","Generator Loss: [1.0650608539581299, 0.7638044953346252, 0.15062816441059113]\n","Discriminator Loss: 0.6015166284159932\n","Generator Loss: [0.7716348767280579, 0.7384368181228638, 0.016599036753177643]\n","Discriminator Loss: 0.5698897908623621\n","Generator Loss: [0.8330700397491455, 0.7949955463409424, 0.01903725042939186]\n","Discriminator Loss: 0.5602795211525518\n","Generator Loss: [0.847883939743042, 0.8258109092712402, 0.011036520823836327]\n","Discriminator Loss: 0.5779730754366028\n","Generator Loss: [0.7881495356559753, 0.754421591758728, 0.01686396449804306]\n","Discriminator Loss: 0.6112649513961514\n","Generator Loss: [0.8114094138145447, 0.7881602644920349, 0.011624569073319435]\n","Discriminator Loss: 0.5725999500173202\n","Generator Loss: [0.9854878783226013, 0.8531374931335449, 0.0661752000451088]\n","Discriminator Loss: 0.579683718882734\n","Generator Loss: [0.9319326281547546, 0.901540994644165, 0.01519582699984312]\n","Discriminator Loss: 0.585595271535567\n","Generator Loss: [0.8432864546775818, 0.8189579248428345, 0.012164255604147911]\n","Discriminator Loss: 0.576192544265723\n","Generator Loss: [1.5285298824310303, 0.8032063245773315, 0.36266177892684937]\n","Discriminator Loss: 0.5834154230105923\n","Generator Loss: [0.9792450666427612, 0.7800964117050171, 0.09957432746887207]\n","Discriminator Loss: 0.6085176130145555\n","Generator Loss: [0.7621701955795288, 0.7328296899795532, 0.014670262113213539]\n","Discriminator Loss: 0.5769486421013426\n","Generator Loss: [4.659620761871338, 0.8093776702880859, 1.925121545791626]\n","Discriminator Loss: 0.5788623684966296\n","Generator Loss: [0.8500123023986816, 0.8244680762290955, 0.012772107496857643]\n","Discriminator Loss: 0.6044267287288676\n","Generator Loss: [0.7140253782272339, 0.6848365068435669, 0.014594441279768944]\n","Discriminator Loss: 0.5823949418409029\n","Generator Loss: [0.9862320423126221, 0.7639119029045105, 0.11116008460521698]\n","Discriminator Loss: 0.5907414819157566\n","Generator Loss: [0.9092214107513428, 0.7642029523849487, 0.07250922918319702]\n","Discriminator Loss: 0.5726006539625814\n","Generator Loss: [0.8305801749229431, 0.8030733466148376, 0.013753402046859264]\n","Discriminator Loss: 0.5787258227901475\n","Generator Loss: [0.8459828495979309, 0.820202648639679, 0.012890090234577656]\n","Discriminator Loss: 0.5855131658863684\n","Generator Loss: [0.8496414422988892, 0.8274213075637817, 0.011110062710940838]\n","Discriminator Loss: 0.5915460812666424\n","Generator Loss: [0.7843376994132996, 0.7537969350814819, 0.015270394273102283]\n","Discriminator Loss: 0.5721606965689716\n","Generator Loss: [0.8326773643493652, 0.7835543155670166, 0.02456151507794857]\n","Discriminator Loss: 0.5780349687411217\n","Generator Loss: [0.8140885233879089, 0.7910489439964294, 0.011519781313836575]\n","Discriminator Loss: 0.573689739791007\n","Generator Loss: [0.7663763761520386, 0.7373430728912354, 0.014516657218337059]\n","Discriminator Loss: 0.5759132397506619\n","Generator Loss: [1.0685479640960693, 0.7480632662773132, 0.16024234890937805]\n","Discriminator Loss: 0.581421394526842\n","Generator Loss: [0.8775979280471802, 0.8440074920654297, 0.016795212402939796]\n","Discriminator Loss: 0.575193674012553\n","Generator Loss: [0.9434948563575745, 0.916476309299469, 0.013509261421859264]\n","Discriminator Loss: 0.5707584319679881\n","Generator Loss: [0.9774349927902222, 0.9330742955207825, 0.022180357947945595]\n","Discriminator Loss: 0.5922851438554062\n","Generator Loss: [1.002215027809143, 0.9327255487442017, 0.0347447469830513]\n","Discriminator Loss: 0.5649388563942921\n","Generator Loss: [1.0045720338821411, 0.9662373661994934, 0.0191673431545496]\n","Discriminator Loss: 0.6167717262851511\n","Generator Loss: [0.9479069113731384, 0.9259341359138489, 0.01098638866096735]\n","Discriminator Loss: 0.5647832195227238\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 2/10 [1:02:23<3:42:05, 1665.70s/it]"]},{"name":"stdout","output_type":"stream","text":["Generator Loss: [0.9799147248268127, 0.9510765075683594, 0.014419106766581535]\n","Epoch: 2\n","Discriminator Loss: 0.5934584661445115\n","Generator Loss: [0.7932219505310059, 0.7636231184005737, 0.01479942537844181]\n","Discriminator Loss: 0.586206319861958\n","Generator Loss: [0.7369515299797058, 0.7009677886962891, 0.017991872504353523]\n","Discriminator Loss: 0.5550873750908067\n","Generator Loss: [0.7333082556724548, 0.7075927257537842, 0.012857764028012753]\n","Discriminator Loss: 0.6011784273432568\n","Generator Loss: [0.8255801200866699, 0.7824885845184326, 0.021545782685279846]\n","Discriminator Loss: 0.5655485584284179\n","Generator Loss: [1.1280219554901123, 0.844417154788971, 0.14180240035057068]\n","Discriminator Loss: 0.6323684251037776\n","Generator Loss: [1.2695610523223877, 0.8585829138755798, 0.20548906922340393]\n","Discriminator Loss: 0.5810435427119955\n","Generator Loss: [1.0404924154281616, 1.007378101348877, 0.016557171940803528]\n","Discriminator Loss: 0.5802200832076778\n","Generator Loss: [1.0429569482803345, 1.005867600440979, 0.018544647842645645]\n","Discriminator Loss: 0.6016686812181433\n","Generator Loss: [1.0101079940795898, 0.9041411280632019, 0.05298343673348427]\n","Discriminator Loss: 0.5882930418010801\n","Generator Loss: [0.9912140369415283, 0.9064763188362122, 0.04236886277794838]\n","Discriminator Loss: 0.5784759524940455\n","Generator Loss: [0.9370417594909668, 0.8802067637443542, 0.02841748297214508]\n","Discriminator Loss: 0.5695371722176787\n","Generator Loss: [0.9187199473381042, 0.8897885084152222, 0.014465725049376488]\n","Discriminator Loss: 0.5967016345821321\n","Generator Loss: [0.9966342449188232, 0.9551506042480469, 0.02074180543422699]\n","Discriminator Loss: 0.5889928472661268\n","Generator Loss: [0.8881887793540955, 0.8571302890777588, 0.01552925817668438]\n","Discriminator Loss: 0.5705725367297418\n","Generator Loss: [0.9459834694862366, 0.8692141771316528, 0.038384657353162766]\n","Discriminator Loss: 0.5678430253537954\n","Generator Loss: [0.9343024492263794, 0.890227198600769, 0.02203764021396637]\n","Discriminator Loss: 0.6022164018431795\n","Generator Loss: [0.8652880191802979, 0.8234332799911499, 0.02092735841870308]\n","Discriminator Loss: 0.5666076801699091\n","Generator Loss: [0.8656408786773682, 0.8429586291313171, 0.011341121979057789]\n","Discriminator Loss: 0.5666718868160388\n","Generator Loss: [0.8900167942047119, 0.8445731401443481, 0.022721825167536736]\n","Discriminator Loss: 0.5619595809912425\n","Generator Loss: [0.8674465417861938, 0.8385564088821411, 0.014445075765252113]\n","Discriminator Loss: 0.5659182599811174\n","Generator Loss: [0.9269272089004517, 0.8403438329696655, 0.04329170286655426]\n","Discriminator Loss: 0.5642357300966978\n","Generator Loss: [0.9385974407196045, 0.9096033573150635, 0.01449703797698021]\n","Discriminator Loss: 0.5728686830389051\n","Generator Loss: [0.9233596324920654, 0.8362034559249878, 0.04357807710766792]\n","Discriminator Loss: 0.5981448498205282\n","Generator Loss: [0.7841091752052307, 0.7404888868331909, 0.0218101404607296]\n","Discriminator Loss: 0.5897542656311998\n","Generator Loss: [0.8694707155227661, 0.8430585861206055, 0.013206054456532001]\n","Discriminator Loss: 0.5691023684557877\n","Generator Loss: [0.9219167828559875, 0.824455976486206, 0.04873041436076164]\n","Discriminator Loss: 0.565963873435976\n","Generator Loss: [0.880292534828186, 0.8537619709968567, 0.013265278190374374]\n","Discriminator Loss: 0.5975311488909938\n","Generator Loss: [0.7945761680603027, 0.7708286046981812, 0.011873790062963963]\n","Discriminator Loss: 0.5995791118984926\n","Generator Loss: [0.8732777833938599, 0.8512380123138428, 0.011019889265298843]\n","Discriminator Loss: 0.6166624130273703\n","Generator Loss: [0.776402473449707, 0.755886435508728, 0.010258009657263756]\n","Discriminator Loss: 0.5848341861710651\n","Generator Loss: [0.8980073928833008, 0.8727368116378784, 0.012635300867259502]\n","Discriminator Loss: 0.5666307526844321\n","Generator Loss: [0.8949871063232422, 0.8588289618492126, 0.018079068511724472]\n","Discriminator Loss: 0.6382754458172712\n","Generator Loss: [0.7716923952102661, 0.7440600991249084, 0.013816162943840027]\n","Discriminator Loss: 0.6142762374365702\n","Generator Loss: [0.8514952063560486, 0.8294886350631714, 0.01100328378379345]\n","Discriminator Loss: 0.6364017582764063\n","Generator Loss: [0.8903919458389282, 0.8640579581260681, 0.013166990131139755]\n","Discriminator Loss: 0.5600840208826412\n","Generator Loss: [0.9025601148605347, 0.8623775839805603, 0.02009127475321293]\n","Discriminator Loss: 0.5840287497558165\n","Generator Loss: [0.9858703017234802, 0.9422090649604797, 0.02183062583208084]\n","Discriminator Loss: 0.588706854818156\n","Generator Loss: [0.891728937625885, 0.8723812103271484, 0.009673849679529667]\n","Discriminator Loss: 0.5863364321849076\n","Generator Loss: [0.9731131196022034, 0.842836856842041, 0.06513813138008118]\n","Discriminator Loss: 0.6149500799219823\n","Generator Loss: [0.9706403017044067, 0.946762204170227, 0.01193904597312212]\n","Discriminator Loss: 0.5814685957884649\n","Generator Loss: [0.8521450757980347, 0.8221868872642517, 0.01497909240424633]\n","Discriminator Loss: 0.6127778064419545\n","Generator Loss: [0.6403898000717163, 0.5955607891082764, 0.02241450361907482]\n","Discriminator Loss: 0.7135655200472684\n","Generator Loss: [0.9956017136573792, 0.9696121215820312, 0.01299480814486742]\n","Discriminator Loss: 0.5696416173886973\n","Generator Loss: [0.7840448617935181, 0.7564585208892822, 0.013793176040053368]\n","Discriminator Loss: 0.6375281328728306\n","Generator Loss: [1.0477392673492432, 1.0257049798965454, 0.011017155833542347]\n","Discriminator Loss: 0.5801268491341034\n","Generator Loss: [1.182373046875, 0.8906927108764648, 0.14584016799926758]\n","Discriminator Loss: 0.5889634124614531\n","Generator Loss: [0.9808963537216187, 0.897573709487915, 0.041661329567432404]\n","Discriminator Loss: 0.610330177238211\n","Generator Loss: [0.8355362415313721, 0.8108258247375488, 0.012355223298072815]\n","Discriminator Loss: 0.5871566192799946\n","Generator Loss: [0.9704116582870483, 0.9385929107666016, 0.015909375622868538]\n","Discriminator Loss: 0.5773786933696101\n","Generator Loss: [0.9938992857933044, 0.9641801118850708, 0.014859572984278202]\n","Discriminator Loss: 0.5787421638378873\n","Generator Loss: [1.0243573188781738, 0.9927512407302856, 0.015803050249814987]\n","Discriminator Loss: 0.6541542774211848\n","Generator Loss: [0.8006442189216614, 0.7774239778518677, 0.011610112152993679]\n","Discriminator Loss: 0.5830265493423212\n","Generator Loss: [0.9181796312332153, 0.8872492909431458, 0.015465162694454193]\n","Discriminator Loss: 0.598273438270553\n","Generator Loss: [0.817703902721405, 0.7929104566574097, 0.0123967370018363]\n","Discriminator Loss: 0.6300903864757856\n","Generator Loss: [0.7689793109893799, 0.718475878238678, 0.025251716375350952]\n","Discriminator Loss: 0.5914657327593886\n","Generator Loss: [1.112674593925476, 0.9704645872116089, 0.0711049810051918]\n","Discriminator Loss: 0.568368337546417\n","Generator Loss: [1.0247806310653687, 0.9656744003295898, 0.029553093016147614]\n","Discriminator Loss: 0.6027636879880447\n","Generator Loss: [0.9099388122558594, 0.8870696425437927, 0.011434595100581646]\n","Discriminator Loss: 0.5896490679588169\n","Generator Loss: [0.8285110592842102, 0.8065037131309509, 0.01100368145853281]\n","Discriminator Loss: 0.6080642804881791\n","Generator Loss: [0.7865040302276611, 0.6923128962516785, 0.04709555581212044]\n","Discriminator Loss: 0.5754785835160874\n","Generator Loss: [0.8372552990913391, 0.8064483404159546, 0.015403484925627708]\n","Discriminator Loss: 0.5730106164955941\n","Generator Loss: [0.8110283017158508, 0.7899531126022339, 0.010537595488131046]\n","Discriminator Loss: 0.5766953895326878\n","Generator Loss: [0.8174418807029724, 0.7953656911849976, 0.011038089171051979]\n","Discriminator Loss: 0.5695798126289446\n","Generator Loss: [0.8163295388221741, 0.7458539009094238, 0.035237811505794525]\n","Discriminator Loss: 0.5601234639834729\n","Generator Loss: [0.739318311214447, 0.7050371170043945, 0.01714060828089714]\n","Discriminator Loss: 0.5678584680754284\n","Generator Loss: [0.7917380332946777, 0.7558783888816833, 0.017929818481206894]\n","Discriminator Loss: 0.5902383737629862\n","Generator Loss: [0.7669015526771545, 0.7224795818328857, 0.0222109854221344]\n","Discriminator Loss: 0.5848118692920252\n","Generator Loss: [0.7769622802734375, 0.7532834410667419, 0.011839411221444607]\n","Discriminator Loss: 0.5792581947571307\n","Generator Loss: [0.8112556338310242, 0.7731466293334961, 0.019054515287280083]\n","Discriminator Loss: 0.5777293973369524\n","Generator Loss: [0.8069747686386108, 0.7807117700576782, 0.013131501153111458]\n","Discriminator Loss: 0.5585174839252431\n","Generator Loss: [0.7839913368225098, 0.764559268951416, 0.00971604511141777]\n","Discriminator Loss: 0.5726589854775739\n","Generator Loss: [0.7551261186599731, 0.7327501773834229, 0.011187956668436527]\n","Discriminator Loss: 0.6209292248386191\n","Generator Loss: [0.8672441244125366, 0.8214335441589355, 0.02290527895092964]\n","Discriminator Loss: 0.5810782122334786\n","Generator Loss: [1.014708399772644, 0.8767059445381165, 0.06900123506784439]\n","Discriminator Loss: 0.6134208664334437\n","Generator Loss: [0.854426383972168, 0.7772350311279297, 0.03859568014740944]\n","Discriminator Loss: 0.6255579069256783\n","Generator Loss: [0.7797476053237915, 0.7601687908172607, 0.009789404459297657]\n","Discriminator Loss: 0.5896016814222094\n","Generator Loss: [0.8525440096855164, 0.8308120965957642, 0.010865949094295502]\n","Discriminator Loss: 0.5614954976354056\n","Generator Loss: [0.8687905073165894, 0.8462682366371155, 0.01126113161444664]\n","Discriminator Loss: 0.6004170071319095\n","Generator Loss: [0.8887030482292175, 0.8633699417114258, 0.012666541151702404]\n","Discriminator Loss: 0.5687692567880731\n","Generator Loss: [0.848514974117279, 0.7898545861244202, 0.0293301809579134]\n","Discriminator Loss: 0.595458417032205\n","Generator Loss: [0.8221174478530884, 0.7948681116104126, 0.013624681159853935]\n","Discriminator Loss: 0.5652325740011293\n","Generator Loss: [0.8532233834266663, 0.8240370750427246, 0.014593163505196571]\n","Discriminator Loss: 0.609063710544433\n","Generator Loss: [0.9809548258781433, 0.8899677991867065, 0.04549350589513779]\n","Discriminator Loss: 0.5775418167904718\n","Generator Loss: [0.825739860534668, 0.7829940915107727, 0.021372880786657333]\n","Discriminator Loss: 0.5835053410264663\n","Generator Loss: [0.8761165738105774, 0.8178107738494873, 0.02915291115641594]\n","Discriminator Loss: 0.5552568935836462\n","Generator Loss: [1.0546765327453613, 0.845392107963562, 0.10464219748973846]\n","Discriminator Loss: 0.5896282114554197\n","Generator Loss: [0.776401698589325, 0.7522103786468506, 0.012095662765204906]\n","Discriminator Loss: 0.5857936727779816\n","Generator Loss: [0.8346112370491028, 0.7511106133460999, 0.04175031930208206]\n","Discriminator Loss: 0.57252463587065\n","Generator Loss: [0.8443102836608887, 0.8208521604537964, 0.01172905508428812]\n","Discriminator Loss: 0.5989248131299973\n","Generator Loss: [0.853083610534668, 0.8319401144981384, 0.010571736842393875]\n","Discriminator Loss: 0.5640626140884706\n","Generator Loss: [1.0376651287078857, 0.8487163186073303, 0.09447439759969711]\n","Discriminator Loss: 0.5834537653900043\n","Generator Loss: [0.8590507507324219, 0.8367422819137573, 0.011154241859912872]\n","Discriminator Loss: 0.5666275803960161\n","Generator Loss: [0.8743218183517456, 0.8420190811157227, 0.016151364892721176]\n","Discriminator Loss: 0.5830825084995013\n","Generator Loss: [0.9091053009033203, 0.844878613948822, 0.03211333975195885]\n","Discriminator Loss: 0.562556458869949\n","Generator Loss: [0.8748063445091248, 0.8364633321762085, 0.019171515479683876]\n","Discriminator Loss: 0.5755986837320961\n","Generator Loss: [0.9029676914215088, 0.8685587644577026, 0.017204459756612778]\n","Discriminator Loss: 0.5704208155075321\n","Generator Loss: [0.8809409737586975, 0.849096417427063, 0.015922265127301216]\n","Discriminator Loss: 0.565189987028134\n","Generator Loss: [0.935139536857605, 0.8866968154907227, 0.024221345782279968]\n","Discriminator Loss: 0.5916537000593962\n","Generator Loss: [0.9248265624046326, 0.8525606989860535, 0.03613293915987015]\n","Discriminator Loss: 0.5587610109178058\n","Generator Loss: [0.910076379776001, 0.8463085889816284, 0.03188391029834747]\n","Discriminator Loss: 0.5850565969540185\n","Generator Loss: [0.9275442957878113, 0.9061093330383301, 0.010717472061514854]\n","Discriminator Loss: 0.5644247026793892\n","Generator Loss: [0.9190202355384827, 0.8947453498840332, 0.012137439101934433]\n","Discriminator Loss: 0.6045467777912563\n","Generator Loss: [0.908165454864502, 0.8668168783187866, 0.020674273371696472]\n","Discriminator Loss: 0.6003323146396724\n","Generator Loss: [0.8969424366950989, 0.8076967000961304, 0.044622864574193954]\n","Discriminator Loss: 0.6306134633341571\n","Generator Loss: [0.7822263836860657, 0.7555029392242432, 0.01336171105504036]\n","Discriminator Loss: 0.6086884706091951\n","Generator Loss: [0.848021388053894, 0.8246514797210693, 0.011684964410960674]\n","Discriminator Loss: 0.5786473878688412\n","Generator Loss: [0.9389140009880066, 0.9150371551513672, 0.011938413605093956]\n","Discriminator Loss: 0.5470801800838672\n","Generator Loss: [1.1001405715942383, 1.0744178295135498, 0.01286138966679573]\n","Discriminator Loss: 0.5698949108118541\n","Generator Loss: [1.081855058670044, 1.0434428453445435, 0.019206102937459946]\n","Discriminator Loss: 0.5793637435563141\n","Generator Loss: [1.0216230154037476, 0.9957107305526733, 0.012956155464053154]\n","Discriminator Loss: 0.6189481356741453\n","Generator Loss: [0.8456245064735413, 0.820981502532959, 0.012321507558226585]\n","Discriminator Loss: 0.6116079531166179\n","Generator Loss: [1.0193125009536743, 0.9934242367744446, 0.01294412650167942]\n","Discriminator Loss: 0.5718387648230419\n","Generator Loss: [1.0240013599395752, 0.9758985042572021, 0.024051405489444733]\n","Discriminator Loss: 0.6119481482310221\n","Generator Loss: [0.8951700329780579, 0.8549733757972717, 0.020098332315683365]\n","Discriminator Loss: 0.619147947334568\n","Generator Loss: [0.8656056523323059, 0.8380720019340515, 0.013766826130449772]\n","Discriminator Loss: 0.5670052898130962\n","Generator Loss: [0.9069792032241821, 0.8638957738876343, 0.021541722118854523]\n","Discriminator Loss: 0.5677041372473468\n","Generator Loss: [0.9532278180122375, 0.9125633835792542, 0.0203322134912014]\n","Discriminator Loss: 0.5628753999408218\n","Generator Loss: [0.9345778226852417, 0.905795693397522, 0.014391071163117886]\n","Discriminator Loss: 0.6200862046098337\n","Generator Loss: [0.8849754333496094, 0.8531790971755981, 0.015898164361715317]\n","Discriminator Loss: 0.5595371604431421\n","Generator Loss: [1.6618390083312988, 0.8175247311592102, 0.4221571385860443]\n","Discriminator Loss: 0.5582484995611594\n","Generator Loss: [0.8920931816101074, 0.865440845489502, 0.013326161541044712]\n","Discriminator Loss: 0.6144140254909871\n","Generator Loss: [0.7659024000167847, 0.7305846810340881, 0.017658859491348267]\n","Discriminator Loss: 0.6326084956526756\n","Generator Loss: [3.509580135345459, 0.8468330502510071, 1.3313735723495483]\n","Discriminator Loss: 0.7792151248049777\n","Generator Loss: [0.8003345727920532, 0.7674387693405151, 0.016447898000478745]\n","Discriminator Loss: 0.8921684517408721\n","Generator Loss: [0.8685125708580017, 0.8444281816482544, 0.012042190879583359]\n","Discriminator Loss: 0.5758601912530139\n","Generator Loss: [0.8016564249992371, 0.754746675491333, 0.02345486544072628]\n","Discriminator Loss: 0.6911795092746615\n","Generator Loss: [0.8992205262184143, 0.8765213489532471, 0.011349581182003021]\n","Discriminator Loss: 0.8366500601259759\n","Generator Loss: [0.7049180865287781, 0.68091881275177, 0.011999636888504028]\n","Discriminator Loss: 0.754814432177227\n","Generator Loss: [0.8553275465965271, 0.8131923675537109, 0.021067583933472633]\n","Discriminator Loss: 0.7753738844767213\n","Generator Loss: [0.6871837973594666, 0.6343603134155273, 0.026411734521389008]\n","Discriminator Loss: 0.8977699466049671\n","Generator Loss: [1.0896915197372437, 1.0295968055725098, 0.03004736267030239]\n","Discriminator Loss: 0.6425127410248024\n","Generator Loss: [1.013433814048767, 0.991339385509491, 0.011047243140637875]\n","Discriminator Loss: 0.7181969574709228\n","Generator Loss: [1.1145190000534058, 0.7673149108886719, 0.17360204458236694]\n","Discriminator Loss: 0.7093833904937128\n","Generator Loss: [1.1380317211151123, 1.0170217752456665, 0.06050494685769081]\n","Discriminator Loss: 0.6272661899602099\n","Generator Loss: [0.8742405772209167, 0.8443220853805542, 0.01495925709605217]\n","Discriminator Loss: 0.7208380101073999\n","Generator Loss: [0.9941294193267822, 0.965923547744751, 0.014102930203080177]\n","Discriminator Loss: 0.6754319035344452\n","Generator Loss: [0.9685606360435486, 0.9390947818756104, 0.014732934534549713]\n","Discriminator Loss: 0.7153869310495793\n","Generator Loss: [0.7699851393699646, 0.7135319113731384, 0.028226621448993683]\n","Discriminator Loss: 0.6219421845016768\n","Generator Loss: [0.9516789317131042, 0.9317262172698975, 0.009976360015571117]\n","Discriminator Loss: 0.6622533015906811\n","Generator Loss: [1.0145996809005737, 0.9437237977981567, 0.03543795272707939]\n","Discriminator Loss: 0.623302703683521\n","Generator Loss: [0.9654150605201721, 0.8150279521942139, 0.07519355416297913]\n","Discriminator Loss: 0.5942112009570337\n","Generator Loss: [0.9892224073410034, 0.8464105129241943, 0.07140593230724335]\n","Discriminator Loss: 0.6198964332839978\n","Generator Loss: [0.8184534311294556, 0.7894817590713501, 0.014485827647149563]\n","Discriminator Loss: 0.6461433962481351\n","Generator Loss: [0.9899157881736755, 0.9016619920730591, 0.04412690922617912]\n","Discriminator Loss: 0.6579602721681113\n","Generator Loss: [0.8659791350364685, 0.8423665761947632, 0.011806284077465534]\n","Discriminator Loss: 0.6554057997557265\n","Generator Loss: [0.8874315619468689, 0.858363687992096, 0.014533945359289646]\n","Discriminator Loss: 0.5630521431125999\n","Generator Loss: [0.9129301309585571, 0.865321159362793, 0.023804470896720886]\n","Discriminator Loss: 0.6240563405629018\n","Generator Loss: [0.7787073254585266, 0.7519326210021973, 0.013387356884777546]\n","Discriminator Loss: 0.5994399438782239\n","Generator Loss: [0.8095964193344116, 0.7665987014770508, 0.02149886079132557]\n","Discriminator Loss: 0.6430466247832101\n","Generator Loss: [0.8143981099128723, 0.7912014722824097, 0.011598309502005577]\n","Discriminator Loss: 0.5660798168680685\n","Generator Loss: [0.8461715579032898, 0.8004732131958008, 0.02284916117787361]\n","Discriminator Loss: 0.6187583118830844\n","Generator Loss: [0.880425751209259, 0.854727566242218, 0.012849087826907635]\n","Discriminator Loss: 0.5781443366515759\n","Generator Loss: [0.888329803943634, 0.8143642544746399, 0.03698277845978737]\n","Discriminator Loss: 0.5935569116554689\n","Generator Loss: [0.8111143708229065, 0.7839758396148682, 0.013569268397986889]\n","Discriminator Loss: 0.616856098968583\n","Generator Loss: [0.8157832026481628, 0.772486686706543, 0.021648254245519638]\n","Discriminator Loss: 0.6115687697379144\n","Generator Loss: [0.9981276988983154, 0.7401546835899353, 0.12898652255535126]\n","Discriminator Loss: 0.5759699105310574\n","Generator Loss: [0.8997716903686523, 0.7854123115539551, 0.057179681956768036]\n","Discriminator Loss: 0.5757563309598481\n","Generator Loss: [0.797008752822876, 0.7515241503715515, 0.02274230681359768]\n","Discriminator Loss: 0.5663286873568723\n","Generator Loss: [0.7846522331237793, 0.7568255662918091, 0.013913321308791637]\n","Discriminator Loss: 0.5771499013171706\n","Generator Loss: [0.8186374306678772, 0.7990659475326538, 0.009785749018192291]\n","Discriminator Loss: 0.5834178145305486\n","Generator Loss: [1.4767742156982422, 0.7767568826675415, 0.35000869631767273]\n","Discriminator Loss: 0.5605371759702393\n","Generator Loss: [0.7700504660606384, 0.7426474094390869, 0.013701530173420906]\n","Discriminator Loss: 0.5888887802748286\n","Generator Loss: [0.7846933007240295, 0.7643202543258667, 0.010186523199081421]\n","Discriminator Loss: 0.5966192275518551\n","Generator Loss: [0.7314801812171936, 0.6907216310501099, 0.02037927880883217]\n","Discriminator Loss: 0.6224573382969538\n","Generator Loss: [0.8522347807884216, 0.8274826407432556, 0.012376081198453903]\n","Discriminator Loss: 0.6636273228268692\n","Generator Loss: [0.8758989572525024, 0.8521456718444824, 0.011876635253429413]\n","Discriminator Loss: 0.5797930736134731\n","Generator Loss: [1.3938477039337158, 1.3666632175445557, 0.01359222549945116]\n","Discriminator Loss: 0.5821003330056556\n","Generator Loss: [1.0460058450698853, 1.0219922065734863, 0.012006793171167374]\n","Discriminator Loss: 0.580749626825309\n","Generator Loss: [1.11194908618927, 1.0503792762756348, 0.03078492544591427]\n","Discriminator Loss: 0.5916053265809751\n","Generator Loss: [0.9181082248687744, 0.8667557835578918, 0.025676218792796135]\n","Discriminator Loss: 0.5771024843361374\n","Generator Loss: [0.9547802209854126, 0.8576038479804993, 0.04858817160129547]\n","Discriminator Loss: 0.607219358702423\n","Generator Loss: [1.0861737728118896, 1.0582890510559082, 0.013942369259893894]\n","Discriminator Loss: 0.5860765461693518\n","Generator Loss: [0.8469756841659546, 0.8248143196105957, 0.011080670170485973]\n","Discriminator Loss: 0.5713841266951931\n","Generator Loss: [0.8567034006118774, 0.8151004910469055, 0.02080145850777626]\n","Discriminator Loss: 0.5706016986532632\n","Generator Loss: [0.8547229766845703, 0.8317129611968994, 0.011504996567964554]\n","Discriminator Loss: 0.5683092424642382\n","Generator Loss: [0.8542947769165039, 0.8323901295661926, 0.010952327400445938]\n","Discriminator Loss: 0.576767694496084\n","Generator Loss: [0.8336549401283264, 0.8120962977409363, 0.010779308155179024]\n","Discriminator Loss: 0.578167603059228\n","Generator Loss: [0.8622652292251587, 0.8121431469917297, 0.025061028078198433]\n","Discriminator Loss: 0.5852319909608923\n","Generator Loss: [0.9016121625900269, 0.7553311586380005, 0.07314051687717438]\n","Discriminator Loss: 0.6203718231681705\n","Generator Loss: [0.8065478205680847, 0.7764285802841187, 0.015059622935950756]\n","Discriminator Loss: 0.5943215831794078\n","Generator Loss: [0.9560645818710327, 0.786075234413147, 0.08499467372894287]\n","Discriminator Loss: 0.5826863149941346\n","Generator Loss: [2.154798746109009, 0.7317497134208679, 0.7115245461463928]\n","Discriminator Loss: 0.5982954395658453\n","Generator Loss: [1.2853507995605469, 0.8065956234931946, 0.23937757313251495]\n","Discriminator Loss: 0.5658044955525838\n","Generator Loss: [0.8160372972488403, 0.7883787751197815, 0.013829261995851994]\n","Discriminator Loss: 0.5736318232584381\n","Generator Loss: [0.8217561841011047, 0.7960599660873413, 0.012848114594817162]\n","Discriminator Loss: 0.5601383747416548\n","Generator Loss: [0.8286962509155273, 0.8069552183151245, 0.010870504193007946]\n","Discriminator Loss: 0.6504538722638245\n","Generator Loss: [0.7978395223617554, 0.7641023993492126, 0.01686856523156166]\n","Discriminator Loss: 0.6126939046498592\n","Generator Loss: [0.76243656873703, 0.7382508516311646, 0.01209285669028759]\n","Discriminator Loss: 0.5901303591745091\n","Generator Loss: [1.1607286930084229, 0.8890897035598755, 0.13581952452659607]\n","Discriminator Loss: 0.6039626842612051\n","Generator Loss: [0.9585052132606506, 0.9321460127830505, 0.013179600238800049]\n","Discriminator Loss: 0.6319404545938596\n","Generator Loss: [0.8924833536148071, 0.8701199293136597, 0.011181704699993134]\n","Discriminator Loss: 0.629730260312499\n","Generator Loss: [0.8483002781867981, 0.7940766215324402, 0.027111820876598358]\n","Discriminator Loss: 0.595072865486145\n","Generator Loss: [0.8248395323753357, 0.7999069690704346, 0.01246628351509571]\n","Discriminator Loss: 0.5902445388542219\n","Generator Loss: [0.9037294387817383, 0.8118326663970947, 0.045948389917612076]\n","Discriminator Loss: 0.6818864605070303\n","Generator Loss: [0.7276714444160461, 0.701662003993988, 0.013004709035158157]\n","Discriminator Loss: 0.6796789302161415\n","Generator Loss: [1.0297693014144897, 0.8096858263015747, 0.11004175990819931]\n","Discriminator Loss: 0.5839952601390905\n","Generator Loss: [0.848143994808197, 0.8255566358566284, 0.011293688789010048]\n","Discriminator Loss: 0.5999761454786494\n","Generator Loss: [0.8332270383834839, 0.8079518675804138, 0.012637576088309288]\n","Discriminator Loss: 0.5822221269409056\n","Generator Loss: [0.850521981716156, 0.8257570266723633, 0.012382478453218937]\n","Discriminator Loss: 0.5821096767231211\n","Generator Loss: [0.985466480255127, 0.8258441686630249, 0.07981115579605103]\n","Discriminator Loss: 0.574888833472869\n","Generator Loss: [0.8556967973709106, 0.8237118721008301, 0.015992475673556328]\n","Discriminator Loss: 0.5648354806944553\n","Generator Loss: [0.870058536529541, 0.8429770469665527, 0.013540740124881268]\n","Discriminator Loss: 0.6108668523938832\n","Generator Loss: [0.850695013999939, 0.8301764726638794, 0.010259266942739487]\n","Discriminator Loss: 0.5737226917844964\n","Generator Loss: [0.8662495613098145, 0.8411532640457153, 0.012548135593533516]\n","Discriminator Loss: 0.5814601901456626\n","Generator Loss: [0.8979209065437317, 0.8371743559837341, 0.030373280867934227]\n","Discriminator Loss: 0.5990242397429029\n","Generator Loss: [0.8009967803955078, 0.7780307531356812, 0.011483028531074524]\n","Discriminator Loss: 0.5877086711352604\n","Generator Loss: [0.8182999491691589, 0.7974838614463806, 0.010408034548163414]\n","Discriminator Loss: 0.5709119478178764\n","Generator Loss: [0.8463243842124939, 0.8253225088119507, 0.010500946082174778]\n","Discriminator Loss: 0.5715403448057259\n","Generator Loss: [0.8561369776725769, 0.8282796144485474, 0.013928670436143875]\n","Discriminator Loss: 0.6125930413436436\n","Generator Loss: [0.8955051302909851, 0.7506768703460693, 0.07241413742303848]\n","Discriminator Loss: 0.6291893117486325\n","Generator Loss: [0.845979630947113, 0.8186517357826233, 0.013663949444890022]\n","Discriminator Loss: 0.5950560318233329\n","Generator Loss: [0.8744316697120667, 0.8442286849021912, 0.015101494267582893]\n","Discriminator Loss: 0.5815292698825942\n","Generator Loss: [0.8920121788978577, 0.8504347205162048, 0.02078874222934246]\n","Discriminator Loss: 0.5802285094450781\n","Generator Loss: [0.8561123609542847, 0.8137749433517456, 0.02116870880126953]\n","Discriminator Loss: 0.5748135105895926\n","Generator Loss: [0.8537512421607971, 0.832491934299469, 0.010629652068018913]\n","Discriminator Loss: 0.5685519992275658\n","Generator Loss: [0.8911803364753723, 0.8483449816703796, 0.021417686715722084]\n","Discriminator Loss: 0.5621706248493865\n","Generator Loss: [0.8636898398399353, 0.8063226342201233, 0.02868358977138996]\n","Discriminator Loss: 0.5616188995845732\n","Generator Loss: [0.8421173095703125, 0.8135545253753662, 0.014281382784247398]\n","Discriminator Loss: 0.5774165224429453\n","Generator Loss: [0.8336538672447205, 0.8067047595977783, 0.01347455196082592]\n","Discriminator Loss: 0.5541163485504512\n","Generator Loss: [0.8433046936988831, 0.8128905296325684, 0.015207071788609028]\n","Discriminator Loss: 0.591039124316012\n","Generator Loss: [0.8041137456893921, 0.7587519884109497, 0.022680863738059998]\n","Discriminator Loss: 0.5813929361356713\n","Generator Loss: [0.85279381275177, 0.8247600197792053, 0.014016886241734028]\n","Discriminator Loss: 0.5629586607883539\n","Generator Loss: [0.8527703285217285, 0.8258899450302124, 0.013440205715596676]\n","Discriminator Loss: 0.5639928179425624\n","Generator Loss: [0.8164542317390442, 0.7902059555053711, 0.013124147430062294]\n","Discriminator Loss: 0.5620946085282412\n","Generator Loss: [0.9057701826095581, 0.8681910037994385, 0.01878960058093071]\n","Discriminator Loss: 0.595715018048395\n","Generator Loss: [0.8593313694000244, 0.8378170728683472, 0.01075713336467743]\n","Discriminator Loss: 0.5723607885729507\n","Generator Loss: [0.8343893885612488, 0.7875033617019653, 0.023443011566996574]\n","Discriminator Loss: 0.5717702569390894\n","Generator Loss: [0.8443679809570312, 0.811587929725647, 0.016390033066272736]\n","Discriminator Loss: 0.5656376774081764\n","Generator Loss: [0.8495899438858032, 0.8201041221618652, 0.014742912724614143]\n","Discriminator Loss: 0.5803485461483433\n","Generator Loss: [0.8492791056632996, 0.8232437372207642, 0.013017694465816021]\n","Discriminator Loss: 0.5523002084851214\n","Generator Loss: [0.8974285125732422, 0.8216845393180847, 0.037871986627578735]\n","Discriminator Loss: 0.6477111162498659\n","Generator Loss: [0.9388173222541809, 0.7627208828926086, 0.08804821968078613]\n","Discriminator Loss: 0.6030545802568668\n","Generator Loss: [0.8644483685493469, 0.842830240726471, 0.010809064842760563]\n","Discriminator Loss: 0.6209636959338241\n","Generator Loss: [0.9385713934898376, 0.8644391298294067, 0.03706613555550575]\n","Discriminator Loss: 0.5926213033453678\n","Generator Loss: [0.8054307699203491, 0.7751059532165527, 0.01516241766512394]\n","Discriminator Loss: 0.5609316033442155\n","Generator Loss: [0.7829142212867737, 0.7640747427940369, 0.009419742971658707]\n","Discriminator Loss: 0.5745498918749945\n","Generator Loss: [0.8076437711715698, 0.7843348979949951, 0.011654435656964779]\n","Discriminator Loss: 0.5629570216515276\n","Generator Loss: [0.8628291487693787, 0.7940713167190552, 0.03437892347574234]\n","Discriminator Loss: 0.5654045583942207\n","Generator Loss: [0.8331804871559143, 0.802166223526001, 0.01550712063908577]\n","Discriminator Loss: 0.5542272349284758\n","Generator Loss: [0.9175401329994202, 0.8299194574356079, 0.04381032660603523]\n","Discriminator Loss: 0.5656844794866629\n","Generator Loss: [0.841357946395874, 0.8173398971557617, 0.012009010650217533]\n","Discriminator Loss: 0.6045315244209633\n","Generator Loss: [0.8633013963699341, 0.817542314529419, 0.022879552096128464]\n","Discriminator Loss: 0.5602421908824908\n","Generator Loss: [0.8543046712875366, 0.8329695463180542, 0.010667563416063786]\n","Discriminator Loss: 0.6225738645007368\n","Generator Loss: [0.926423966884613, 0.8280326128005981, 0.049195677042007446]\n","Discriminator Loss: 0.5650288615852332\n","Generator Loss: [0.8475707769393921, 0.8243951201438904, 0.011587821878492832]\n","Discriminator Loss: 0.576976737998848\n","Generator Loss: [0.8517955541610718, 0.8297421932220459, 0.01102666836231947]\n","Discriminator Loss: 0.5728403709754275\n","Generator Loss: [0.8569920659065247, 0.8223928213119507, 0.017299631610512733]\n","Discriminator Loss: 0.557355173104952\n","Generator Loss: [0.83394855260849, 0.813389778137207, 0.010279376991093159]\n","Discriminator Loss: 0.6876889406303235\n","Generator Loss: [0.8693999648094177, 0.8457653522491455, 0.011817298829555511]\n","Discriminator Loss: 0.5997302831365232\n","Generator Loss: [0.8195475935935974, 0.7993959784507751, 0.010075805708765984]\n","Discriminator Loss: 0.5889046316879103\n","Generator Loss: [0.8249754905700684, 0.7964327335357666, 0.014271387830376625]\n","Discriminator Loss: 0.5616186011466198\n","Generator Loss: [0.819380521774292, 0.7982770800590515, 0.010551707819104195]\n","Discriminator Loss: 0.5582377804121279\n","Generator Loss: [0.8200513124465942, 0.7969311475753784, 0.011560071259737015]\n","Discriminator Loss: 0.6001117555169913\n","Generator Loss: [0.8199121356010437, 0.787196934223175, 0.016357602551579475]\n","Discriminator Loss: 0.6104816943807236\n","Generator Loss: [0.7773648500442505, 0.7500208616256714, 0.01367199793457985]\n","Discriminator Loss: 0.5782518261576115\n","Generator Loss: [0.923554539680481, 0.8113256096839905, 0.056114453822374344]\n","Discriminator Loss: 0.57644842695845\n","Generator Loss: [0.8275052905082703, 0.803086519241333, 0.012209395878016949]\n","Discriminator Loss: 0.5747143645130564\n","Generator Loss: [0.8336804509162903, 0.8016753792762756, 0.016002537682652473]\n","Discriminator Loss: 0.5601424167398363\n","Generator Loss: [0.8504193425178528, 0.8202633857727051, 0.015077986754477024]\n","Discriminator Loss: 0.555715245909596\n","Generator Loss: [0.8647552728652954, 0.8062000274658203, 0.02927761897444725]\n","Discriminator Loss: 0.580503420867899\n","Generator Loss: [0.9376429319381714, 0.818584144115448, 0.05952940508723259]\n","Discriminator Loss: 0.5593556928615726\n","Generator Loss: [0.8161569833755493, 0.7796435356140137, 0.018256712704896927]\n","Discriminator Loss: 0.5408988954695815\n","Generator Loss: [0.9048086404800415, 0.8242801427841187, 0.04026424139738083]\n","Discriminator Loss: 0.5937330766446394\n","Generator Loss: [0.8077741861343384, 0.7835695743560791, 0.012102305889129639]\n","Discriminator Loss: 0.5535441091196844\n","Generator Loss: [0.8003787398338318, 0.7725194692611694, 0.013929645530879498]\n","Discriminator Loss: 0.5598197623585293\n","Generator Loss: [0.8415323495864868, 0.8035550117492676, 0.018988683819770813]\n","Discriminator Loss: 0.5645207720899634\n","Generator Loss: [1.0603880882263184, 0.8010764718055725, 0.12965577840805054]\n","Discriminator Loss: 0.5786897323851008\n","Generator Loss: [0.8158751726150513, 0.7863112688064575, 0.014781944453716278]\n","Discriminator Loss: 0.5962047925449951\n","Generator Loss: [0.8047173023223877, 0.7847270369529724, 0.00999512616544962]\n","Discriminator Loss: 0.5593179034785862\n","Generator Loss: [0.8073843717575073, 0.787121057510376, 0.010131667368113995]\n","Discriminator Loss: 0.577703420069156\n","Generator Loss: [0.7872599363327026, 0.7662053108215332, 0.010527318343520164]\n","Discriminator Loss: 0.5724539075899884\n","Generator Loss: [0.7970709800720215, 0.7698366641998291, 0.013617156073451042]\n","Discriminator Loss: 0.5475032407466642\n","Generator Loss: [0.8084056973457336, 0.7864248752593994, 0.01099039800465107]\n","Discriminator Loss: 0.5701777774920629\n","Generator Loss: [0.856372058391571, 0.7921202182769775, 0.03212590888142586]\n","Discriminator Loss: 0.5955750674256706\n","Generator Loss: [0.8565813302993774, 0.7702125310897827, 0.04318438842892647]\n","Discriminator Loss: 0.565524117948371\n","Generator Loss: [0.8357166051864624, 0.8113969564437866, 0.012159829027950764]\n","Discriminator Loss: 0.5505498487218574\n","Generator Loss: [0.8390441536903381, 0.778320848941803, 0.030361641198396683]\n","Discriminator Loss: 0.5541802675506915\n","Generator Loss: [0.8067219853401184, 0.7282156944274902, 0.03925315663218498]\n","Discriminator Loss: 0.5596804140877794\n","Generator Loss: [0.8035997748374939, 0.7727652788162231, 0.015417247079312801]\n","Discriminator Loss: 0.5781508470317931\n","Generator Loss: [0.7661080360412598, 0.7455035448074341, 0.010302259586751461]\n","Discriminator Loss: 0.5808772276868694\n","Generator Loss: [0.8103640079498291, 0.7876273989677429, 0.01136829424649477]\n","Discriminator Loss: 0.546535223529645\n","Generator Loss: [0.7987748384475708, 0.7755459547042847, 0.011614441871643066]\n","Discriminator Loss: 0.5737114353887591\n","Generator Loss: [1.1530095338821411, 0.8048692941665649, 0.17407011985778809]\n","Discriminator Loss: 0.5450398140164907\n","Generator Loss: [0.8287032842636108, 0.8068755865097046, 0.010913855396211147]\n","Discriminator Loss: 0.5667164205424342\n","Generator Loss: [0.8604785203933716, 0.8262367248535156, 0.01712089404463768]\n","Discriminator Loss: 0.5709655499649671\n","Generator Loss: [0.8280419111251831, 0.8062668442726135, 0.01088754367083311]\n","Discriminator Loss: 0.5920045295497403\n","Generator Loss: [0.7824999094009399, 0.752028226852417, 0.015235856175422668]\n","Discriminator Loss: 0.5618991245428333\n","Generator Loss: [0.8308535218238831, 0.812301754951477, 0.009275871329009533]\n","Discriminator Loss: 0.552348197066749\n","Generator Loss: [0.8205792903900146, 0.7999035120010376, 0.01033789198845625]\n","Discriminator Loss: 0.558651974561144\n","Generator Loss: [0.8283308148384094, 0.784234881401062, 0.022047972306609154]\n","Discriminator Loss: 0.5524077542413579\n","Generator Loss: [0.7725054621696472, 0.7515480518341064, 0.01047870796173811]\n","Discriminator Loss: 0.5987515041597362\n","Generator Loss: [0.800234854221344, 0.7760255932807922, 0.012104636058211327]\n","Discriminator Loss: 0.5451380750528187\n","Generator Loss: [0.8898117542266846, 0.8670792579650879, 0.011366249993443489]\n","Discriminator Loss: 0.5537254370283335\n","Generator Loss: [0.8683111071586609, 0.8363597393035889, 0.015975676476955414]\n","Discriminator Loss: 0.5549833281565952\n","Generator Loss: [0.7950491905212402, 0.775732159614563, 0.009658506140112877]\n","Discriminator Loss: 0.5830361481639557\n","Generator Loss: [0.8607293367385864, 0.8357466459274292, 0.012491344474256039]\n","Discriminator Loss: 0.580836661968533\n","Generator Loss: [0.8249368071556091, 0.7839038372039795, 0.02051648125052452]\n","Discriminator Loss: 0.567196868969404\n","Generator Loss: [0.7859975099563599, 0.7522226572036743, 0.016887441277503967]\n","Discriminator Loss: 0.5645525633262878\n","Generator Loss: [0.8421314358711243, 0.8161059617996216, 0.013012749142944813]\n","Discriminator Loss: 0.5657148507507372\n","Generator Loss: [0.8498849272727966, 0.8190174102783203, 0.015433772467076778]\n","Discriminator Loss: 0.5747305804934513\n","Generator Loss: [0.8130348920822144, 0.7629978656768799, 0.02501850575208664]\n","Discriminator Loss: 0.5614426689026004\n","Generator Loss: [1.054610013961792, 0.8256207704544067, 0.11449459940195084]\n","Discriminator Loss: 0.5657073583488454\n","Generator Loss: [0.8125559091567993, 0.7678341865539551, 0.022360872477293015]\n","Discriminator Loss: 0.5495650785478574\n","Generator Loss: [0.779265820980072, 0.7567341923713684, 0.011265821754932404]\n","Discriminator Loss: 0.5888367169045523\n","Generator Loss: [0.8282458186149597, 0.7912458777427673, 0.018499966710805893]\n","Discriminator Loss: 0.5493863138631241\n","Generator Loss: [0.8341721296310425, 0.8128328323364258, 0.010669653303921223]\n","Discriminator Loss: 0.5894766890078245\n","Generator Loss: [0.7999349236488342, 0.7775588035583496, 0.011188063770532608]\n","Discriminator Loss: 0.5613297868721929\n","Generator Loss: [1.2009276151657104, 0.7965349555015564, 0.20219632983207703]\n","Discriminator Loss: 0.5713287022317672\n","Generator Loss: [0.862925112247467, 0.8326277732849121, 0.015148655511438847]\n","Discriminator Loss: 0.5557242361501267\n","Generator Loss: [0.8148530125617981, 0.7843496799468994, 0.015251673758029938]\n","Discriminator Loss: 0.5642192268005601\n","Generator Loss: [0.8158256411552429, 0.7911033034324646, 0.01236115861684084]\n","Discriminator Loss: 0.5519897854574083\n","Generator Loss: [0.8102031946182251, 0.7720056772232056, 0.01909877359867096]\n","Discriminator Loss: 0.5608018380389694\n","Generator Loss: [0.8968809843063354, 0.790608823299408, 0.05313607677817345]\n","Discriminator Loss: 0.5556940647420561\n","Generator Loss: [0.809478223323822, 0.7884666919708252, 0.010505777783691883]\n","Discriminator Loss: 0.5620861011029774\n","Generator Loss: [0.8143939971923828, 0.7934447526931763, 0.010474619455635548]\n","Discriminator Loss: 0.5621556521273305\n","Generator Loss: [0.7814938426017761, 0.7521268129348755, 0.01468350738286972]\n","Discriminator Loss: 0.5537117805033631\n","Generator Loss: [0.8337001204490662, 0.7890692949295044, 0.02231542579829693]\n","Discriminator Loss: 0.5514471568967565\n","Generator Loss: [0.7238604426383972, 0.701905369758606, 0.010977537371218204]\n","Discriminator Loss: 0.5731136382473778\n","Generator Loss: [0.6651564836502075, 0.6283192038536072, 0.018418651074171066]\n","Discriminator Loss: 0.5671469975441141\n","Generator Loss: [0.8021713495254517, 0.7806849479675293, 0.010743203572928905]\n","Discriminator Loss: 0.5768911577324616\n","Generator Loss: [0.7507577538490295, 0.7176326513290405, 0.016562556847929955]\n","Discriminator Loss: 0.562144348044967\n","Generator Loss: [0.7511953711509705, 0.7274398803710938, 0.01187775656580925]\n","Discriminator Loss: 0.54571802521059\n","Generator Loss: [0.7559891939163208, 0.7331069707870483, 0.011441103182733059]\n","Discriminator Loss: 0.573429062363175\n","Generator Loss: [0.7025840878486633, 0.6794486045837402, 0.011567738838493824]\n","Discriminator Loss: 0.5880988908174913\n","Generator Loss: [0.9383111596107483, 0.8629671931266785, 0.037671975791454315]\n","Discriminator Loss: 0.5635449020628585\n","Generator Loss: [0.9042210578918457, 0.8701626658439636, 0.017029181122779846]\n","Discriminator Loss: 0.5578981146245496\n","Generator Loss: [1.055823564529419, 0.8134543895721436, 0.12118461728096008]\n","Discriminator Loss: 0.5755500247978489\n","Generator Loss: [0.827103316783905, 0.804611086845398, 0.01124610286206007]\n","Discriminator Loss: 0.5577931217849255\n","Generator Loss: [0.8752775192260742, 0.8068627715110779, 0.034207358956336975]\n","Discriminator Loss: 0.5490578708522662\n","Generator Loss: [0.8335155844688416, 0.7967485189437866, 0.018383532762527466]\n","Discriminator Loss: 0.5568732214933334\n","Generator Loss: [0.8153687119483948, 0.7934054732322693, 0.010981610044836998]\n","Discriminator Loss: 0.5716210798873362\n","Generator Loss: [0.807079553604126, 0.7831881046295166, 0.011945709586143494]\n","Discriminator Loss: 0.5588341086204309\n","Generator Loss: [0.7941329479217529, 0.7451125383377075, 0.024510212242603302]\n","Discriminator Loss: 0.5522818889567134\n","Generator Loss: [0.7796240448951721, 0.7579339146614075, 0.010845078155398369]\n","Discriminator Loss: 0.5645787519242731\n","Generator Loss: [0.7351641654968262, 0.7148371934890747, 0.010163474828004837]\n","Discriminator Loss: 0.5539130416727858\n","Generator Loss: [0.8335849642753601, 0.7805335521697998, 0.02652571350336075]\n","Discriminator Loss: 0.5600929185275163\n","Generator Loss: [0.7592058181762695, 0.714580774307251, 0.022312508895993233]\n","Discriminator Loss: 0.5585922685859259\n","Generator Loss: [0.840672492980957, 0.7370471954345703, 0.05181266367435455]\n","Discriminator Loss: 0.5617351313576364\n","Generator Loss: [0.7022140622138977, 0.671242356300354, 0.015485853888094425]\n","Discriminator Loss: 0.564051938470584\n","Generator Loss: [0.8282596468925476, 0.8014640808105469, 0.013397781178355217]\n","Discriminator Loss: 0.5621670172040467\n","Generator Loss: [0.8078456521034241, 0.7824751138687134, 0.012685269117355347]\n","Discriminator Loss: 0.5492078510433203\n","Generator Loss: [0.8392435312271118, 0.7558854222297668, 0.04167906194925308]\n","Discriminator Loss: 0.5543477499213623\n","Generator Loss: [0.8036279678344727, 0.7819108366966248, 0.010858573950827122]\n","Discriminator Loss: 0.5520095116862649\n","Generator Loss: [0.7880247235298157, 0.7563693523406982, 0.015827681869268417]\n","Discriminator Loss: 0.5541451603567111\n","Generator Loss: [0.7924444675445557, 0.7636611461639404, 0.014391651377081871]\n","Discriminator Loss: 0.5491840372560546\n","Generator Loss: [0.8060276508331299, 0.7304180860519409, 0.03780478611588478]\n","Discriminator Loss: 0.5518883704171458\n","Generator Loss: [0.7897831797599792, 0.7644850015640259, 0.012649092823266983]\n","Discriminator Loss: 0.5485335809426033\n","Generator Loss: [0.8039045333862305, 0.7675195932388306, 0.018192464485764503]\n","Discriminator Loss: 0.5540991176021635\n","Generator Loss: [0.7572007775306702, 0.7091736197471619, 0.02401358261704445]\n","Discriminator Loss: 0.5715736044639925\n","Generator Loss: [0.7983159422874451, 0.7736848592758179, 0.01231552753597498]\n","Discriminator Loss: 0.5606834958507534\n","Generator Loss: [0.9567092657089233, 0.744510293006897, 0.10609947144985199]\n","Discriminator Loss: 0.5540536374046496\n","Generator Loss: [0.7999284267425537, 0.7697790861129761, 0.01507467869669199]\n","Discriminator Loss: 0.5532080987468362\n","Generator Loss: [0.8465279340744019, 0.8262602090835571, 0.010133872739970684]\n","Discriminator Loss: 0.5578810293272909\n","Generator Loss: [0.8256399631500244, 0.791415810585022, 0.01711207441985607]\n","Discriminator Loss: 0.5388431729370495\n","Generator Loss: [0.8770204186439514, 0.8480672240257263, 0.014476599171757698]\n","Discriminator Loss: 0.545806108269062\n","Generator Loss: [0.9333926439285278, 0.7952449321746826, 0.06907384842634201]\n","Discriminator Loss: 0.5538714283211448\n","Generator Loss: [0.8930028080940247, 0.8495410680770874, 0.021730875596404076]\n","Discriminator Loss: 0.5924779422657593\n","Generator Loss: [0.8315235376358032, 0.8057693839073181, 0.012877088040113449]\n","Discriminator Loss: 0.5573531637201086\n","Generator Loss: [0.8487036228179932, 0.8062575459480286, 0.021223044022917747]\n","Discriminator Loss: 0.5485995890048798\n","Generator Loss: [0.83783358335495, 0.8092031478881836, 0.014315225183963776]\n","Discriminator Loss: 0.5626210179889313\n","Generator Loss: [0.7453504204750061, 0.722093403339386, 0.011628508567810059]\n","Discriminator Loss: 0.5893258838823385\n","Generator Loss: [0.802945077419281, 0.7781064510345459, 0.012419321574270725]\n","Discriminator Loss: 0.5575533566261583\n","Generator Loss: [0.7707493901252747, 0.7486667633056641, 0.01104130782186985]\n","Discriminator Loss: 0.5676190905214753\n","Generator Loss: [0.8460363745689392, 0.8114453554153442, 0.017295515164732933]\n","Discriminator Loss: 0.5550404038513079\n","Generator Loss: [0.7568149566650391, 0.7343392372131348, 0.01123786624521017]\n","Discriminator Loss: 0.5707875224352392\n","Generator Loss: [0.8211872577667236, 0.7532780170440674, 0.03395460918545723]\n","Discriminator Loss: 0.5504563970962408\n","Generator Loss: [0.740489661693573, 0.7101525664329529, 0.015168555080890656]\n","Discriminator Loss: 0.5786643436877057\n","Generator Loss: [0.9698185920715332, 0.7620269060134888, 0.10389582812786102]\n","Discriminator Loss: 0.5626519838806416\n","Generator Loss: [0.7700504660606384, 0.7474650144577026, 0.011292722076177597]\n","Discriminator Loss: 0.5515310127830162\n","Generator Loss: [0.8153846859931946, 0.7926430702209473, 0.011370809748768806]\n","Discriminator Loss: 0.5567552820721176\n","Generator Loss: [0.778623640537262, 0.7514629364013672, 0.013580341823399067]\n","Discriminator Loss: 0.5832753712202248\n","Generator Loss: [0.8657398223876953, 0.7941740155220032, 0.035782888531684875]\n","Discriminator Loss: 0.5533964000860578\n","Generator Loss: [0.7176366448402405, 0.6961667537689209, 0.01073495950549841]\n","Discriminator Loss: 0.5657942081725196\n","Generator Loss: [0.703172504901886, 0.6699509024620056, 0.016610797494649887]\n","Discriminator Loss: 0.5960631575035222\n","Generator Loss: [0.8207483291625977, 0.7977635860443115, 0.011492375284433365]\n","Discriminator Loss: 0.5527677722639055\n","Generator Loss: [0.8187822103500366, 0.7990124225616455, 0.00988490879535675]\n","Discriminator Loss: 0.555783066623917\n","Generator Loss: [0.8176799416542053, 0.7981635332107544, 0.009758209809660912]\n","Discriminator Loss: 0.5460808028055908\n","Generator Loss: [0.8389269113540649, 0.8009556531906128, 0.018985627219080925]\n","Discriminator Loss: 0.5511458975133792\n","Generator Loss: [0.8132159113883972, 0.7800041437149048, 0.016605887562036514]\n","Discriminator Loss: 0.5531723674794193\n","Generator Loss: [0.9338659644126892, 0.7642276287078857, 0.08481916040182114]\n","Discriminator Loss: 0.5469862237041525\n","Generator Loss: [0.8715866804122925, 0.802735447883606, 0.03442561998963356]\n","Discriminator Loss: 0.5463545895745483\n","Generator Loss: [0.8025617003440857, 0.7447863817214966, 0.028887659311294556]\n","Discriminator Loss: 0.5744183737406274\n","Generator Loss: [0.8249950408935547, 0.7717165946960449, 0.026639219373464584]\n","Discriminator Loss: 0.5578355622274103\n","Generator Loss: [0.8250618577003479, 0.7887605428695679, 0.018150663003325462]\n","Discriminator Loss: 0.5521208926220424\n","Generator Loss: [1.9259707927703857, 0.8145758509635925, 0.555697500705719]\n","Discriminator Loss: 0.5356336856348207\n","Generator Loss: [0.9677377939224243, 0.927863359451294, 0.019937217235565186]\n","Discriminator Loss: 0.5879894504723779\n","Generator Loss: [0.8231142163276672, 0.7940181493759155, 0.014548022300004959]\n","Discriminator Loss: 0.5491320278488274\n","Generator Loss: [0.7948378920555115, 0.7661485075950623, 0.014344695024192333]\n","Discriminator Loss: 0.5460569349888829\n","Generator Loss: [0.8393017053604126, 0.8183872699737549, 0.01045722421258688]\n","Discriminator Loss: 0.546845166491039\n","Generator Loss: [0.9411274790763855, 0.9046410322189331, 0.01824321411550045]\n","Discriminator Loss: 0.5498138748462225\n","Generator Loss: [1.05977463722229, 0.8540908098220825, 0.10284194350242615]\n","Discriminator Loss: 0.5488217090387479\n","Generator Loss: [0.825315535068512, 0.7941392064094543, 0.01558815035969019]\n","Discriminator Loss: 0.5672926932747941\n","Generator Loss: [0.8477120995521545, 0.8175682425498962, 0.015071935020387173]\n","Discriminator Loss: 0.5341883624241746\n","Generator Loss: [1.141432285308838, 0.8286150693893433, 0.1564086377620697]\n","Discriminator Loss: 0.5478819223362734\n","Generator Loss: [0.7179036140441895, 0.688398003578186, 0.014752810820937157]\n","Discriminator Loss: 0.5626184995903714\n","Generator Loss: [0.9054584503173828, 0.8508018255233765, 0.027328316122293472]\n","Discriminator Loss: 0.5510343520863898\n","Generator Loss: [0.915542483329773, 0.8913463354110718, 0.012098066508769989]\n","Discriminator Loss: 0.5534877327709182\n","Generator Loss: [0.8631135821342468, 0.811610758304596, 0.02575140818953514]\n","Discriminator Loss: 0.5485759640496326\n","Generator Loss: [0.8816967606544495, 0.8444107174873352, 0.01864301972091198]\n","Discriminator Loss: 0.54159774798336\n","Generator Loss: [0.854733943939209, 0.8322880268096924, 0.01122294832020998]\n","Discriminator Loss: 0.6211578499305688\n","Generator Loss: [1.0228906869888306, 0.8329906463623047, 0.09495003521442413]\n","Discriminator Loss: 0.5540959749669128\n","Generator Loss: [0.8194383978843689, 0.7637344598770142, 0.027851957827806473]\n","Discriminator Loss: 0.5635784603873617\n","Generator Loss: [0.8276338577270508, 0.7753775119781494, 0.02612815797328949]\n","Discriminator Loss: 0.5551278953807923\n","Generator Loss: [0.8040808439254761, 0.7748970985412598, 0.014591874554753304]\n","Discriminator Loss: 0.5578267744276673\n","Generator Loss: [0.8460505604743958, 0.823930561542511, 0.011060012504458427]\n","Discriminator Loss: 0.5644855189384543\n","Generator Loss: [0.8045501112937927, 0.7831757664680481, 0.01068717148154974]\n","Discriminator Loss: 0.5782031938433647\n","Generator Loss: [0.8256034255027771, 0.8025804758071899, 0.011511486954987049]\n","Discriminator Loss: 0.6638677086812095\n","Generator Loss: [0.7833001613616943, 0.7610507011413574, 0.011124718002974987]\n","Discriminator Loss: 0.7430470858371336\n","Generator Loss: [0.7251697182655334, 0.7050738334655762, 0.010047949850559235]\n","Discriminator Loss: 0.7032716555986553\n","Generator Loss: [0.8181673884391785, 0.7978302836418152, 0.010168563574552536]\n","Discriminator Loss: 0.6676825694739819\n","Generator Loss: [0.9415790438652039, 0.9213576912879944, 0.01011067908257246]\n","Discriminator Loss: 0.8324798927933443\n","Generator Loss: [0.8557913303375244, 0.8305176496505737, 0.012636840343475342]\n","Discriminator Loss: 0.6862441236153245\n","Generator Loss: [1.7450902462005615, 1.7200086116790771, 0.012540828436613083]\n","Discriminator Loss: 0.6620522098965012\n","Generator Loss: [1.271736979484558, 1.2450881004333496, 0.013324419036507607]\n","Discriminator Loss: 0.6347097689285874\n","Generator Loss: [1.0405062437057495, 1.010045051574707, 0.015230596996843815]\n","Discriminator Loss: 0.7692239468051412\n","Generator Loss: [0.9139996767044067, 0.8880044221878052, 0.012997627258300781]\n","Discriminator Loss: 0.7883325617149239\n","Generator Loss: [3.8527469635009766, 3.622267007827759, 0.1152399480342865]\n","Discriminator Loss: 0.6888949225467513\n","Generator Loss: [1.0768200159072876, 0.9590444564819336, 0.058887779712677]\n","Discriminator Loss: 0.5971632753498852\n","Generator Loss: [0.9725638628005981, 0.9056797027587891, 0.03344208374619484]\n","Discriminator Loss: 0.5979385938335327\n","Generator Loss: [0.9371368288993835, 0.9084057807922363, 0.014365511015057564]\n","Discriminator Loss: 0.6160985739843454\n","Generator Loss: [1.2122886180877686, 1.1688681840896606, 0.02171020396053791]\n","Discriminator Loss: 0.5924129796039779\n","Generator Loss: [0.9953300356864929, 0.9483967423439026, 0.02346663549542427]\n","Discriminator Loss: 0.6346847651147982\n","Generator Loss: [1.038016676902771, 1.0083407163619995, 0.01483798399567604]\n","Discriminator Loss: 0.6502082448569126\n","Generator Loss: [0.8731645345687866, 0.8417569994926453, 0.01570376195013523]\n","Discriminator Loss: 0.6196858484763652\n","Generator Loss: [0.945472002029419, 0.8713114261627197, 0.037080295383930206]\n","Discriminator Loss: 0.6866599642235087\n","Generator Loss: [0.9497810006141663, 0.9253514409065247, 0.01221476960927248]\n","Discriminator Loss: 0.6211671853561711\n","Generator Loss: [0.9163398146629333, 0.8754522800445557, 0.020443761721253395]\n","Discriminator Loss: 0.5823318854891113\n","Generator Loss: [0.822482168674469, 0.7952343225479126, 0.013623926788568497]\n","Discriminator Loss: 0.591205515913316\n","Generator Loss: [0.9457371234893799, 0.9244945049285889, 0.010621313005685806]\n","Discriminator Loss: 0.5818579997867346\n","Generator Loss: [0.8332827091217041, 0.8095579147338867, 0.011862390674650669]\n","Discriminator Loss: 0.5688197657946148\n","Generator Loss: [0.8698576092720032, 0.8455641269683838, 0.012146741151809692]\n","Discriminator Loss: 0.5830090422241483\n","Generator Loss: [0.9934102296829224, 0.967945396900177, 0.012732401490211487]\n","Discriminator Loss: 0.5711180500074988\n","Generator Loss: [0.8956829309463501, 0.8641353249549866, 0.01577380672097206]\n","Discriminator Loss: 0.5613961182498315\n","Generator Loss: [0.8663957118988037, 0.8383618593215942, 0.014016921631991863]\n","Discriminator Loss: 0.5621283190921531\n","Generator Loss: [0.7676629424095154, 0.7368086576461792, 0.01542715635150671]\n","Discriminator Loss: 0.6062357429982512\n","Generator Loss: [0.8603630661964417, 0.8274511098861694, 0.016455968841910362]\n","Discriminator Loss: 0.56228349700541\n","Generator Loss: [0.9029822945594788, 0.8524988889694214, 0.025241704657673836]\n","Discriminator Loss: 0.5578398208308499\n","Generator Loss: [0.8760243058204651, 0.8495628833770752, 0.013230704702436924]\n","Discriminator Loss: 0.585576631128788\n","Generator Loss: [0.8808579444885254, 0.8531665205955505, 0.013845698907971382]\n","Discriminator Loss: 0.5729839391860878\n","Generator Loss: [0.8820863962173462, 0.8510043621063232, 0.01554101426154375]\n","Discriminator Loss: 0.5545865511157899\n","Generator Loss: [0.8523255586624146, 0.821481466293335, 0.015422041527926922]\n","Discriminator Loss: 0.5676873595348297\n","Generator Loss: [0.8649996519088745, 0.8441535830497742, 0.010423029772937298]\n","Discriminator Loss: 0.5724826517835027\n","Generator Loss: [0.8599202036857605, 0.8328922986984253, 0.013513956218957901]\n","Discriminator Loss: 0.5728822939818201\n","Generator Loss: [0.8695126175880432, 0.8240674138069153, 0.022722594439983368]\n","Discriminator Loss: 0.5858622358682624\n","Generator Loss: [0.8663673996925354, 0.7784774303436279, 0.043944988399744034]\n","Discriminator Loss: 0.5681596484555484\n","Generator Loss: [0.8437497615814209, 0.8245477676391602, 0.009600993245840073]\n","Discriminator Loss: 0.6561961467523361\n","Generator Loss: [0.7951771020889282, 0.7625433802604675, 0.01631687581539154]\n","Discriminator Loss: 0.5847831422506715\n","Generator Loss: [0.7817170023918152, 0.7593271136283875, 0.011194935068488121]\n","Discriminator Loss: 0.5635426581720822\n","Generator Loss: [0.8323120474815369, 0.7919933795928955, 0.020159320905804634]\n","Discriminator Loss: 0.5654445817999658\n","Generator Loss: [0.871773898601532, 0.8511922359466553, 0.010290817357599735]\n","Discriminator Loss: 0.5978466616288642\n","Generator Loss: [0.9522742629051208, 0.7895327210426331, 0.0813707634806633]\n","Discriminator Loss: 0.5778207249095431\n","Generator Loss: [0.847507894039154, 0.8157944679260254, 0.015856705605983734]\n","Discriminator Loss: 0.5598458538297564\n","Generator Loss: [0.8316150307655334, 0.7780579328536987, 0.026778558269143105]\n","Discriminator Loss: 0.5602620099671185\n","Generator Loss: [0.8615638613700867, 0.8375275135040283, 0.01201816089451313]\n","Discriminator Loss: 0.5566581856419361\n","Generator Loss: [0.8732284903526306, 0.843782901763916, 0.014722785912454128]\n","Discriminator Loss: 0.5759012529015308\n","Generator Loss: [0.8253833651542664, 0.7864484786987305, 0.019467435777187347]\n","Discriminator Loss: 0.572080416703102\n","Generator Loss: [0.7915661931037903, 0.762911319732666, 0.01432743389159441]\n","Discriminator Loss: 0.5531722886626085\n","Generator Loss: [1.0297551155090332, 0.781651496887207, 0.1240517869591713]\n","Discriminator Loss: 0.5525327256764285\n","Generator Loss: [0.8405324220657349, 0.8117290735244751, 0.01440167985856533]\n","Discriminator Loss: 0.5471742689987877\n","Generator Loss: [0.8676552772521973, 0.8377386927604675, 0.014958294108510017]\n","Discriminator Loss: 0.5899792586133117\n","Generator Loss: [0.7873281240463257, 0.7663223743438721, 0.010502873919904232]\n","Discriminator Loss: 0.5865092769527109\n","Generator Loss: [0.8775476813316345, 0.8567896485328674, 0.010379022918641567]\n","Discriminator Loss: 0.621037352251733\n","Generator Loss: [0.8568216562271118, 0.8276658058166504, 0.014577929861843586]\n","Discriminator Loss: 0.5710089557032916\n","Generator Loss: [1.6246838569641113, 0.8197208046913147, 0.4024814963340759]\n","Discriminator Loss: 0.5551292544259923\n","Generator Loss: [0.8292967677116394, 0.8032824397087097, 0.01300717517733574]\n","Discriminator Loss: 0.5644801454764092\n","Generator Loss: [0.8258647918701172, 0.7840101718902588, 0.02092730812728405]\n","Discriminator Loss: 0.5608379550103564\n","Generator Loss: [0.7760001420974731, 0.7550432682037354, 0.010478443466126919]\n","Discriminator Loss: 0.565594147059528\n","Generator Loss: [0.8003523945808411, 0.77667635679245, 0.01183802168816328]\n","Discriminator Loss: 0.5270046606492542\n","Generator Loss: [0.8679061532020569, 0.8294171094894409, 0.019244516268372536]\n","Discriminator Loss: 0.5590776711633225\n","Generator Loss: [0.8469703793525696, 0.7908649444580078, 0.02805272862315178]\n","Discriminator Loss: 0.5582179527627886\n","Generator Loss: [0.8101885914802551, 0.7764428853988647, 0.016872845590114594]\n","Discriminator Loss: 0.5817566206796982\n","Generator Loss: [0.8531585931777954, 0.8314023017883301, 0.010878150351345539]\n","Discriminator Loss: 0.5820185632001085\n","Generator Loss: [0.8782166242599487, 0.7823809385299683, 0.04791785404086113]\n","Discriminator Loss: 0.5827481869964686\n","Generator Loss: [0.8930809497833252, 0.8279848098754883, 0.03254805505275726]\n","Discriminator Loss: 0.5758719194946025\n","Generator Loss: [0.9459761381149292, 0.8103257417678833, 0.06782519072294235]\n","Discriminator Loss: 0.5589241272609797\n","Generator Loss: [4.316444396972656, 0.818678617477417, 1.7488828897476196]\n","Discriminator Loss: 0.5532208410586463\n","Generator Loss: [0.8448965549468994, 0.8237390518188477, 0.010578744113445282]\n","Discriminator Loss: 0.5627781380171655\n","Generator Loss: [0.8684154748916626, 0.8358709812164307, 0.016272258013486862]\n","Discriminator Loss: 0.6111489849372447\n","Generator Loss: [0.8264703750610352, 0.8035837411880493, 0.011443302035331726]\n","Discriminator Loss: 0.5670129157942938\n","Generator Loss: [0.8253182768821716, 0.803342342376709, 0.010987954214215279]\n","Discriminator Loss: 0.5596029496118717\n","Generator Loss: [0.8086080551147461, 0.7841283082962036, 0.012239878997206688]\n","Discriminator Loss: 0.6234054750875657\n","Generator Loss: [0.8201953172683716, 0.7851905822753906, 0.017502371221780777]\n","Discriminator Loss: 0.5453852472473955\n","Generator Loss: [0.9258664846420288, 0.8080487847328186, 0.0589088574051857]\n","Discriminator Loss: 0.5428325188331655\n","Generator Loss: [0.8291494250297546, 0.7956415414810181, 0.01675393246114254]\n","Discriminator Loss: 0.5561073525132088\n","Generator Loss: [1.0256918668746948, 0.8026426434516907, 0.11152461171150208]\n","Discriminator Loss: 0.5776705155203672\n","Generator Loss: [0.8208133578300476, 0.7996893525123596, 0.010561998002231121]\n","Discriminator Loss: 0.5546798448740446\n","Generator Loss: [0.8185318112373352, 0.7941833734512329, 0.012174205854535103]\n","Discriminator Loss: 0.5777407142959419\n","Generator Loss: [0.8593674898147583, 0.7741490602493286, 0.042609211057424545]\n","Discriminator Loss: 0.5776854905907385\n","Generator Loss: [0.8259816765785217, 0.7735253572463989, 0.026228148490190506]\n","Discriminator Loss: 0.5697991167980945\n","Generator Loss: [0.8464730978012085, 0.754160463809967, 0.04615631699562073]\n","Discriminator Loss: 0.5653931639171788\n","Generator Loss: [0.8205140829086304, 0.7833619117736816, 0.01857607066631317]\n","Discriminator Loss: 0.557572476716814\n","Generator Loss: [1.2648561000823975, 0.7891695499420166, 0.23784326016902924]\n","Discriminator Loss: 0.5660329483471287\n","Generator Loss: [0.7810196876525879, 0.7600865960121155, 0.010466530919075012]\n","Discriminator Loss: 0.5554888488150027\n","Generator Loss: [1.0410125255584717, 0.800421953201294, 0.12029528617858887]\n","Discriminator Loss: 0.5483141809381777\n","Generator Loss: [0.8170155882835388, 0.79473876953125, 0.011138404719531536]\n","Discriminator Loss: 0.5513585958251497\n","Generator Loss: [0.8416264653205872, 0.8114964962005615, 0.015064972452819347]\n","Discriminator Loss: 0.5558419716289791\n","Generator Loss: [0.8449946045875549, 0.8197478652000427, 0.012623381800949574]\n","Discriminator Loss: 0.5738129555320484\n","Generator Loss: [0.8110048770904541, 0.7832735776901245, 0.013865641318261623]\n","Discriminator Loss: 0.5584742177088629\n","Generator Loss: [0.8501287698745728, 0.8240041732788086, 0.013062283396720886]\n","Discriminator Loss: 0.5520128380576352\n","Generator Loss: [0.847308874130249, 0.8200433850288391, 0.013632739894092083]\n","Discriminator Loss: 0.5768775400065351\n","Generator Loss: [0.8327115178108215, 0.8022019863128662, 0.015254759229719639]\n","Discriminator Loss: 0.560817928513643\n","Generator Loss: [0.8352596163749695, 0.8050702810287476, 0.015094678848981857]\n","Discriminator Loss: 0.5658342273854942\n","Generator Loss: [0.8050163984298706, 0.7857416868209839, 0.009637370705604553]\n","Discriminator Loss: 0.5481646780099254\n","Generator Loss: [0.820667564868927, 0.7878514528274536, 0.016408057883381844]\n","Discriminator Loss: 0.5439664286896004\n","Generator Loss: [0.8245711922645569, 0.7917592525482178, 0.01640595868229866]\n","Discriminator Loss: 0.552204511810487\n","Generator Loss: [0.8000056743621826, 0.7763251066207886, 0.011840282939374447]\n","Discriminator Loss: 0.5748665558930952\n","Generator Loss: [0.8071750402450562, 0.7805408835411072, 0.013317093253135681]\n","Discriminator Loss: 0.563519807306875\n","Generator Loss: [0.8050395250320435, 0.778462290763855, 0.013288610614836216]\n","Discriminator Loss: 0.5601709793609189\n","Generator Loss: [0.8300418853759766, 0.7926880121231079, 0.018676942214369774]\n","Discriminator Loss: 0.5755167583411094\n","Generator Loss: [0.8157842755317688, 0.7926609516143799, 0.011561654508113861]\n","Discriminator Loss: 0.5640108820753085\n","Generator Loss: [0.8071661591529846, 0.7876709699630737, 0.009747598320245743]\n","Discriminator Loss: 0.5538735760201234\n","Generator Loss: [0.8326156735420227, 0.7832272052764893, 0.02469423972070217]\n","Discriminator Loss: 0.553353266135673\n","Generator Loss: [0.8212012052536011, 0.7960798740386963, 0.012560669332742691]\n","Discriminator Loss: 0.549212474925298\n","Generator Loss: [0.8071313500404358, 0.7812997102737427, 0.012915818020701408]\n","Discriminator Loss: 0.5623578002268914\n","Generator Loss: [0.8398383259773254, 0.8061747550964355, 0.016831789165735245]\n","Discriminator Loss: 0.544122351844635\n","Generator Loss: [0.8214502930641174, 0.7949766516685486, 0.013236808590590954]\n","Discriminator Loss: 0.5325988926924765\n","Generator Loss: [0.8700075745582581, 0.8342750668525696, 0.01786624640226364]\n","Discriminator Loss: 0.5428104676448129\n","Generator Loss: [0.8440567851066589, 0.8223724365234375, 0.010842161253094673]\n","Discriminator Loss: 0.5962629488767561\n","Generator Loss: [0.825947105884552, 0.8027158379554749, 0.011615642346441746]\n","Discriminator Loss: 0.5754221736988256\n","Generator Loss: [0.8920861482620239, 0.8281838893890381, 0.031951140612363815]\n","Discriminator Loss: 0.5456263761971059\n","Generator Loss: [0.8451335430145264, 0.816899299621582, 0.014117129147052765]\n","Discriminator Loss: 0.5818845674020849\n","Generator Loss: [1.0629754066467285, 0.8123669624328613, 0.1253041923046112]\n","Discriminator Loss: 0.5629700736972154\n","Generator Loss: [0.8338972330093384, 0.8034378886222839, 0.015229682438075542]\n","Discriminator Loss: 0.5501680479274\n","Generator Loss: [0.8563151955604553, 0.8198325037956238, 0.018241334706544876]\n","Discriminator Loss: 0.543959210279354\n","Generator Loss: [0.8315649032592773, 0.8098718523979187, 0.010846518911421299]\n","Discriminator Loss: 0.5460120209027082\n","Generator Loss: [0.8213765025138855, 0.7901232838630676, 0.01562660187482834]\n","Discriminator Loss: 0.5655980878609626\n","Generator Loss: [0.8460684418678284, 0.8236098289489746, 0.011229309253394604]\n","Discriminator Loss: 0.5404563373558631\n","Generator Loss: [0.9473896622657776, 0.8267528414726257, 0.060318417847156525]\n","Discriminator Loss: 0.55548304846252\n","Generator Loss: [0.8516650199890137, 0.8219245672225952, 0.01487023662775755]\n","Discriminator Loss: 0.5447023690066999\n","Generator Loss: [0.8309618234634399, 0.8075211048126221, 0.01172037422657013]\n","Discriminator Loss: 0.5514992161115515\n","Generator Loss: [1.4798352718353271, 0.8091837763786316, 0.3353257477283478]\n","Discriminator Loss: 0.5479112436105424\n","Generator Loss: [0.9844236373901367, 0.8014987707138062, 0.09146243333816528]\n","Discriminator Loss: 0.558507567484412\n","Generator Loss: [0.8524538278579712, 0.8243989944458008, 0.014027410186827183]\n","Discriminator Loss: 0.5552585920340789\n","Generator Loss: [4.295564651489258, 0.7910473346710205, 1.7522587776184082]\n","Discriminator Loss: 0.570228283682809\n","Generator Loss: [0.8339003920555115, 0.8094814419746399, 0.012209481559693813]\n","Discriminator Loss: 0.5449046157027624\n","Generator Loss: [0.8257924914360046, 0.7989668846130371, 0.01341281644999981]\n","Discriminator Loss: 0.5448518518578567\n","Generator Loss: [1.014846920967102, 0.8057999610900879, 0.10452345758676529]\n","Discriminator Loss: 0.5578706614032853\n","Generator Loss: [0.9334073066711426, 0.7972607612609863, 0.06807327270507812]\n","Discriminator Loss: 0.5441722129980917\n","Generator Loss: [0.8286021947860718, 0.8027589321136475, 0.012921642512083054]\n","Discriminator Loss: 0.5590413166974031\n","Generator Loss: [0.8183112740516663, 0.7942460775375366, 0.012032604776322842]\n","Discriminator Loss: 0.5482431874224858\n","Generator Loss: [0.8173595666885376, 0.796099066734314, 0.010630262084305286]\n","Discriminator Loss: 0.5519300280993775\n","Generator Loss: [0.8218002319335938, 0.7931520342826843, 0.01432410255074501]\n","Discriminator Loss: 0.5608112208747116\n","Generator Loss: [0.8478788733482361, 0.8006919622421265, 0.023593466728925705]\n","Discriminator Loss: 0.546280545720947\n","Generator Loss: [0.8185094594955444, 0.7965320944786072, 0.010988684371113777]\n","Discriminator Loss: 0.5451953878118729\n","Generator Loss: [0.8197457194328308, 0.7929404973983765, 0.013402598910033703]\n","Discriminator Loss: 0.5407536918683036\n","Generator Loss: [1.0982513427734375, 0.7964993715286255, 0.150875985622406]\n","Discriminator Loss: 0.5512177601885924\n","Generator Loss: [0.8191338181495667, 0.7889347076416016, 0.015099565498530865]\n","Discriminator Loss: 0.5462103440258943\n","Generator Loss: [0.8118206262588501, 0.786298394203186, 0.012761124409735203]\n","Discriminator Loss: 0.5497849446246619\n","Generator Loss: [0.8463670611381531, 0.8036681413650513, 0.021349459886550903]\n","Discriminator Loss: 0.5452010321077978\n","Generator Loss: [0.8658785820007324, 0.8014044165611267, 0.03223709762096405]\n","Discriminator Loss: 0.5538351441809937\n","Generator Loss: [0.8405299782752991, 0.8077296018600464, 0.016400175169110298]\n","Discriminator Loss: 0.5581256053883408\n","Generator Loss: [0.8261757493019104, 0.8054182529449463, 0.01037875097244978]\n","Discriminator Loss: 0.5486675192987605\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 3/10 [1:14:21<2:23:50, 1232.87s/it]"]},{"name":"stdout","output_type":"stream","text":["Generator Loss: [0.835989773273468, 0.8087340593338013, 0.013627846725285053]\n","Epoch: 3\n","Discriminator Loss: 0.5460218234493368\n","Generator Loss: [0.8251730799674988, 0.798560619354248, 0.01330622285604477]\n","Discriminator Loss: 0.5436287211805393\n","Generator Loss: [0.8317883014678955, 0.7987315058708191, 0.016528408974409103]\n","Discriminator Loss: 0.5609508642514811\n","Generator Loss: [0.8400421738624573, 0.8155809640884399, 0.01223060954362154]\n","Discriminator Loss: 0.5500264206984866\n","Generator Loss: [0.8308572769165039, 0.790977418422699, 0.01993992179632187]\n","Discriminator Loss: 0.5645290446827858\n","Generator Loss: [1.0725563764572144, 0.8019933700561523, 0.135281503200531]\n","Discriminator Loss: 0.5645344222066342\n","Generator Loss: [1.1571097373962402, 0.770439624786377, 0.19333507120609283]\n","Discriminator Loss: 0.5495771557289117\n","Generator Loss: [0.8054878115653992, 0.7737022638320923, 0.015892785042524338]\n","Discriminator Loss: 0.5474616489955224\n","Generator Loss: [0.8119798898696899, 0.778847336769104, 0.016566287726163864]\n","Discriminator Loss: 0.5563691059724079\n","Generator Loss: [0.8676443099975586, 0.7732261419296265, 0.04720906913280487]\n","Discriminator Loss: 0.5549240915424889\n","Generator Loss: [0.8551883101463318, 0.7760204076766968, 0.0395839549601078]\n","Discriminator Loss: 0.5512479203998737\n","Generator Loss: [0.8288168907165527, 0.7746926546096802, 0.027062103152275085]\n","Discriminator Loss: 0.5438467702533671\n","Generator Loss: [0.7962974905967712, 0.7693052291870117, 0.013496123254299164]\n","Discriminator Loss: 0.5616209518029791\n","Generator Loss: [0.8138436079025269, 0.7779597640037537, 0.017941914498806]\n","Discriminator Loss: 0.5606911414615752\n","Generator Loss: [0.8068703413009644, 0.7777665257453918, 0.014551903121173382]\n","Discriminator Loss: 0.5473476335728265\n","Generator Loss: [0.8302902579307556, 0.758240818977356, 0.03602471202611923]\n","Discriminator Loss: 0.5480644612171091\n","Generator Loss: [0.8141454458236694, 0.773009717464447, 0.020567867904901505]\n","Discriminator Loss: 0.5665904966190283\n","Generator Loss: [0.8231663107872009, 0.7885069251060486, 0.017329683527350426]\n","Discriminator Loss: 0.5619733959101723\n","Generator Loss: [0.7877224683761597, 0.7660069465637207, 0.010857747867703438]\n","Discriminator Loss: 0.5515201265443466\n","Generator Loss: [0.8044650554656982, 0.7623547315597534, 0.021055173128843307]\n","Discriminator Loss: 0.5447761747054756\n","Generator Loss: [0.8008815050125122, 0.7739846706390381, 0.013448408804833889]\n","Discriminator Loss: 0.5452680439304913\n","Generator Loss: [0.8423791527748108, 0.7611809968948364, 0.040599070489406586]\n","Discriminator Loss: 0.5425338862896751\n","Generator Loss: [0.8124377131462097, 0.7855786085128784, 0.013429565355181694]\n","Discriminator Loss: 0.5500958762945629\n","Generator Loss: [0.855816662311554, 0.773735523223877, 0.0410405732691288]\n","Discriminator Loss: 0.5751867046387815\n","Generator Loss: [0.7770295143127441, 0.7361911535263062, 0.02041919343173504]\n","Discriminator Loss: 0.5642578598271939\n","Generator Loss: [0.8027148246765137, 0.7777572274208069, 0.012478810735046864]\n","Discriminator Loss: 0.5530351252755281\n","Generator Loss: [0.863001823425293, 0.7714090943336487, 0.04579634964466095]\n","Discriminator Loss: 0.542047528517287\n","Generator Loss: [0.8146833181381226, 0.7897467613220215, 0.012468263506889343]\n","Discriminator Loss: 0.5652110068945149\n","Generator Loss: [0.771051287651062, 0.7484780550003052, 0.011286620981991291]\n","Discriminator Loss: 0.5837507219393956\n","Generator Loss: [0.7992950081825256, 0.7781417369842529, 0.01057662907987833]\n","Discriminator Loss: 0.5765498954879149\n","Generator Loss: [0.795363187789917, 0.775580644607544, 0.00989126693457365]\n","Discriminator Loss: 0.5567751725343442\n","Generator Loss: [0.7957375645637512, 0.7715489268302917, 0.01209430955350399]\n","Discriminator Loss: 0.5430591718668438\n","Generator Loss: [0.8031344413757324, 0.7715710401535034, 0.015781698748469353]\n","Discriminator Loss: 0.557622298158094\n","Generator Loss: [0.7904954552650452, 0.7648382782936096, 0.012828578241169453]\n","Discriminator Loss: 0.5598849229809275\n","Generator Loss: [0.7770695686340332, 0.7560106515884399, 0.010529451072216034]\n","Discriminator Loss: 0.5595248528152297\n","Generator Loss: [0.7751775979995728, 0.7505906820297241, 0.012293452396988869]\n","Discriminator Loss: 0.5520660696001869\n","Generator Loss: [0.8144527673721313, 0.7767093181610107, 0.018871713429689407]\n","Discriminator Loss: 0.5424395101599657\n","Generator Loss: [0.8231148719787598, 0.7848454713821411, 0.019134704023599625]\n","Discriminator Loss: 0.5465626994846389\n","Generator Loss: [0.7980851531028748, 0.7796478271484375, 0.009218664839863777]\n","Discriminator Loss: 0.5550957345440111\n","Generator Loss: [0.8909575343132019, 0.7680454254150391, 0.06145605817437172]\n","Discriminator Loss: 0.5507238378195325\n","Generator Loss: [0.7997714877128601, 0.7769101858139038, 0.011430658400058746]\n","Discriminator Loss: 0.5619022053160734\n","Generator Loss: [0.8087144494056702, 0.7803693413734436, 0.014172566123306751]\n","Discriminator Loss: 0.5718074208180042\n","Generator Loss: [0.7581127882003784, 0.7162883877754211, 0.020912187173962593]\n","Discriminator Loss: 0.5902111908671941\n","Generator Loss: [0.8116053938865662, 0.7870100140571594, 0.012297676876187325]\n","Discriminator Loss: 0.5553931680733513\n","Generator Loss: [0.7889801859855652, 0.7624842524528503, 0.013247976079583168]\n","Discriminator Loss: 0.5672164993179649\n","Generator Loss: [0.8076759576797485, 0.7861904501914978, 0.010742738842964172]\n","Discriminator Loss: 0.5477119854622288\n","Generator Loss: [1.0728939771652222, 0.7931069135665894, 0.1398935467004776]\n","Discriminator Loss: 0.5555041408915713\n","Generator Loss: [0.8623193502426147, 0.7837732434272766, 0.039273060858249664]\n","Discriminator Loss: 0.5586684867794247\n","Generator Loss: [0.8175494074821472, 0.7938795685768127, 0.011834914796054363]\n","Discriminator Loss: 0.5468187772839883\n","Generator Loss: [0.8276085257530212, 0.7978328466415405, 0.014887847006320953]\n","Discriminator Loss: 0.545651089756575\n","Generator Loss: [0.8265513181686401, 0.7980012893676758, 0.014275015331804752]\n","Discriminator Loss: 0.5546782769797574\n","Generator Loss: [0.8285563588142395, 0.7979663014411926, 0.015295041725039482]\n","Discriminator Loss: 0.5709239086618254\n","Generator Loss: [0.819286584854126, 0.7965009212493896, 0.011392833665013313]\n","Discriminator Loss: 0.5487064244762223\n","Generator Loss: [0.8198322057723999, 0.7907222509384155, 0.014554974623024464]\n","Discriminator Loss: 0.5624755942808406\n","Generator Loss: [0.8362724184989929, 0.8130305409431458, 0.011620949022471905]\n","Discriminator Loss: 0.560418228720664\n","Generator Loss: [0.8437630534172058, 0.7966134548187256, 0.02357480116188526]\n","Discriminator Loss: 0.5570247893210762\n","Generator Loss: [0.9122151136398315, 0.7803822755813599, 0.06591641902923584]\n","Discriminator Loss: 0.542836945514864\n","Generator Loss: [0.8519794344902039, 0.798531174659729, 0.02672412060201168]\n","Discriminator Loss: 0.5627481662777427\n","Generator Loss: [0.8101276159286499, 0.7886189222335815, 0.010754333809018135]\n","Discriminator Loss: 0.5540800430408126\n","Generator Loss: [0.8036983013153076, 0.7832340002059937, 0.010232154279947281]\n","Discriminator Loss: 0.5572822624817491\n","Generator Loss: [0.8612830638885498, 0.7745647430419922, 0.04335914924740791]\n","Discriminator Loss: 0.5457909563574503\n","Generator Loss: [0.8089110255241394, 0.7799952030181885, 0.014457907527685165]\n","Discriminator Loss: 0.5543051036693214\n","Generator Loss: [0.8107438087463379, 0.7906808853149414, 0.01003145333379507]\n","Discriminator Loss: 0.5468807024535636\n","Generator Loss: [0.8107536435127258, 0.7900927066802979, 0.010330482386052608]\n","Discriminator Loss: 0.5519313544227771\n","Generator Loss: [0.844454824924469, 0.7804902791976929, 0.03198227286338806]\n","Discriminator Loss: 0.5450918356054899\n","Generator Loss: [0.803780198097229, 0.7719048261642456, 0.015937691554427147]\n","Discriminator Loss: 0.5499263053907271\n","Generator Loss: [0.8193483352661133, 0.7857823371887207, 0.016783002763986588]\n","Discriminator Loss: 0.5490292104432228\n","Generator Loss: [0.8034297227859497, 0.7619234323501587, 0.02075314335525036]\n","Discriminator Loss: 0.5595610735899754\n","Generator Loss: [0.7957435846328735, 0.7732890844345093, 0.01122724637389183]\n","Discriminator Loss: 0.5471066112595508\n","Generator Loss: [0.8251938223838806, 0.7894162535667419, 0.017888784408569336]\n","Discriminator Loss: 0.5562924036348704\n","Generator Loss: [0.8079426884651184, 0.7826566696166992, 0.012643018737435341]\n","Discriminator Loss: 0.5446753377273126\n","Generator Loss: [0.7955882549285889, 0.7766044735908508, 0.00949190091341734]\n","Discriminator Loss: 0.5490483477806265\n","Generator Loss: [0.792373538017273, 0.7707033753395081, 0.010835082270205021]\n","Discriminator Loss: 0.5687329755264727\n","Generator Loss: [0.8256953954696655, 0.7826138138771057, 0.021540779620409012]\n","Discriminator Loss: 0.5455567674143822\n","Generator Loss: [0.8929110169410706, 0.7608146667480469, 0.06604818254709244]\n","Discriminator Loss: 0.5561510333354818\n","Generator Loss: [0.8332040309906006, 0.7617092132568359, 0.035747408866882324]\n","Discriminator Loss: 0.5618340449364041\n","Generator Loss: [0.8011879920959473, 0.7817994356155396, 0.009694288484752178]\n","Discriminator Loss: 0.5491157268543247\n","Generator Loss: [0.799793541431427, 0.7785841822624207, 0.010604673996567726]\n","Discriminator Loss: 0.5513874062489776\n","Generator Loss: [0.7981058955192566, 0.776975691318512, 0.010565104894340038]\n","Discriminator Loss: 0.5527958998964095\n","Generator Loss: [0.796004056930542, 0.7720558643341064, 0.011974095366895199]\n","Discriminator Loss: 0.5609081273942138\n","Generator Loss: [0.8414027690887451, 0.7887344360351562, 0.026334170252084732]\n","Discriminator Loss: 0.5552885780452925\n","Generator Loss: [0.8073983788490295, 0.7805807590484619, 0.013408818282186985]\n","Discriminator Loss: 0.5473501950973514\n","Generator Loss: [0.8073669672012329, 0.7792642712593079, 0.014051335863769054]\n","Discriminator Loss: 0.5548872116105485\n","Generator Loss: [0.8687031865119934, 0.7822648882865906, 0.04321916028857231]\n","Discriminator Loss: 0.5698485792399879\n","Generator Loss: [0.8087331056594849, 0.771456241607666, 0.01863841712474823]\n","Discriminator Loss: 0.5486302518438606\n","Generator Loss: [0.8324003219604492, 0.7791093587875366, 0.026645477861166]\n","Discriminator Loss: 0.5442841638032405\n","Generator Loss: [0.9683164358139038, 0.7844333648681641, 0.09194153547286987]\n","Discriminator Loss: 0.5661441239662963\n","Generator Loss: [0.8172889947891235, 0.7940068244934082, 0.011641091667115688]\n","Discriminator Loss: 0.5586366417637691\n","Generator Loss: [0.8603886365890503, 0.7809479236602783, 0.03972036391496658]\n","Discriminator Loss: 0.5475767304615147\n","Generator Loss: [0.7993201613426208, 0.7767301797866821, 0.01129498053342104]\n","Discriminator Loss: 0.5667815424840228\n","Generator Loss: [0.8114341497421265, 0.7910230159759521, 0.010205577127635479]\n","Discriminator Loss: 0.5484724987854861\n","Generator Loss: [0.9633113741874695, 0.7840501070022583, 0.089630626142025]\n","Discriminator Loss: 0.5517030789787896\n","Generator Loss: [0.8088390231132507, 0.7879066467285156, 0.010466188192367554]\n","Discriminator Loss: 0.5481595191322413\n","Generator Loss: [0.8164958953857422, 0.7871407270431519, 0.014677594415843487]\n","Discriminator Loss: 0.5467004201345844\n","Generator Loss: [0.8416892290115356, 0.7828686237335205, 0.029410291463136673]\n","Discriminator Loss: 0.5433850624540355\n","Generator Loss: [0.814231276512146, 0.7797714471817017, 0.017229899764060974]\n","Discriminator Loss: 0.553037114064864\n","Generator Loss: [0.831954300403595, 0.7992212772369385, 0.01636650040745735]\n","Discriminator Loss: 0.5474982754421944\n","Generator Loss: [0.82123863697052, 0.7914746999740601, 0.014881972223520279]\n","Discriminator Loss: 0.5444967656703739\n","Generator Loss: [0.8389503955841064, 0.7937566637992859, 0.022596869617700577]\n","Discriminator Loss: 0.5552879547212797\n","Generator Loss: [0.8617923855781555, 0.793371319770813, 0.03421052172780037]\n","Discriminator Loss: 0.5410673020433023\n","Generator Loss: [0.8508415818214417, 0.7917857766151428, 0.02952791005373001]\n","Discriminator Loss: 0.5490675505725449\n","Generator Loss: [0.8215484619140625, 0.8006244897842407, 0.010461985133588314]\n","Discriminator Loss: 0.546309798807215\n","Generator Loss: [0.8132805228233337, 0.7898336052894592, 0.011723470874130726]\n","Discriminator Loss: 0.5458706364024692\n","Generator Loss: [0.8417555093765259, 0.8019856810569763, 0.019884908571839333]\n","Discriminator Loss: 0.551029120450039\n","Generator Loss: [0.8725691437721252, 0.7887508273124695, 0.04190915822982788]\n","Discriminator Loss: 0.5495290718863544\n","Generator Loss: [0.8166534900665283, 0.7908947467803955, 0.012879367917776108]\n","Discriminator Loss: 0.5449857055273242\n","Generator Loss: [0.820875346660614, 0.7972004413604736, 0.011837461963295937]\n","Discriminator Loss: 0.5521887985651119\n","Generator Loss: [0.8125458359718323, 0.7889757752418518, 0.011785043403506279]\n","Discriminator Loss: 0.5446281646745774\n","Generator Loss: [0.8115990161895752, 0.7868309020996094, 0.01238407101482153]\n","Discriminator Loss: 0.5500374193106836\n","Generator Loss: [0.8217967748641968, 0.7873416543006897, 0.017227575182914734]\n","Discriminator Loss: 0.549715137700332\n","Generator Loss: [0.7982006072998047, 0.7735518217086792, 0.01232439000159502]\n","Discriminator Loss: 0.551322800336493\n","Generator Loss: [0.8043804168701172, 0.7809259295463562, 0.011727238073945045]\n","Discriminator Loss: 0.5422982761501771\n","Generator Loss: [0.8102186322212219, 0.7860984802246094, 0.012060074135661125]\n","Discriminator Loss: 0.5460109641717281\n","Generator Loss: [0.8268004655838013, 0.7809369564056396, 0.02293175458908081]\n","Discriminator Loss: 0.5624649980868526\n","Generator Loss: [0.8143272995948792, 0.7760608792304993, 0.01913321763277054]\n","Discriminator Loss: 0.5462961510529567\n","Generator Loss: [0.8053837418556213, 0.779333233833313, 0.013025264255702496]\n","Discriminator Loss: 0.5591342275656643\n","Generator Loss: [0.8251566886901855, 0.7857255935668945, 0.019715549424290657]\n","Discriminator Loss: 0.5532872905896511\n","Generator Loss: [0.8184586763381958, 0.780158519744873, 0.019150082021951675]\n","Discriminator Loss: 0.547553389602399\n","Generator Loss: [0.8072270154953003, 0.7801892757415771, 0.013518866151571274]\n","Discriminator Loss: 0.5485036408899759\n","Generator Loss: [0.8076774477958679, 0.7771725654602051, 0.015252429991960526]\n","Discriminator Loss: 0.5403710460195725\n","Generator Loss: [1.576025128364563, 0.77374267578125, 0.4011412262916565]\n","Discriminator Loss: 0.5542699448369603\n","Generator Loss: [0.8146501183509827, 0.78925621509552, 0.012696961872279644]\n","Discriminator Loss: 0.5500030443799915\n","Generator Loss: [0.7966718673706055, 0.7643959522247314, 0.01613796316087246]\n","Discriminator Loss: 0.5463094870283385\n","Generator Loss: [3.3456008434295654, 0.7836248278617859, 1.2809879779815674]\n","Discriminator Loss: 0.5463096503990528\n","Generator Loss: [0.8010756969451904, 0.7709827423095703, 0.015046465210616589]\n","Discriminator Loss: 0.5689456784475624\n","Generator Loss: [0.7738933563232422, 0.7521477937698364, 0.010872776620090008]\n","Discriminator Loss: 0.5605658180811588\n","Generator Loss: [0.8523575067520142, 0.8092870712280273, 0.021535219624638557]\n","Discriminator Loss: 0.5524850980164047\n","Generator Loss: [0.8056308627128601, 0.7839808464050293, 0.010825010016560555]\n","Discriminator Loss: 0.5490187899904413\n","Generator Loss: [0.8060013651847839, 0.7832571268081665, 0.01137210987508297]\n","Discriminator Loss: 0.5603176380664081\n","Generator Loss: [0.8177065849304199, 0.7783061861991882, 0.01970018446445465]\n","Discriminator Loss: 0.5594573282451165\n","Generator Loss: [0.8181073665618896, 0.7689634561538696, 0.024571962654590607]\n","Discriminator Loss: 0.5521061279087007\n","Generator Loss: [0.8266030550003052, 0.7707927227020264, 0.027905181050300598]\n","Discriminator Loss: 0.550497972231824\n","Generator Loss: [0.8094074130058289, 0.7896972894668579, 0.009855063632130623]\n","Discriminator Loss: 0.5474026713563944\n","Generator Loss: [1.1190569400787354, 0.7906531095504761, 0.16420191526412964]\n","Discriminator Loss: 0.5488590802560793\n","Generator Loss: [0.8899862766265869, 0.7776495218276978, 0.056168362498283386]\n","Discriminator Loss: 0.5441133441017882\n","Generator Loss: [0.7957421541213989, 0.7677651643753052, 0.013988501392304897]\n","Discriminator Loss: 0.5542283652457627\n","Generator Loss: [0.8015356659889221, 0.775088906288147, 0.013223373331129551]\n","Discriminator Loss: 0.551637370383105\n","Generator Loss: [0.7916229963302612, 0.7639418244361877, 0.013840597122907639]\n","Discriminator Loss: 0.5504101012738829\n","Generator Loss: [0.8358621597290039, 0.782861590385437, 0.026500295847654343]\n","Discriminator Loss: 0.5448576331609729\n","Generator Loss: [0.805969774723053, 0.7871509790420532, 0.009409387595951557]\n","Discriminator Loss: 0.5483273334320984\n","Generator Loss: [0.8557383418083191, 0.7884886264801025, 0.033624857664108276]\n","Discriminator Loss: 0.5466166207934293\n","Generator Loss: [0.921924889087677, 0.7805047631263733, 0.07071005553007126]\n","Discriminator Loss: 0.5429455858702568\n","Generator Loss: [0.9181514978408813, 0.7819450497627258, 0.06810322403907776]\n","Discriminator Loss: 0.5439784146092279\n","Generator Loss: [0.7969585657119751, 0.7694599032402039, 0.013749334029853344]\n","Discriminator Loss: 0.5503483837328531\n","Generator Loss: [0.8743748068809509, 0.7904053926467896, 0.041984718292951584]\n","Discriminator Loss: 0.5489745145632696\n","Generator Loss: [0.8028987050056458, 0.7799645662307739, 0.011467074044048786]\n","Discriminator Loss: 0.5458283783232218\n","Generator Loss: [0.8004946112632751, 0.7731764316558838, 0.013659083284437656]\n","Discriminator Loss: 0.5503162386885379\n","Generator Loss: [0.8350840210914612, 0.7897087931632996, 0.022687610238790512]\n","Discriminator Loss: 0.540311455900337\n","Generator Loss: [0.8097503781318665, 0.7841012477874756, 0.012824556790292263]\n","Discriminator Loss: 0.5525181300399709\n","Generator Loss: [0.793147623538971, 0.7516096234321594, 0.02076900564134121]\n","Discriminator Loss: 0.56657587790869\n","Generator Loss: [0.8187901973724365, 0.7960488796234131, 0.011370661668479443]\n","Discriminator Loss: 0.5469834649848053\n","Generator Loss: [0.8477636575698853, 0.8042632937431335, 0.021750176325440407]\n","Discriminator Loss: 0.572514643181421\n","Generator Loss: [0.7979453206062317, 0.7734780311584473, 0.012233634479343891]\n","Discriminator Loss: 0.5521601783812002\n","Generator Loss: [0.8538886308670044, 0.783846914768219, 0.035020861774683]\n","Discriminator Loss: 0.5465682486324113\n","Generator Loss: [0.8113622665405273, 0.7855424880981445, 0.012909891083836555]\n","Discriminator Loss: 0.575932832244689\n","Generator Loss: [0.8320143818855286, 0.791501522064209, 0.020256439223885536]\n","Discriminator Loss: 0.5520671218873758\n","Generator Loss: [1.0161579847335815, 0.7778040170669556, 0.11917697638273239]\n","Discriminator Loss: 0.5430818634486059\n","Generator Loss: [0.8836742639541626, 0.7753048539161682, 0.054184697568416595]\n","Discriminator Loss: 0.548275173005095\n","Generator Loss: [0.8310970067977905, 0.7879050970077515, 0.021595951169729233]\n","Discriminator Loss: 0.5475822278731357\n","Generator Loss: [0.799995481967926, 0.77326500415802, 0.013365231454372406]\n","Discriminator Loss: 0.546685732453625\n","Generator Loss: [0.8012308478355408, 0.7824405431747437, 0.00939514022320509]\n","Discriminator Loss: 0.547171870170132\n","Generator Loss: [1.4451642036437988, 0.7731894850730896, 0.3359873592853546]\n","Discriminator Loss: 0.5736119278135448\n","Generator Loss: [0.7606750726699829, 0.7348321676254272, 0.012921438552439213]\n","Discriminator Loss: 0.563859965888696\n","Generator Loss: [0.8066927194595337, 0.787451982498169, 0.009620357304811478]\n","Discriminator Loss: 0.5686847668275732\n","Generator Loss: [0.7985990643501282, 0.7595507502555847, 0.019524162635207176]\n","Discriminator Loss: 0.5625474332582598\n","Generator Loss: [0.8207530379295349, 0.7973588705062866, 0.01169709675014019]\n","Discriminator Loss: 0.5571276943710473\n","Generator Loss: [0.8041126132011414, 0.7812308073043823, 0.011440915986895561]\n","Discriminator Loss: 0.5531721222232591\n","Generator Loss: [0.8096879124641418, 0.7841119766235352, 0.012787973508238792]\n","Discriminator Loss: 0.546783189859525\n","Generator Loss: [0.8180771470069885, 0.7953804731369019, 0.011348343454301357]\n","Discriminator Loss: 0.5493732750428535\n","Generator Loss: [0.8416123390197754, 0.7855641841888428, 0.028024064376950264]\n","Discriminator Loss: 0.5494598023069557\n","Generator Loss: [0.8386421203613281, 0.7927089333534241, 0.022966604679822922]\n","Discriminator Loss: 0.5423892973285547\n","Generator Loss: [0.8762246370315552, 0.7861989736557007, 0.045012835413217545]\n","Discriminator Loss: 0.5481004631310498\n","Generator Loss: [0.8120191693305969, 0.78593510389328, 0.013042032718658447]\n","Discriminator Loss: 0.5607921420141793\n","Generator Loss: [0.8155348896980286, 0.794529914855957, 0.01050247810781002]\n","Discriminator Loss: 0.5412965363339026\n","Generator Loss: [0.8407598733901978, 0.8012696504592896, 0.019745104014873505]\n","Discriminator Loss: 0.5433281769865062\n","Generator Loss: [0.811115026473999, 0.7905452251434326, 0.010284911841154099]\n","Discriminator Loss: 0.5397944769383685\n","Generator Loss: [0.8166372179985046, 0.7957714796066284, 0.010432872921228409]\n","Discriminator Loss: 0.5429835294371514\n","Generator Loss: [0.8153675198554993, 0.7946130037307739, 0.010377269238233566]\n","Discriminator Loss: 0.5405693398961375\n","Generator Loss: [0.838176965713501, 0.7899376153945923, 0.024119660258293152]\n","Discriminator Loss: 0.5463324575875959\n","Generator Loss: [0.9297828674316406, 0.7893251180648804, 0.07022887468338013]\n","Discriminator Loss: 0.5509234203068445\n","Generator Loss: [0.8327080607414246, 0.8038281202316284, 0.01443998422473669]\n","Discriminator Loss: 0.5523224447824759\n","Generator Loss: [0.9526665210723877, 0.7883658409118652, 0.08215032517910004]\n","Discriminator Loss: 0.5470813702195301\n","Generator Loss: [2.1357266902923584, 0.7722923159599304, 0.6817172169685364]\n","Discriminator Loss: 0.5494928957741649\n","Generator Loss: [1.2436014413833618, 0.7896720170974731, 0.22696469724178314]\n","Discriminator Loss: 0.5398458449944883\n","Generator Loss: [0.8315752744674683, 0.8054008483886719, 0.013087203726172447]\n","Discriminator Loss: 0.5433488912340181\n","Generator Loss: [0.8223596215248108, 0.798121452331543, 0.012119080871343613]\n","Discriminator Loss: 0.5411328338877865\n","Generator Loss: [0.8174462914466858, 0.7963972091674805, 0.010524541139602661]\n","Discriminator Loss: 0.5488578774311463\n","Generator Loss: [0.8137866258621216, 0.783428966999054, 0.015178822912275791]\n","Discriminator Loss: 0.5539947625911736\n","Generator Loss: [0.819435715675354, 0.7964082956314087, 0.011513696052134037]\n","Discriminator Loss: 0.5918091699568322\n","Generator Loss: [1.0502533912658691, 0.7915301322937012, 0.1293616145849228]\n","Discriminator Loss: 0.5568900559264875\n","Generator Loss: [0.8065000772476196, 0.7809537649154663, 0.01277314405888319]\n","Discriminator Loss: 0.5550630149809876\n","Generator Loss: [0.8193492293357849, 0.7981327176094055, 0.010608265176415443]\n","Discriminator Loss: 0.5602418094531458\n","Generator Loss: [0.8373907208442688, 0.7853469848632812, 0.026021862402558327]\n","Discriminator Loss: 0.5636538719336386\n","Generator Loss: [0.8125594854354858, 0.7888243794441223, 0.011867562308907509]\n","Discriminator Loss: 0.5535624243657367\n","Generator Loss: [0.8834418058395386, 0.7952697277069092, 0.0440860353410244]\n","Discriminator Loss: 0.5631343634740915\n","Generator Loss: [0.7954742908477783, 0.7723093032836914, 0.01158248633146286]\n","Discriminator Loss: 0.5568805894017714\n","Generator Loss: [0.9857938885688782, 0.7796846628189087, 0.10305460542440414]\n","Discriminator Loss: 0.5593696071009617\n","Generator Loss: [0.8038431406021118, 0.7825392484664917, 0.010651941411197186]\n","Discriminator Loss: 0.5448813849470753\n","Generator Loss: [0.8165982961654663, 0.7926772832870483, 0.011960498057305813]\n","Discriminator Loss: 0.5442642742636963\n","Generator Loss: [0.8224561214447021, 0.7990875840187073, 0.011684258468449116]\n","Discriminator Loss: 0.5552239031167119\n","Generator Loss: [0.9421882629394531, 0.7891857624053955, 0.07650123536586761]\n","Discriminator Loss: 0.5479469677266025\n","Generator Loss: [0.8072357773780823, 0.7769564390182495, 0.015139682218432426]\n","Discriminator Loss: 0.5424918476619496\n","Generator Loss: [0.8097714185714722, 0.7840733528137207, 0.012849039398133755]\n","Discriminator Loss: 0.5635840421773537\n","Generator Loss: [0.8002585172653198, 0.7806410193443298, 0.009808756411075592]\n","Discriminator Loss: 0.5478082880144939\n","Generator Loss: [0.8051241040229797, 0.7815384864807129, 0.011792819015681744]\n","Discriminator Loss: 0.5427653632577858\n","Generator Loss: [0.8454017639160156, 0.7865713834762573, 0.029415203258395195]\n","Discriminator Loss: 0.5470191649292246\n","Generator Loss: [0.815896213054657, 0.7940806150436401, 0.010907812975347042]\n","Discriminator Loss: 0.5541190074836777\n","Generator Loss: [0.8023626804351807, 0.7822891473770142, 0.010036768391728401]\n","Discriminator Loss: 0.5461633803060977\n","Generator Loss: [0.8029463887214661, 0.78264319896698, 0.010151606053113937]\n","Discriminator Loss: 0.5481016661988178\n","Generator Loss: [0.810226559638977, 0.7842811346054077, 0.012972715310752392]\n","Discriminator Loss: 0.543224779132288\n","Generator Loss: [0.9076219797134399, 0.7694874405860901, 0.06906726211309433]\n","Discriminator Loss: 0.5486168670722691\n","Generator Loss: [0.8074343800544739, 0.7826935648918152, 0.012370394542813301]\n","Discriminator Loss: 0.5535532245394279\n","Generator Loss: [0.8277197480201721, 0.799434244632721, 0.014142759144306183]\n","Discriminator Loss: 0.5441233351957635\n","Generator Loss: [0.8349817395210266, 0.7953437566757202, 0.019819004461169243]\n","Discriminator Loss: 0.5458076098366291\n","Generator Loss: [0.8581995368003845, 0.8178240060806274, 0.02018776908516884]\n","Discriminator Loss: 0.5509953254968423\n","Generator Loss: [0.8235838413238525, 0.8032486438751221, 0.010167588479816914]\n","Discriminator Loss: 0.5489070970397734\n","Generator Loss: [0.8437931537628174, 0.8026361465454102, 0.020578503608703613]\n","Discriminator Loss: 0.5467235704163613\n","Generator Loss: [0.8476954102516174, 0.8011147975921631, 0.02329031005501747]\n","Discriminator Loss: 0.5489901803521207\n","Generator Loss: [0.8120355606079102, 0.7856871485710144, 0.013174205087125301]\n","Discriminator Loss: 0.5547027183274622\n","Generator Loss: [0.8263937830924988, 0.8007731437683105, 0.012810314074158669]\n","Discriminator Loss: 0.5490483078210673\n","Generator Loss: [0.8039623498916626, 0.7754192352294922, 0.014271567575633526]\n","Discriminator Loss: 0.5478749496687669\n","Generator Loss: [0.8082495927810669, 0.7676664590835571, 0.020291557535529137]\n","Discriminator Loss: 0.5510558062123891\n","Generator Loss: [0.8177982568740845, 0.7910901308059692, 0.013354071415960789]\n","Discriminator Loss: 0.5535189388610888\n","Generator Loss: [0.7954038977622986, 0.7694569826126099, 0.012973455712199211]\n","Discriminator Loss: 0.546167065015652\n","Generator Loss: [0.7936081886291504, 0.7690020799636841, 0.012303042225539684]\n","Discriminator Loss: 0.5495532929553519\n","Generator Loss: [0.816805899143219, 0.7821681499481201, 0.01731887459754944]\n","Discriminator Loss: 0.5478575041452132\n","Generator Loss: [0.799148678779602, 0.7784963250160217, 0.010326186195015907]\n","Discriminator Loss: 0.5473529836126545\n","Generator Loss: [0.8094828128814697, 0.7676683068275452, 0.020907264202833176]\n","Discriminator Loss: 0.5470008229895029\n","Generator Loss: [0.8057447671890259, 0.7749318480491638, 0.015406451188027859]\n","Discriminator Loss: 0.546093979759462\n","Generator Loss: [0.7915757298469543, 0.7632366418838501, 0.014169543981552124]\n","Discriminator Loss: 0.5471494791272562\n","Generator Loss: [0.8174381852149963, 0.7920979857444763, 0.012670092284679413]\n","Discriminator Loss: 0.5535453826669254\n","Generator Loss: [0.8238712549209595, 0.750611424446106, 0.036629918962717056]\n","Discriminator Loss: 0.5488292731934052\n","Generator Loss: [0.9372139573097229, 0.7669048309326172, 0.08515455573797226]\n","Discriminator Loss: 0.5525115555392404\n","Generator Loss: [0.7864031791687012, 0.765676736831665, 0.010363234207034111]\n","Discriminator Loss: 0.5583812575969205\n","Generator Loss: [0.8454289436340332, 0.7745893001556396, 0.035419806838035583]\n","Discriminator Loss: 0.5438653293440439\n","Generator Loss: [0.8186053037643433, 0.7903296947479248, 0.014137794263660908]\n","Discriminator Loss: 0.5536412216897588\n","Generator Loss: [0.8138735890388489, 0.7959972023963928, 0.008938203565776348]\n","Discriminator Loss: 0.5435059031779019\n","Generator Loss: [0.8038614392280579, 0.7817364931106567, 0.011062473058700562]\n","Discriminator Loss: 0.5452982512888411\n","Generator Loss: [0.8391476273536682, 0.7741657495498657, 0.03249093517661095]\n","Discriminator Loss: 0.5487072386113141\n","Generator Loss: [0.7916544079780579, 0.7628189325332642, 0.014417743310332298]\n","Discriminator Loss: 0.5431175820262979\n","Generator Loss: [0.8796136975288391, 0.7957667112350464, 0.04192349314689636]\n","Discriminator Loss: 0.5432854333012074\n","Generator Loss: [0.7898683547973633, 0.7671623229980469, 0.011353005655109882]\n","Discriminator Loss: 0.5668915788783124\n","Generator Loss: [0.8423118591308594, 0.7990254163742065, 0.02164320833981037]\n","Discriminator Loss: 0.5545469955868612\n","Generator Loss: [0.8076190948486328, 0.7867532968521118, 0.010432898066937923]\n","Discriminator Loss: 0.5670320114722927\n","Generator Loss: [0.853814423084259, 0.7599092721939087, 0.046952586621046066]\n","Discriminator Loss: 0.5561102205447241\n","Generator Loss: [0.8117184042930603, 0.78968745470047, 0.011015472933650017]\n","Discriminator Loss: 0.5473623295742982\n","Generator Loss: [0.8046929836273193, 0.7840378284454346, 0.01032758504152298]\n","Discriminator Loss: 0.5432452178756648\n","Generator Loss: [0.82509845495224, 0.7926444411277771, 0.016227010637521744]\n","Discriminator Loss: 0.5371449364433829\n","Generator Loss: [0.8316642045974731, 0.8112280368804932, 0.010218090377748013]\n","Discriminator Loss: 0.5621848749506171\n","Generator Loss: [0.7948614954948425, 0.7718774676322937, 0.011491999961435795]\n","Discriminator Loss: 0.5559043926350569\n","Generator Loss: [0.7987918853759766, 0.7792869806289673, 0.009752467274665833]\n","Discriminator Loss: 0.5489427451957454\n","Generator Loss: [0.8025850057601929, 0.7752612829208374, 0.013661874458193779]\n","Discriminator Loss: 0.5495656067014352\n","Generator Loss: [0.810938835144043, 0.7905849814414978, 0.010176926851272583]\n","Discriminator Loss: 0.5487793752927246\n","Generator Loss: [0.7952750325202942, 0.7729039788246155, 0.011185519397258759]\n","Discriminator Loss: 0.5533948239308302\n","Generator Loss: [0.7915058135986328, 0.761940062046051, 0.014782869257032871]\n","Discriminator Loss: 0.5539398141499987\n","Generator Loss: [0.7978017330169678, 0.7729068398475647, 0.012447444722056389]\n","Discriminator Loss: 0.5450435744724018\n","Generator Loss: [0.8940193057060242, 0.7870887517929077, 0.05346526578068733]\n","Discriminator Loss: 0.5486965545896965\n","Generator Loss: [0.8143345713615417, 0.7917970418930054, 0.011268770322203636]\n","Discriminator Loss: 0.5470504878139764\n","Generator Loss: [0.813839316368103, 0.7838192582130432, 0.015010042116045952]\n","Discriminator Loss: 0.5457789103893447\n","Generator Loss: [0.8113263249397278, 0.7835772633552551, 0.013874516822397709]\n","Discriminator Loss: 0.5421553280848457\n","Generator Loss: [0.8416252136230469, 0.7852545380592346, 0.028185345232486725]\n","Discriminator Loss: 0.547390566886861\n","Generator Loss: [0.908888041973114, 0.7942577600479126, 0.05731513351202011]\n","Discriminator Loss: 0.5471445716193557\n","Generator Loss: [0.8099331855773926, 0.7760547399520874, 0.01693921722471714]\n","Discriminator Loss: 0.5424400224510464\n","Generator Loss: [0.8588199615478516, 0.7825896739959717, 0.03811515495181084]\n","Discriminator Loss: 0.5444215120978697\n","Generator Loss: [0.7978932857513428, 0.7746893167495728, 0.011601998470723629]\n","Discriminator Loss: 0.5407958925497951\n","Generator Loss: [0.8286078572273254, 0.8024976849555969, 0.013055097311735153]\n","Discriminator Loss: 0.542302473095333\n","Generator Loss: [0.8263262510299683, 0.7910646200180054, 0.0176308024674654]\n","Discriminator Loss: 0.5412506545526412\n","Generator Loss: [1.0438694953918457, 0.7959513068199158, 0.12395907938480377]\n","Discriminator Loss: 0.5522963857038121\n","Generator Loss: [0.816607654094696, 0.7889422178268433, 0.013832726515829563]\n","Discriminator Loss: 0.5590672826274385\n","Generator Loss: [0.8041125535964966, 0.7850087881088257, 0.009551876224577427]\n","Discriminator Loss: 0.5449274369093473\n","Generator Loss: [0.7961450219154358, 0.7768383026123047, 0.009653365239501]\n","Discriminator Loss: 0.548779397700855\n","Generator Loss: [0.7979612946510315, 0.7783879041671753, 0.009786700829863548]\n","Discriminator Loss: 0.5484381135720469\n","Generator Loss: [0.799651026725769, 0.7748129963874817, 0.012419011443853378]\n","Discriminator Loss: 0.5411362983832078\n","Generator Loss: [0.8117297887802124, 0.7907127737998962, 0.010508498176932335]\n","Discriminator Loss: 0.5488030622736915\n","Generator Loss: [0.8341026306152344, 0.772899329662323, 0.030601635575294495]\n","Discriminator Loss: 0.5470126834534312\n","Generator Loss: [0.8637319803237915, 0.7820016145706177, 0.04086518660187721]\n","Discriminator Loss: 0.5490048151077644\n","Generator Loss: [0.8094374537467957, 0.7871236205101013, 0.011156927794218063]\n","Discriminator Loss: 0.5452316726004938\n","Generator Loss: [0.8250110149383545, 0.7696031928062439, 0.027703907340765]\n","Discriminator Loss: 0.5484435659418523\n","Generator Loss: [0.8548102974891663, 0.7796095609664917, 0.03760036826133728]\n","Discriminator Loss: 0.548250819396344\n","Generator Loss: [0.810211718082428, 0.7821692824363708, 0.014021209441125393]\n","Discriminator Loss: 0.5518145968644603\n","Generator Loss: [0.7862020134925842, 0.767189621925354, 0.009506207890808582]\n","Discriminator Loss: 0.5665107778768288\n","Generator Loss: [0.8006548285484314, 0.7796199321746826, 0.01051743421703577]\n","Discriminator Loss: 0.5453338313964196\n","Generator Loss: [0.7944149971008301, 0.7727764248847961, 0.01081928238272667]\n","Discriminator Loss: 0.5492181287409039\n","Generator Loss: [1.1254488229751587, 0.7889738082885742, 0.16823752224445343]\n","Discriminator Loss: 0.5443554209246031\n","Generator Loss: [0.8030374646186829, 0.7819187641143799, 0.01055934652686119]\n","Discriminator Loss: 0.5445594938546492\n","Generator Loss: [0.8208948969841003, 0.788557231426239, 0.01616884395480156]\n","Discriminator Loss: 0.5441170237172628\n","Generator Loss: [0.8058032393455505, 0.7851865291595459, 0.010308348573744297]\n","Discriminator Loss: 0.552310077688162\n","Generator Loss: [0.8101340532302856, 0.7806625366210938, 0.014735760167241096]\n","Discriminator Loss: 0.5445675132068573\n","Generator Loss: [0.7944574356079102, 0.7762092351913452, 0.009124098345637321]\n","Discriminator Loss: 0.5425339767498372\n","Generator Loss: [0.8024426102638245, 0.7826012372970581, 0.009920677170157433]\n","Discriminator Loss: 0.5404383347722614\n","Generator Loss: [0.8146868944168091, 0.7727982997894287, 0.02094428613781929]\n","Discriminator Loss: 0.5402177281102922\n","Generator Loss: [0.7939566969871521, 0.7745277881622314, 0.009714445099234581]\n","Discriminator Loss: 0.5513411259889835\n","Generator Loss: [0.8128846287727356, 0.7899075150489807, 0.011488564312458038]\n","Discriminator Loss: 0.534783802992024\n","Generator Loss: [0.8226184248924255, 0.8001176118850708, 0.01125041488558054]\n","Discriminator Loss: 0.5425273985229069\n","Generator Loss: [0.8240722417831421, 0.7939814329147339, 0.015045415610074997]\n","Discriminator Loss: 0.5411114532180363\n","Generator Loss: [0.7818925380706787, 0.7629333138465881, 0.00947961863130331]\n","Discriminator Loss: 0.5565577410761762\n","Generator Loss: [0.8462408185005188, 0.8225919604301453, 0.01182442344725132]\n","Discriminator Loss: 0.5470764550600506\n","Generator Loss: [0.8496876358985901, 0.8110252022743225, 0.01933121122419834]\n","Discriminator Loss: 0.5679250753755696\n","Generator Loss: [0.7868284583091736, 0.7561696767807007, 0.015329376794397831]\n","Discriminator Loss: 0.5461659071352187\n","Generator Loss: [0.8209311366081238, 0.7964710593223572, 0.012230033986270428]\n","Discriminator Loss: 0.554211661026784\n","Generator Loss: [0.8499737977981567, 0.8207164406776428, 0.014628686010837555]\n","Discriminator Loss: 0.5605021357469013\n","Generator Loss: [0.8069583177566528, 0.7596540451049805, 0.02365214005112648]\n","Discriminator Loss: 0.549921484591323\n","Generator Loss: [1.0390570163726807, 0.8220231533050537, 0.10851696133613586]\n","Discriminator Loss: 0.5551977230170451\n","Generator Loss: [0.8160117268562317, 0.7755658626556396, 0.020222919061779976]\n","Discriminator Loss: 0.5449247442902561\n","Generator Loss: [0.8097152709960938, 0.7884849905967712, 0.010615147650241852]\n","Discriminator Loss: 0.5532776441827991\n","Generator Loss: [0.7947506904602051, 0.7593683004379272, 0.01769120804965496]\n","Discriminator Loss: 0.536427531427762\n","Generator Loss: [0.8219038248062134, 0.80153489112854, 0.010184453800320625]\n","Discriminator Loss: 0.5453071422230096\n","Generator Loss: [0.826422929763794, 0.8053752779960632, 0.010523831471800804]\n","Discriminator Loss: 0.5432999681133879\n","Generator Loss: [1.1577730178833008, 0.7870146036148071, 0.1853792369365692]\n","Discriminator Loss: 0.5557242028844485\n","Generator Loss: [0.810381293296814, 0.7817368507385254, 0.014322228729724884]\n","Discriminator Loss: 0.5584251351001512\n","Generator Loss: [0.8657496571540833, 0.8377466201782227, 0.014001509174704552]\n","Discriminator Loss: 0.559998790859936\n","Generator Loss: [0.8465662598609924, 0.8230710029602051, 0.011747634038329124]\n","Discriminator Loss: 0.5447359903891993\n","Generator Loss: [0.8327467441558838, 0.798521101474762, 0.017112810164690018]\n","Discriminator Loss: 0.5407861238327314\n","Generator Loss: [0.9027255773544312, 0.7999265193939209, 0.05139951407909393]\n","Discriminator Loss: 0.5390786366042448\n","Generator Loss: [0.8239551782608032, 0.8038853406906128, 0.010034929029643536]\n","Discriminator Loss: 0.5436234983787926\n","Generator Loss: [0.8396544456481934, 0.8195793628692627, 0.010037546046078205]\n","Discriminator Loss: 0.5431070580152664\n","Generator Loss: [0.8348051309585571, 0.8070221543312073, 0.013891486451029778]\n","Discriminator Loss: 0.5409633857689187\n","Generator Loss: [0.8719985485076904, 0.8319863080978394, 0.02000610902905464]\n","Discriminator Loss: 0.540010401901327\n","Generator Loss: [0.8351197838783264, 0.8142056465148926, 0.010457082651555538]\n","Discriminator Loss: 0.5494922888610745\n","Generator Loss: [0.8557210564613342, 0.8232351541519165, 0.016242964193224907]\n","Discriminator Loss: 0.5628467862779871\n","Generator Loss: [0.8469746112823486, 0.8261443376541138, 0.010415137745440006]\n","Discriminator Loss: 0.543325881752935\n","Generator Loss: [0.83636873960495, 0.8054839968681335, 0.01544237695634365]\n","Discriminator Loss: 0.54808082816362\n","Generator Loss: [0.8511281609535217, 0.8284274339675903, 0.011350373737514019]\n","Discriminator Loss: 0.5423394642903077\n","Generator Loss: [0.8341872096061707, 0.8119313716888428, 0.011127922683954239]\n","Discriminator Loss: 0.5646429287894534\n","Generator Loss: [0.7995631694793701, 0.7763553857803345, 0.011603890918195248]\n","Discriminator Loss: 0.5577232238629222\n","Generator Loss: [0.8646953701972961, 0.7914256453514099, 0.03663485497236252]\n","Discriminator Loss: 0.5526695538592321\n","Generator Loss: [0.8449804186820984, 0.8121806979179382, 0.01639985479414463]\n","Discriminator Loss: 0.545045852293697\n","Generator Loss: [1.0584450960159302, 0.8248180150985718, 0.1168135553598404]\n","Discriminator Loss: 0.5471217549520588\n","Generator Loss: [0.8509746789932251, 0.8287442922592163, 0.011115179397165775]\n","Discriminator Loss: 0.5541443420352152\n","Generator Loss: [0.8672108054161072, 0.8013803362846375, 0.03291522338986397]\n","Discriminator Loss: 0.5413691439980539\n","Generator Loss: [0.840201199054718, 0.804965078830719, 0.017618052661418915]\n","Discriminator Loss: 0.5418972198458505\n","Generator Loss: [0.828670084476471, 0.8066023588180542, 0.011033855378627777]\n","Discriminator Loss: 0.5410466974772135\n","Generator Loss: [0.8379102349281311, 0.8146292567253113, 0.011640490964055061]\n","Discriminator Loss: 0.5468773966331355\n","Generator Loss: [0.8669506311416626, 0.8202781677246094, 0.023336246609687805]\n","Discriminator Loss: 0.5488142490585233\n","Generator Loss: [0.8255412578582764, 0.8038862943649292, 0.010827478021383286]\n","Discriminator Loss: 0.549468188103674\n","Generator Loss: [0.8047494888305664, 0.7821158170700073, 0.011316845193505287]\n","Discriminator Loss: 0.5439517929298745\n","Generator Loss: [0.84494549036026, 0.7906765341758728, 0.027134479954838753]\n","Discriminator Loss: 0.5609217081619136\n","Generator Loss: [0.8565076589584351, 0.8107937574386597, 0.02285696007311344]\n","Discriminator Loss: 0.5451851594098116\n","Generator Loss: [0.8962955474853516, 0.7954576015472412, 0.05041896924376488]\n","Discriminator Loss: 0.5506299475182459\n","Generator Loss: [0.8111484050750732, 0.7813986539840698, 0.014874882996082306]\n","Discriminator Loss: 0.5421035871968343\n","Generator Loss: [0.8304403424263, 0.804288387298584, 0.013075963594019413]\n","Discriminator Loss: 0.5518452336109476\n","Generator Loss: [0.8364658951759338, 0.8109480738639832, 0.012758896686136723]\n","Discriminator Loss: 0.5395886811820674\n","Generator Loss: [0.8805546164512634, 0.8006156086921692, 0.039969492703676224]\n","Discriminator Loss: 0.5436491733871662\n","Generator Loss: [0.8220164775848389, 0.7994867563247681, 0.011264875531196594]\n","Discriminator Loss: 0.5414867547715403\n","Generator Loss: [0.8217957019805908, 0.7919669151306152, 0.014914381317794323]\n","Discriminator Loss: 0.5388072279565677\n","Generator Loss: [0.8277425765991211, 0.8007086515426636, 0.013516951352357864]\n","Discriminator Loss: 0.5424486514566524\n","Generator Loss: [0.8714601993560791, 0.8003958463668823, 0.03553219139575958]\n","Discriminator Loss: 0.5392186123940519\n","Generator Loss: [0.813601553440094, 0.7870404720306396, 0.0132805360481143]\n","Discriminator Loss: 0.5417836146425543\n","Generator Loss: [0.8298562169075012, 0.79345703125, 0.01819959655404091]\n","Discriminator Loss: 0.5440882552211406\n","Generator Loss: [0.8326510787010193, 0.7841609716415405, 0.02424505725502968]\n","Discriminator Loss: 0.554566469430938\n","Generator Loss: [0.825829267501831, 0.8012973666191101, 0.012265941128134727]\n","Discriminator Loss: 0.5439379012768768\n","Generator Loss: [0.9903309941291809, 0.7857798337936401, 0.10227557271718979]\n","Discriminator Loss: 0.5461146390298381\n","Generator Loss: [0.8114811182022095, 0.782564640045166, 0.01445824559777975]\n","Discriminator Loss: 0.5476546777681506\n","Generator Loss: [0.829720139503479, 0.8086656332015991, 0.010527257807552814]\n","Discriminator Loss: 0.5420245101113323\n","Generator Loss: [0.8431512713432312, 0.8066331148147583, 0.018259067088365555]\n","Discriminator Loss: 0.5356120145315799\n","Generator Loss: [0.850029706954956, 0.8157482743263245, 0.0171407088637352]\n","Discriminator Loss: 0.5436281839447474\n","Generator Loss: [0.9569237232208252, 0.8188075423240662, 0.06905809789896011]\n","Discriminator Loss: 0.5403535878904222\n","Generator Loss: [0.8655754327774048, 0.8204811811447144, 0.022547129541635513]\n","Discriminator Loss: 0.5647166305889186\n","Generator Loss: [0.8298011422157288, 0.8025636672973633, 0.013618728145956993]\n","Discriminator Loss: 0.5441099941326684\n","Generator Loss: [0.8206623196601868, 0.781570315361023, 0.019546005874872208]\n","Discriminator Loss: 0.5421708441608644\n","Generator Loss: [0.8491516709327698, 0.8223768472671509, 0.013387399725615978]\n","Discriminator Loss: 0.5444043937550305\n","Generator Loss: [0.8230646252632141, 0.8008242249488831, 0.011120188049972057]\n","Discriminator Loss: 0.5502840798180841\n","Generator Loss: [0.825370192527771, 0.8017326593399048, 0.011818775907158852]\n","Discriminator Loss: 0.569561242875352\n","Generator Loss: [0.8309147357940674, 0.809832751750946, 0.010540987364947796]\n","Discriminator Loss: 0.5480886259792896\n","Generator Loss: [0.8093635439872742, 0.7780485153198242, 0.015657521784305573]\n","Discriminator Loss: 0.5526650464707927\n","Generator Loss: [0.7823756337165833, 0.761213481426239, 0.010581078939139843]\n","Discriminator Loss: 0.5533425089051889\n","Generator Loss: [0.851839542388916, 0.7887259721755981, 0.03155677393078804]\n","Discriminator Loss: 0.5573818915545417\n","Generator Loss: [0.8327986001968384, 0.8054009675979614, 0.013698814436793327]\n","Discriminator Loss: 0.5471858073287876\n","Generator Loss: [0.9741686582565308, 0.7740789651870728, 0.100044846534729]\n","Discriminator Loss: 0.544992175680818\n","Generator Loss: [0.8106655478477478, 0.7896965146064758, 0.010484528727829456]\n","Discriminator Loss: 0.5499507509057366\n","Generator Loss: [0.7988171577453613, 0.7770676612854004, 0.010874755680561066]\n","Discriminator Loss: 0.5610327928225161\n","Generator Loss: [0.7786740064620972, 0.7533470392227173, 0.012663471512496471]\n","Discriminator Loss: 0.5535913953863201\n","Generator Loss: [0.8865615725517273, 0.8176884055137634, 0.03443659096956253]\n","Discriminator Loss: 0.554963778079582\n","Generator Loss: [0.8236998319625854, 0.803367555141449, 0.010166145861148834]\n","Discriminator Loss: 0.5519684319151565\n","Generator Loss: [0.8016385436058044, 0.7700066566467285, 0.01581595465540886]\n","Discriminator Loss: 0.5539074840944522\n","Generator Loss: [0.8327144980430603, 0.811046302318573, 0.010834104381501675]\n","Discriminator Loss: 0.5471382226987771\n","Generator Loss: [0.803568422794342, 0.7848328948020935, 0.009367760270833969]\n","Discriminator Loss: 0.5445968067961076\n","Generator Loss: [0.8030903339385986, 0.7844619154930115, 0.009314213879406452]\n","Discriminator Loss: 0.5438100893516093\n","Generator Loss: [0.8327558636665344, 0.7961636781692505, 0.018296103924512863]\n","Discriminator Loss: 0.543659859278705\n","Generator Loss: [0.8238906264305115, 0.7928903698921204, 0.015500136651098728]\n","Discriminator Loss: 0.5442832477701813\n","Generator Loss: [0.9371214509010315, 0.7732827663421631, 0.0819193422794342]\n","Discriminator Loss: 0.5454435609408392\n","Generator Loss: [0.8353068828582764, 0.7715940475463867, 0.03185641020536423]\n","Discriminator Loss: 0.5454696169945237\n","Generator Loss: [0.848217248916626, 0.7923969030380249, 0.02791016921401024]\n","Discriminator Loss: 0.5478416279656813\n","Generator Loss: [0.8330649137496948, 0.7822799682617188, 0.025392474606633186]\n","Discriminator Loss: 0.5492689489656186\n","Generator Loss: [0.8163104057312012, 0.7832251787185669, 0.016542617231607437]\n","Discriminator Loss: 0.5409021567065793\n","Generator Loss: [1.8624558448791504, 0.7781357765197754, 0.5421600341796875]\n","Discriminator Loss: 0.5259835679062235\n","Generator Loss: [0.8364607691764832, 0.8002848029136658, 0.01808798685669899]\n","Discriminator Loss: 0.5389399903287995\n","Generator Loss: [0.8039511442184448, 0.776634156703949, 0.013658490963280201]\n","Discriminator Loss: 0.5368813663608307\n","Generator Loss: [0.8043598532676697, 0.7775837182998657, 0.013388080522418022]\n","Discriminator Loss: 0.5478058421044807\n","Generator Loss: [0.8024681210517883, 0.7825273275375366, 0.009970409795641899]\n","Discriminator Loss: 0.5603559053970457\n","Generator Loss: [0.8403231501579285, 0.807403564453125, 0.01645979844033718]\n","Discriminator Loss: 0.541234869823711\n","Generator Loss: [1.0079303979873657, 0.8084078431129456, 0.09976126253604889]\n","Discriminator Loss: 0.5465161946685839\n","Generator Loss: [0.8322886824607849, 0.8028603196144104, 0.014714192599058151]\n","Discriminator Loss: 0.5537085130226842\n","Generator Loss: [0.8234239816665649, 0.7955026626586914, 0.013960647396743298]\n","Discriminator Loss: 0.5413047008732974\n","Generator Loss: [1.0821107625961304, 0.7812458276748657, 0.15043245255947113]\n","Discriminator Loss: 0.5785090734425467\n","Generator Loss: [0.7821110486984253, 0.7544441819190979, 0.013833418488502502]\n","Discriminator Loss: 0.5636708637921402\n","Generator Loss: [0.8492171168327332, 0.7983367443084717, 0.025440184399485588]\n","Discriminator Loss: 0.5516642948705339\n","Generator Loss: [0.8171994090080261, 0.7943934202194214, 0.011403006501495838]\n","Discriminator Loss: 0.5474686588277109\n","Generator Loss: [0.8372868895530701, 0.792180061340332, 0.02255341038107872]\n","Discriminator Loss: 0.5494161101669306\n","Generator Loss: [0.7816072106361389, 0.7462591528892517, 0.017674023285508156]\n","Discriminator Loss: 0.552360157482326\n","Generator Loss: [0.8336445093154907, 0.8121026158332825, 0.010770936496555805]\n","Discriminator Loss: 0.5469956677316077\n","Generator Loss: [0.9871841073036194, 0.8062679767608643, 0.09045807272195816]\n","Discriminator Loss: 0.5481577554783144\n","Generator Loss: [0.8432352542877197, 0.7894809246063232, 0.02687716856598854]\n","Discriminator Loss: 0.5505816229415359\n","Generator Loss: [0.7987514734268188, 0.7484307289123535, 0.025160374119877815]\n","Discriminator Loss: 0.5439304181309126\n","Generator Loss: [0.8200587630271912, 0.7925437092781067, 0.013757514767348766]\n","Discriminator Loss: 0.5437726087466217\n","Generator Loss: [0.7910261154174805, 0.7699819803237915, 0.010522059164941311]\n","Discriminator Loss: 0.5505324507921614\n","Generator Loss: [0.7720363140106201, 0.7518278360366821, 0.010104242712259293]\n","Discriminator Loss: 0.5491087748014252\n","Generator Loss: [0.8039270043373108, 0.7822028994560242, 0.010862062685191631]\n","Discriminator Loss: 0.5524211066112912\n","Generator Loss: [0.822084903717041, 0.8014036417007446, 0.010340617038309574]\n","Discriminator Loss: 0.5477314727040721\n","Generator Loss: [0.7905322909355164, 0.7713243961334229, 0.009603935293853283]\n","Discriminator Loss: 0.5574354607974783\n","Generator Loss: [0.8124085664749146, 0.7928518652915955, 0.009778344072401524]\n","Discriminator Loss: 0.5472474288512785\n","Generator Loss: [0.7896069288253784, 0.770423412322998, 0.009591745212674141]\n","Discriminator Loss: 0.5453395021613687\n","Generator Loss: [0.8061994314193726, 0.7823140025138855, 0.011942701414227486]\n","Discriminator Loss: 0.5609929978963919\n","Generator Loss: [0.7741190195083618, 0.7509065866470337, 0.011606213636696339]\n","Discriminator Loss: 0.535802014208457\n","Generator Loss: [0.8458173871040344, 0.8207723498344421, 0.01252252422273159]\n","Discriminator Loss: 0.5887947065525623\n","Generator Loss: [0.7696179151535034, 0.7409204244613647, 0.014348750934004784]\n","Discriminator Loss: 0.5879492605072301\n","Generator Loss: [0.8072035908699036, 0.7830460071563721, 0.012078802101314068]\n","Discriminator Loss: 0.5540994734255946\n","Generator Loss: [1.0222692489624023, 0.8000500202178955, 0.11110963672399521]\n","Discriminator Loss: 0.5496593484399455\n","Generator Loss: [0.8993337154388428, 0.7876915335655212, 0.05582107976078987]\n","Discriminator Loss: 0.5534058130961057\n","Generator Loss: [0.863059937953949, 0.8013685941696167, 0.030845660716295242]\n","Discriminator Loss: 0.5621139592735744\n","Generator Loss: [0.8183764219284058, 0.7932913303375244, 0.012542546726763248]\n","Discriminator Loss: 0.5498649997334724\n","Generator Loss: [0.8403038382530212, 0.8043417930603027, 0.017981031909585]\n","Discriminator Loss: 0.5485273897402294\n","Generator Loss: [0.839166522026062, 0.7961335182189941, 0.02151651121675968]\n","Discriminator Loss: 0.5470861591602443\n","Generator Loss: [0.8062418699264526, 0.7796149253845215, 0.013313459232449532]\n","Discriminator Loss: 0.5529015186912147\n","Generator Loss: [0.8234613537788391, 0.7938451766967773, 0.014808082953095436]\n","Discriminator Loss: 0.5474666917766626\n","Generator Loss: [0.8543807864189148, 0.7830417156219482, 0.03566953167319298]\n","Discriminator Loss: 0.5452034535810526\n","Generator Loss: [0.8055596351623535, 0.7837228775024414, 0.010918369516730309]\n","Discriminator Loss: 0.5441763414310117\n","Generator Loss: [0.8285800814628601, 0.790147066116333, 0.0192165095359087]\n","Discriminator Loss: 0.5433330289997684\n","Generator Loss: [0.812324583530426, 0.7872672080993652, 0.012528682127594948]\n","Discriminator Loss: 0.5410565162273997\n","Generator Loss: [0.8261871337890625, 0.8065183758735657, 0.009834379889070988]\n","Discriminator Loss: 0.5485033961940644\n","Generator Loss: [0.8114191889762878, 0.7898399829864502, 0.010789590887725353]\n","Discriminator Loss: 0.5498461117676925\n","Generator Loss: [0.796062707901001, 0.7738398313522339, 0.011111434549093246]\n","Discriminator Loss: 0.5677085477364017\n","Generator Loss: [0.7746344804763794, 0.7535271048545837, 0.010553685016930103]\n","Discriminator Loss: 0.5695875082810744\n","Generator Loss: [0.8139528632164001, 0.7903642654418945, 0.011794308200478554]\n","Discriminator Loss: 0.5486733908564929\n","Generator Loss: [0.799460768699646, 0.7797681093215942, 0.009846340864896774]\n","Discriminator Loss: 0.5557407388623687\n","Generator Loss: [0.8076716065406799, 0.7876831293106079, 0.009994243271648884]\n","Discriminator Loss: 0.5674213482016057\n","Generator Loss: [0.799079418182373, 0.7767868041992188, 0.011146314442157745]\n","Discriminator Loss: 0.5862965809592424\n","Generator Loss: [0.8572288751602173, 0.8162227272987366, 0.020503085106611252]\n","Discriminator Loss: 0.5508265130665677\n","Generator Loss: [0.8096736073493958, 0.7904635667800903, 0.00960502214729786]\n","Discriminator Loss: 0.5770040666102432\n","Generator Loss: [0.8006319403648376, 0.7783717513084412, 0.01113009825348854]\n","Discriminator Loss: 0.5428857118258748\n","Generator Loss: [0.809391975402832, 0.7827368974685669, 0.013327528722584248]\n","Discriminator Loss: 0.5562048992942437\n","Generator Loss: [0.8103382587432861, 0.7826364040374756, 0.013850925490260124]\n","Discriminator Loss: 0.5477919495933747\n","Generator Loss: [0.7791391611099243, 0.7602493762969971, 0.009444890543818474]\n","Discriminator Loss: 0.5531216478702845\n","Generator Loss: [0.8002931475639343, 0.7749505639076233, 0.012671292759478092]\n","Discriminator Loss: 0.554605618941423\n","Generator Loss: [0.8109594583511353, 0.7675188779830933, 0.02172028459608555]\n","Discriminator Loss: 0.5509567072876962\n","Generator Loss: [0.8336551189422607, 0.7505344152450562, 0.0415603443980217]\n","Discriminator Loss: 0.5654837392612535\n","Generator Loss: [0.7523054480552673, 0.7340185046195984, 0.009143470786511898]\n","Discriminator Loss: 0.5592978157183097\n","Generator Loss: [0.7840896248817444, 0.7539846897125244, 0.015052461065351963]\n","Discriminator Loss: 0.5527537750895135\n","Generator Loss: [0.7564051151275635, 0.735210657119751, 0.01059721875935793]\n","Discriminator Loss: 0.579591011730372\n","Generator Loss: [0.8195538520812988, 0.7807422280311584, 0.019405806437134743]\n","Discriminator Loss: 0.5585054523508006\n","Generator Loss: [0.8810112476348877, 0.8616560697555542, 0.009677575901150703]\n","Discriminator Loss: 0.5607876251197013\n","Generator Loss: [0.9434500932693481, 0.7876101136207581, 0.07791997492313385]\n","Discriminator Loss: 0.554637033437757\n","Generator Loss: [0.8029804229736328, 0.7732845544815063, 0.01484793983399868]\n","Discriminator Loss: 0.5560379582093447\n","Generator Loss: [0.7938951253890991, 0.7439835667610168, 0.024955792352557182]\n","Discriminator Loss: 0.5514083312045841\n","Generator Loss: [0.8343998193740845, 0.8119339346885681, 0.011232942342758179]\n","Discriminator Loss: 0.563445246525589\n","Generator Loss: [0.8087647557258606, 0.7810923457145691, 0.013836215250194073]\n","Discriminator Loss: 0.548216552255326\n","Generator Loss: [0.7719637751579285, 0.735335111618042, 0.018314341083168983]\n","Discriminator Loss: 0.5561225543970068\n","Generator Loss: [0.8115931153297424, 0.784988522529602, 0.013302309438586235]\n","Discriminator Loss: 0.5529746124338999\n","Generator Loss: [1.009617805480957, 0.7719757556915283, 0.11882104724645615]\n","Discriminator Loss: 0.5537668314063922\n","Generator Loss: [0.7835543155670166, 0.7567499876022339, 0.013402150012552738]\n","Discriminator Loss: 0.549046093965444\n","Generator Loss: [0.7992427349090576, 0.7717963457107544, 0.013723182491958141]\n","Discriminator Loss: 0.5479926208026882\n","Generator Loss: [0.7583716511726379, 0.738559365272522, 0.00990615226328373]\n","Discriminator Loss: 0.5469687962613534\n","Generator Loss: [0.8066332340240479, 0.7868916988372803, 0.009870770387351513]\n","Discriminator Loss: 0.5820121434226166\n","Generator Loss: [0.8174709677696228, 0.7894048690795898, 0.01403304934501648]\n","Discriminator Loss: 0.5666026150502148\n","Generator Loss: [1.502068042755127, 0.7439422607421875, 0.3790629208087921]\n","Discriminator Loss: 0.56457625548137\n","Generator Loss: [0.7545639276504517, 0.7300428152084351, 0.012260560877621174]\n","Discriminator Loss: 0.5468543269307702\n","Generator Loss: [0.780781626701355, 0.7414299249649048, 0.019675862044095993]\n","Discriminator Loss: 0.551949926688394\n","Generator Loss: [0.7754072546958923, 0.7554209232330322, 0.00999316293746233]\n","Discriminator Loss: 0.5500517674154253\n","Generator Loss: [0.7850232720375061, 0.7627418041229248, 0.011140739545226097]\n","Discriminator Loss: 0.5564998688896594\n","Generator Loss: [0.8205490112304688, 0.7841689586639404, 0.018190015107393265]\n","Discriminator Loss: 0.5469175005055149\n","Generator Loss: [0.8238246440887451, 0.7703496217727661, 0.02673751302063465]\n","Discriminator Loss: 0.5469974543302669\n","Generator Loss: [0.7677409052848816, 0.7355237007141113, 0.016108613461256027]\n","Discriminator Loss: 0.5575444746918947\n","Generator Loss: [0.839008629322052, 0.8184851408004761, 0.010261733084917068]\n","Discriminator Loss: 0.5510516380800254\n","Generator Loss: [0.8506342172622681, 0.7584458589553833, 0.04609416425228119]\n","Discriminator Loss: 0.5528667923717876\n","Generator Loss: [0.7915217876434326, 0.732167661190033, 0.02967705950140953]\n","Discriminator Loss: 0.5524552302413213\n","Generator Loss: [0.8948420882225037, 0.764019250869751, 0.06541141867637634]\n","Discriminator Loss: 0.5483773376763565\n","Generator Loss: [4.22365140914917, 0.7990454435348511, 1.7123029232025146]\n","Discriminator Loss: 0.5468694190312817\n","Generator Loss: [0.8119250535964966, 0.7920489311218262, 0.009938059374690056]\n","Discriminator Loss: 0.5417937878773955\n","Generator Loss: [0.8364270925521851, 0.8060871958732605, 0.015169954858720303]\n","Discriminator Loss: 0.5638701067582588\n","Generator Loss: [0.8254633545875549, 0.8041151762008667, 0.01067410223186016]\n","Discriminator Loss: 0.5575291603599908\n","Generator Loss: [0.8001405596733093, 0.7796616554260254, 0.010239441879093647]\n","Discriminator Loss: 0.5578763597004581\n","Generator Loss: [0.8191654086112976, 0.7959398031234741, 0.011612813919782639]\n","Discriminator Loss: 0.5617097769090833\n","Generator Loss: [0.8246023058891296, 0.7910422086715698, 0.016780052334070206]\n","Discriminator Loss: 0.5426469929607265\n","Generator Loss: [0.9007976055145264, 0.7877895832061768, 0.05650399997830391]\n","Discriminator Loss: 0.5456263266642054\n","Generator Loss: [0.7954406142234802, 0.7643628120422363, 0.015538888052105904]\n","Discriminator Loss: 0.538516357075423\n","Generator Loss: [0.9764711260795593, 0.7608146071434021, 0.10782826691865921]\n","Discriminator Loss: 0.5544273648065428\n","Generator Loss: [0.8081647753715515, 0.7881501913070679, 0.010007299482822418]\n","Discriminator Loss: 0.5452564539236846\n","Generator Loss: [0.8106904625892639, 0.7875175476074219, 0.011586449109017849]\n","Discriminator Loss: 0.546454308227112\n","Generator Loss: [0.8773394823074341, 0.7958047389984131, 0.040767379105091095]\n","Discriminator Loss: 0.566119647161031\n","Generator Loss: [0.8103465437889099, 0.7612118721008301, 0.02456733025610447]\n","Discriminator Loss: 0.5650215345158358\n","Generator Loss: [0.8609012961387634, 0.771874189376831, 0.04451354965567589]\n","Discriminator Loss: 0.5490207182810991\n","Generator Loss: [0.7930946350097656, 0.7580721378326416, 0.017511239275336266]\n","Discriminator Loss: 0.5490780786130927\n","Generator Loss: [1.2407500743865967, 0.7846544981002808, 0.22804777324199677]\n","Discriminator Loss: 0.5560837824159535\n","Generator Loss: [0.7922754287719727, 0.7734062671661377, 0.009434586390852928]\n","Discriminator Loss: 0.5386737232111045\n","Generator Loss: [1.0344195365905762, 0.8009368181228638, 0.1167413666844368]\n","Discriminator Loss: 0.547962009449293\n","Generator Loss: [0.8090407848358154, 0.7885079979896545, 0.010266397148370743]\n","Discriminator Loss: 0.576832663415189\n","Generator Loss: [0.8473931550979614, 0.8194196224212646, 0.013986769132316113]\n","Discriminator Loss: 0.5476155744399875\n","Generator Loss: [0.8355816006660461, 0.8117074966430664, 0.011937040835618973]\n","Discriminator Loss: 0.5736725441438466\n","Generator Loss: [0.8210291862487793, 0.7950639724731445, 0.012982610613107681]\n","Discriminator Loss: 0.5505873945621715\n","Generator Loss: [0.835222601890564, 0.810664176940918, 0.012279211543500423]\n","Discriminator Loss: 0.5498543433632221\n","Generator Loss: [0.8180111050605774, 0.7922399640083313, 0.012885580770671368]\n","Discriminator Loss: 0.5741359520943661\n","Generator Loss: [0.813088059425354, 0.7846463918685913, 0.01422084029763937]\n","Discriminator Loss: 0.5410293680397444\n","Generator Loss: [0.8128339648246765, 0.7844344973564148, 0.01419973373413086]\n","Discriminator Loss: 0.5486408471151663\n","Generator Loss: [0.8038530349731445, 0.7857357263565063, 0.009058653376996517]\n","Discriminator Loss: 0.5543653709792125\n","Generator Loss: [0.7972275614738464, 0.7674036622047424, 0.014911954291164875]\n","Discriminator Loss: 0.5492665521469462\n","Generator Loss: [0.8154537081718445, 0.7845158576965332, 0.015468926168978214]\n","Discriminator Loss: 0.545589983526952\n","Generator Loss: [0.7831012606620789, 0.7604883909225464, 0.011306438595056534]\n","Discriminator Loss: 0.5512936836021254\n","Generator Loss: [0.7855732440948486, 0.760614812374115, 0.012479215860366821]\n","Discriminator Loss: 0.5509750263190654\n","Generator Loss: [0.8008476495742798, 0.7763772010803223, 0.012235218659043312]\n","Discriminator Loss: 0.5512327719989116\n","Generator Loss: [0.8105708360671997, 0.776031494140625, 0.0172696802765131]\n","Discriminator Loss: 0.5440768653697887\n","Generator Loss: [0.8188831210136414, 0.797486424446106, 0.010698342695832253]\n","Discriminator Loss: 0.5403020285302773\n","Generator Loss: [0.8041789531707764, 0.7860579490661621, 0.009060499258339405]\n","Discriminator Loss: 0.5480901908158557\n","Generator Loss: [0.8329879641532898, 0.7859067320823669, 0.023540625348687172]\n","Discriminator Loss: 0.5481918002788007\n","Generator Loss: [0.8054070472717285, 0.7818742990493774, 0.011766369454562664]\n","Discriminator Loss: 0.5596903233699777\n","Generator Loss: [0.7770339250564575, 0.752892792224884, 0.012070560827851295]\n","Discriminator Loss: 0.5485115589108318\n","Generator Loss: [0.8381211757659912, 0.8061804175376892, 0.015970394015312195]\n","Discriminator Loss: 0.5473333529953379\n","Generator Loss: [0.8111986517906189, 0.7862517833709717, 0.01247342023998499]\n","Discriminator Loss: 0.5645039408609591\n","Generator Loss: [0.7811258435249329, 0.7474625706672668, 0.016831625252962112]\n","Discriminator Loss: 0.5488746261289634\n","Generator Loss: [0.8088960647583008, 0.7887342572212219, 0.010080898180603981]\n","Discriminator Loss: 0.5545241836152854\n","Generator Loss: [0.8142694234848022, 0.7926292419433594, 0.010820100083947182]\n","Discriminator Loss: 0.5501640788443183\n","Generator Loss: [0.84736168384552, 0.7864718437194824, 0.0304449200630188]\n","Discriminator Loss: 0.5458764388222335\n","Generator Loss: [0.8008713126182556, 0.7745555639266968, 0.013157876208424568]\n","Discriminator Loss: 0.5471102567826165\n","Generator Loss: [1.0097178220748901, 0.7771972417831421, 0.11626031249761581]\n","Discriminator Loss: 0.5493188786567771\n","Generator Loss: [0.8006390929222107, 0.7727154493331909, 0.013961808755993843]\n","Discriminator Loss: 0.5442900361413194\n","Generator Loss: [0.8026245832443237, 0.7684644460678101, 0.017080074176192284]\n","Discriminator Loss: 0.5437820010083669\n","Generator Loss: [0.7837584614753723, 0.763485848903656, 0.010136318393051624]\n","Discriminator Loss: 0.5490363101271214\n","Generator Loss: [0.7634903192520142, 0.733856737613678, 0.014816788025200367]\n","Discriminator Loss: 0.5534642836692001\n","Generator Loss: [0.8110764026641846, 0.7898449301719666, 0.010615730658173561]\n","Discriminator Loss: 0.5420105890589184\n","Generator Loss: [0.8997159004211426, 0.7842512130737305, 0.05773235484957695]\n","Discriminator Loss: 0.5421484463040542\n","Generator Loss: [0.8127880096435547, 0.7848623991012573, 0.01396280899643898]\n","Discriminator Loss: 0.5415981417609146\n","Generator Loss: [0.808440625667572, 0.7865853309631348, 0.010927649214863777]\n","Discriminator Loss: 0.5438281431288488\n","Generator Loss: [1.4332866668701172, 0.7827699780464172, 0.32525834441185]\n","Discriminator Loss: 0.5429988371597574\n","Generator Loss: [0.9599059224128723, 0.7838508486747742, 0.08802752941846848]\n","Discriminator Loss: 0.5471507321526587\n","Generator Loss: [0.8225089311599731, 0.7962955236434937, 0.01310670655220747]\n","Discriminator Loss: 0.5442559711809736\n","Generator Loss: [4.144331932067871, 0.7697868347167969, 1.6872724294662476]\n","Discriminator Loss: 0.542144170451138\n","Generator Loss: [0.8124983310699463, 0.7895125150680542, 0.011492920108139515]\n","Discriminator Loss: 0.5588891681654786\n","Generator Loss: [0.802300751209259, 0.7772314548492432, 0.012534642592072487]\n","Discriminator Loss: 0.5479708410075546\n","Generator Loss: [0.9666597247123718, 0.7637166976928711, 0.10147151350975037]\n","Discriminator Loss: 0.5474715345599179\n","Generator Loss: [0.9216464757919312, 0.7897430658340454, 0.06595171988010406]\n","Discriminator Loss: 0.5434663748437742\n","Generator Loss: [0.8030015826225281, 0.7787139415740967, 0.012143821455538273]\n","Discriminator Loss: 0.5446936505486519\n","Generator Loss: [0.7868237495422363, 0.7641106843948364, 0.011356521397829056]\n","Discriminator Loss: 0.5430952512442673\n","Generator Loss: [0.8040066361427307, 0.7836753129959106, 0.010165647603571415]\n","Discriminator Loss: 0.5436851400527303\n","Generator Loss: [0.8056688904762268, 0.7786935567855835, 0.013487664051353931]\n","Discriminator Loss: 0.5415324289097043\n","Generator Loss: [0.8275773525238037, 0.7818995714187622, 0.022838901728391647]\n","Discriminator Loss: 0.5435948159265536\n","Generator Loss: [0.8129427433013916, 0.7922766208648682, 0.010333070531487465]\n","Discriminator Loss: 0.5448202167281124\n","Generator Loss: [0.7861455082893372, 0.761053204536438, 0.012546165846288204]\n","Discriminator Loss: 0.5458658629322599\n","Generator Loss: [1.061438798904419, 0.7677149772644043, 0.14686192572116852]\n","Discriminator Loss: 0.54524088539074\n","Generator Loss: [0.807561457157135, 0.7798150777816772, 0.013873180374503136]\n","Discriminator Loss: 0.5443902728657122\n","Generator Loss: [0.7850778102874756, 0.7615031003952026, 0.01178734190762043]\n","Discriminator Loss: 0.545945814004881\n","Generator Loss: [0.828025758266449, 0.7873684167861938, 0.020328668877482414]\n","Discriminator Loss: 0.540415711579044\n","Generator Loss: [0.8426640033721924, 0.7810864448547363, 0.030788792297244072]\n","Discriminator Loss: 0.5433330142295745\n","Generator Loss: [0.812425971031189, 0.7826728820800781, 0.014876550994813442]\n","Discriminator Loss: 0.5635364969421062\n","Generator Loss: [0.7869137525558472, 0.7673395276069641, 0.009787117131054401]\n","Discriminator Loss: 0.5441707478130411\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 4/10 [1:26:20<1:43:01, 1030.21s/it]"]},{"name":"stdout","output_type":"stream","text":["Generator Loss: [0.8250277042388916, 0.7994619011878967, 0.012782914564013481]\n","Epoch: 4\n","Discriminator Loss: 0.5437030221328314\n","Generator Loss: [0.8063130974769592, 0.7820216417312622, 0.012145716696977615]\n","Discriminator Loss: 0.5440892085680389\n","Generator Loss: [0.805156409740448, 0.7740874886512756, 0.015534457750618458]\n","Discriminator Loss: 0.5441091576431063\n","Generator Loss: [0.8131933808326721, 0.7899398803710938, 0.011626750230789185]\n","Discriminator Loss: 0.551436652574921\n","Generator Loss: [0.7927501201629639, 0.7550551295280457, 0.01884748414158821]\n","Discriminator Loss: 0.5486359342303331\n","Generator Loss: [1.0426757335662842, 0.7789071202278137, 0.13188427686691284]\n","Discriminator Loss: 0.5474402911495417\n","Generator Loss: [1.1490228176116943, 0.7734994888305664, 0.18776166439056396]\n","Discriminator Loss: 0.54511385958358\n","Generator Loss: [0.8017711043357849, 0.7713007926940918, 0.015235156752169132]\n","Discriminator Loss: 0.542368295074084\n","Generator Loss: [0.8256573677062988, 0.7951247692108154, 0.015266289934515953]\n","Discriminator Loss: 0.5459522456421837\n","Generator Loss: [0.8704881072044373, 0.7820995450019836, 0.044194288551807404]\n","Discriminator Loss: 0.5477758755205286\n","Generator Loss: [0.8727210760116577, 0.7969352006912231, 0.03789294511079788]\n","Discriminator Loss: 0.5439769522490678\n","Generator Loss: [0.8213557600975037, 0.7694235444068909, 0.025966109707951546]\n","Discriminator Loss: 0.5434417810965897\n","Generator Loss: [0.7990830540657043, 0.7737650871276855, 0.012658988125622272]\n","Discriminator Loss: 0.5460845638554019\n","Generator Loss: [0.8100532293319702, 0.7772029638290405, 0.016425129026174545]\n","Discriminator Loss: 0.5449831882178842\n","Generator Loss: [0.8251562714576721, 0.7975208759307861, 0.013817710801959038]\n","Discriminator Loss: 0.5420094348028215\n","Generator Loss: [0.8352842330932617, 0.7657254934310913, 0.03477936610579491]\n","Discriminator Loss: 0.5424438393256423\n","Generator Loss: [0.8292046785354614, 0.7898385524749756, 0.019683049991726875]\n","Discriminator Loss: 0.5487309612708486\n","Generator Loss: [0.818880558013916, 0.7887543439865112, 0.015063110738992691]\n","Discriminator Loss: 0.5476038302404049\n","Generator Loss: [0.7997917532920837, 0.7790511846542358, 0.01037027407437563]\n","Discriminator Loss: 0.5407240330805507\n","Generator Loss: [0.8229898810386658, 0.7828540802001953, 0.020067906007170677]\n","Discriminator Loss: 0.5385143640269234\n","Generator Loss: [0.8105604648590088, 0.7850930690765381, 0.012733704410493374]\n","Discriminator Loss: 0.5402310277986544\n","Generator Loss: [0.8578333854675293, 0.7794639468193054, 0.03918470814824104]\n","Discriminator Loss: 0.5480109486234142\n","Generator Loss: [0.8125506639480591, 0.787310779094696, 0.012619954533874989]\n","Discriminator Loss: 0.5425440887465811\n","Generator Loss: [0.8650920391082764, 0.7854703664779663, 0.03981084004044533]\n","Discriminator Loss: 0.5451056820729718\n","Generator Loss: [0.8102219104766846, 0.7711178660392761, 0.01955202966928482]\n","Discriminator Loss: 0.5582674866564048\n","Generator Loss: [0.8086226582527161, 0.7849143743515015, 0.011854142881929874]\n","Discriminator Loss: 0.5460715255467221\n","Generator Loss: [0.8712025284767151, 0.7825607061386108, 0.044320911169052124]\n","Discriminator Loss: 0.5381890577473314\n","Generator Loss: [0.822609007358551, 0.7992076873779297, 0.011700646951794624]\n","Discriminator Loss: 0.5538853616744746\n","Generator Loss: [0.7965231537818909, 0.7751781940460205, 0.010672485455870628]\n","Discriminator Loss: 0.5512734501025989\n","Generator Loss: [0.798073947429657, 0.7778821587562561, 0.01009588036686182]\n","Discriminator Loss: 0.5446427042734285\n","Generator Loss: [0.8177440166473389, 0.7993757724761963, 0.00918411836028099]\n","Discriminator Loss: 0.540799722304655\n","Generator Loss: [0.8016507029533386, 0.7792842388153076, 0.011183218099176884]\n","Discriminator Loss: 0.5380270221648971\n","Generator Loss: [0.8232001066207886, 0.794797420501709, 0.01420134212821722]\n","Discriminator Loss: 0.5671071002689132\n","Generator Loss: [0.7979632616043091, 0.7740520238876343, 0.011955608613789082]\n","Discriminator Loss: 0.5515935296643875\n","Generator Loss: [0.7824235558509827, 0.7624302506446838, 0.009996646083891392]\n","Discriminator Loss: 0.5520072143117432\n","Generator Loss: [0.8080198168754578, 0.7848902344703674, 0.01156480424106121]\n","Discriminator Loss: 0.5503589726940845\n","Generator Loss: [0.8250893354415894, 0.7889559268951416, 0.01806669682264328]\n","Discriminator Loss: 0.545913734647911\n","Generator Loss: [0.8320974111557007, 0.7970156669616699, 0.01754087395966053]\n","Discriminator Loss: 0.5458917200321594\n","Generator Loss: [0.7970254421234131, 0.779497504234314, 0.008763971738517284]\n","Discriminator Loss: 0.545755997307424\n","Generator Loss: [0.8958104252815247, 0.7765735387802124, 0.05961843580007553]\n","Discriminator Loss: 0.5445831590204762\n","Generator Loss: [0.8083772659301758, 0.7867311835289001, 0.010823043994605541]\n","Discriminator Loss: 0.5441617583055631\n","Generator Loss: [0.7946293950080872, 0.767960786819458, 0.013334318064153194]\n","Discriminator Loss: 0.5599679178776569\n","Generator Loss: [0.7132495641708374, 0.6734217405319214, 0.019913917407393456]\n","Discriminator Loss: 0.5725265365545056\n","Generator Loss: [0.8083589673042297, 0.7851414680480957, 0.011608739383518696]\n","Discriminator Loss: 0.5515780211562742\n","Generator Loss: [0.7921498417854309, 0.7669482827186584, 0.012600776739418507]\n","Discriminator Loss: 0.5535594570264948\n","Generator Loss: [0.8079065084457397, 0.7880317568778992, 0.009937372989952564]\n","Discriminator Loss: 0.5530896729469532\n","Generator Loss: [1.048797369003296, 0.7757275104522705, 0.13653495907783508]\n","Discriminator Loss: 0.5534890031285613\n","Generator Loss: [0.843697190284729, 0.7681018114089966, 0.03779769688844681]\n","Discriminator Loss: 0.5484422560330131\n","Generator Loss: [0.8374746441841125, 0.8149948120117188, 0.011239929124712944]\n","Discriminator Loss: 0.5475886855419958\n","Generator Loss: [0.8105247020721436, 0.782720685005188, 0.013902000151574612]\n","Discriminator Loss: 0.5447897393028143\n","Generator Loss: [0.8166635632514954, 0.7898547053337097, 0.013404431752860546]\n","Discriminator Loss: 0.5516220972167503\n","Generator Loss: [0.83414626121521, 0.8055492639541626, 0.014298495836555958]\n","Discriminator Loss: 0.5564068471812789\n","Generator Loss: [0.8124796152114868, 0.7910084128379822, 0.010735603980720043]\n","Discriminator Loss: 0.5480380685421551\n","Generator Loss: [0.8035861253738403, 0.7760260701179504, 0.013780023902654648]\n","Discriminator Loss: 0.5515968902727764\n","Generator Loss: [0.8389419317245483, 0.8171221613883972, 0.010909873992204666]\n","Discriminator Loss: 0.5559117321899976\n","Generator Loss: [0.8292768001556396, 0.7843310236930847, 0.02247290313243866]\n","Discriminator Loss: 0.5542783593591594\n","Generator Loss: [0.9144551753997803, 0.7888097763061523, 0.06282270699739456]\n","Discriminator Loss: 0.5431751730357064\n","Generator Loss: [0.8431155681610107, 0.7930142283439636, 0.025050664320588112]\n","Discriminator Loss: 0.5543519187731363\n","Generator Loss: [0.7840900421142578, 0.7639667987823486, 0.01006163377314806]\n","Discriminator Loss: 0.5464960422868899\n","Generator Loss: [0.8181207180023193, 0.7989373207092285, 0.009591709822416306]\n","Discriminator Loss: 0.5525512432104733\n","Generator Loss: [0.8564642667770386, 0.7734558582305908, 0.04150419309735298]\n","Discriminator Loss: 0.5429575549824222\n","Generator Loss: [0.8028526306152344, 0.7754480242729187, 0.01370231807231903]\n","Discriminator Loss: 0.5478660769113048\n","Generator Loss: [0.8005396723747253, 0.7816542983055115, 0.009442685171961784]\n","Discriminator Loss: 0.5426880065751902\n","Generator Loss: [0.8206414580345154, 0.8012619018554688, 0.009689782746136189]\n","Discriminator Loss: 0.5480834601785318\n","Generator Loss: [0.8377479314804077, 0.7769460082054138, 0.030400946736335754]\n","Discriminator Loss: 0.540702488822717\n","Generator Loss: [0.808364748954773, 0.7784225344657898, 0.01497111190110445]\n","Discriminator Loss: 0.5406863440312009\n","Generator Loss: [0.8152903318405151, 0.7838060855865479, 0.01574210822582245]\n","Discriminator Loss: 0.5468445892365708\n","Generator Loss: [0.8131226301193237, 0.7735795974731445, 0.019771527498960495]\n","Discriminator Loss: 0.5522205186643987\n","Generator Loss: [0.7880827188491821, 0.7668165564537048, 0.010633071884512901]\n","Discriminator Loss: 0.5453603964224385\n","Generator Loss: [0.8425394296646118, 0.808397650718689, 0.01707087829709053]\n","Discriminator Loss: 0.5503649591337307\n","Generator Loss: [0.8044120073318481, 0.7805165648460388, 0.011947729624807835]\n","Discriminator Loss: 0.5469028083889498\n","Generator Loss: [0.7855719923973083, 0.7679296731948853, 0.008821157738566399]\n","Discriminator Loss: 0.5487014380832989\n","Generator Loss: [0.792150616645813, 0.7716759443283081, 0.010237329639494419]\n","Discriminator Loss: 0.5570449447186547\n","Generator Loss: [0.8011784553527832, 0.7597458362579346, 0.020716305822134018]\n","Discriminator Loss: 0.545218643030239\n","Generator Loss: [0.900127112865448, 0.7715803384780884, 0.0642733946442604]\n","Discriminator Loss: 0.5468548108838149\n","Generator Loss: [0.8196004033088684, 0.7515203356742859, 0.03404003754258156]\n","Discriminator Loss: 0.5640033070485515\n","Generator Loss: [0.7659976482391357, 0.748511791229248, 0.008742913603782654]\n","Discriminator Loss: 0.5432221710616432\n","Generator Loss: [0.8249176740646362, 0.8050823211669922, 0.00991768203675747]\n","Discriminator Loss: 0.5444238381423929\n","Generator Loss: [0.8063912391662598, 0.7863512635231018, 0.010019997134804726]\n","Discriminator Loss: 0.5454146392803523\n","Generator Loss: [0.8249196410179138, 0.8022934198379517, 0.011313102208077908]\n","Discriminator Loss: 0.5599428872810677\n","Generator Loss: [0.8526833057403564, 0.8044987916946411, 0.024092257022857666]\n","Discriminator Loss: 0.54907257306877\n","Generator Loss: [0.7924062609672546, 0.7679932117462158, 0.012206525541841984]\n","Discriminator Loss: 0.5470711686102732\n","Generator Loss: [0.79213547706604, 0.7658901214599609, 0.013122686184942722]\n","Discriminator Loss: 0.5489010509827494\n","Generator Loss: [0.8718900084495544, 0.7873044610023499, 0.04229278489947319]\n","Discriminator Loss: 0.5529474467912223\n","Generator Loss: [0.7994530200958252, 0.7647932171821594, 0.017329886555671692]\n","Discriminator Loss: 0.5459180170873879\n","Generator Loss: [0.8220722079277039, 0.7714073061943054, 0.025332460179924965]\n","Discriminator Loss: 0.5412236397969536\n","Generator Loss: [0.9540561437606812, 0.7821732759475708, 0.08594144880771637]\n","Discriminator Loss: 0.5551190371352277\n","Generator Loss: [0.8110842704772949, 0.7887259721755981, 0.011179140768945217]\n","Discriminator Loss: 0.5451426264953625\n","Generator Loss: [0.861193835735321, 0.782416582107544, 0.03938862681388855]\n","Discriminator Loss: 0.5460928801221598\n","Generator Loss: [0.7931576371192932, 0.7691843509674072, 0.011986633762717247]\n","Discriminator Loss: 0.5542880535758741\n","Generator Loss: [0.8142952919006348, 0.7936185598373413, 0.010338367894291878]\n","Discriminator Loss: 0.5423340379893489\n","Generator Loss: [0.9593953490257263, 0.7849883437156677, 0.0872035026550293]\n","Discriminator Loss: 0.5443503106307617\n","Generator Loss: [0.7984919548034668, 0.7788110971450806, 0.009840426966547966]\n","Discriminator Loss: 0.5425570493516716\n","Generator Loss: [0.8056280612945557, 0.7780193090438843, 0.013804366812109947]\n","Discriminator Loss: 0.5429949327317445\n","Generator Loss: [0.834551990032196, 0.7785807847976685, 0.027985597029328346]\n","Discriminator Loss: 0.5404094256955432\n","Generator Loss: [0.8063901662826538, 0.7743927836418152, 0.015998702496290207]\n","Discriminator Loss: 0.546056509335358\n","Generator Loss: [0.8231909275054932, 0.7921699285507202, 0.015510505065321922]\n","Discriminator Loss: 0.5393943012968521\n","Generator Loss: [0.8166810274124146, 0.7885244488716125, 0.014078298583626747]\n","Discriminator Loss: 0.5422173549632134\n","Generator Loss: [0.8273197412490845, 0.7837528586387634, 0.021783437579870224]\n","Discriminator Loss: 0.5482241407062247\n","Generator Loss: [0.8538050651550293, 0.7873581051826477, 0.033223483711481094]\n","Discriminator Loss: 0.5413055406943386\n","Generator Loss: [0.8290945887565613, 0.7725257873535156, 0.028284400701522827]\n","Discriminator Loss: 0.5451824864412629\n","Generator Loss: [0.8155158758163452, 0.7959139943122864, 0.009800948202610016]\n","Discriminator Loss: 0.5531101980923268\n","Generator Loss: [0.8009207248687744, 0.778469443321228, 0.011225654743611813]\n","Discriminator Loss: 0.5427898475081747\n","Generator Loss: [0.8069810271263123, 0.7672855854034424, 0.01984770968556404]\n","Discriminator Loss: 0.5478521504542186\n","Generator Loss: [0.8466925621032715, 0.7645787596702576, 0.04105691611766815]\n","Discriminator Loss: 0.543435749508717\n","Generator Loss: [0.8138830065727234, 0.7898291945457458, 0.012026905082166195]\n","Discriminator Loss: 0.5434949290165605\n","Generator Loss: [0.8105217218399048, 0.789064884185791, 0.01072840578854084]\n","Discriminator Loss: 0.5445145436387975\n","Generator Loss: [0.8006786704063416, 0.7786296606063843, 0.01102451141923666]\n","Discriminator Loss: 0.5424549710478459\n","Generator Loss: [0.7974896430969238, 0.7729378938674927, 0.012275874614715576]\n","Discriminator Loss: 0.5427397212879441\n","Generator Loss: [0.8160573840141296, 0.7826463580131531, 0.016705524176359177]\n","Discriminator Loss: 0.5452895162961795\n","Generator Loss: [0.7789648175239563, 0.7547231912612915, 0.012120824307203293]\n","Discriminator Loss: 0.549195042663996\n","Generator Loss: [0.7853229641914368, 0.7629475593566895, 0.011187698692083359]\n","Discriminator Loss: 0.5424808424131697\n","Generator Loss: [0.815113365650177, 0.792452335357666, 0.011330517008900642]\n","Discriminator Loss: 0.5430066709477614\n","Generator Loss: [0.8242382407188416, 0.7800458669662476, 0.022096196189522743]\n","Discriminator Loss: 0.5478129789898958\n","Generator Loss: [0.8020697832107544, 0.765261709690094, 0.018404047936201096]\n","Discriminator Loss: 0.5454373712891538\n","Generator Loss: [0.7901310324668884, 0.7654842138290405, 0.0123234111815691]\n","Discriminator Loss: 0.5478560412630031\n","Generator Loss: [0.8421287536621094, 0.8051501512527466, 0.018489301204681396]\n","Discriminator Loss: 0.5462084302980657\n","Generator Loss: [0.8201873302459717, 0.7836770415306091, 0.018255140632390976]\n","Discriminator Loss: 0.5435999174624158\n","Generator Loss: [0.8062337040901184, 0.7806508541107178, 0.012791437096893787]\n","Discriminator Loss: 0.5453871218978747\n","Generator Loss: [0.800469696521759, 0.7711254358291626, 0.014672118239104748]\n","Discriminator Loss: 0.5401689521258959\n","Generator Loss: [1.5610086917877197, 0.7777804732322693, 0.3916141390800476]\n","Discriminator Loss: 0.5437286063570355\n","Generator Loss: [0.8098227977752686, 0.7855845093727112, 0.012119143269956112]\n","Discriminator Loss: 0.5493579017929733\n","Generator Loss: [0.7675585746765137, 0.7373290061950684, 0.015114790759980679]\n","Discriminator Loss: 0.5459986803762149\n","Generator Loss: [3.309530258178711, 0.7946317195892334, 1.2574492692947388]\n","Discriminator Loss: 0.5427796093936195\n","Generator Loss: [0.8120460510253906, 0.7839106321334839, 0.014067708514630795]\n","Discriminator Loss: 0.5602714566612121\n","Generator Loss: [0.7500638961791992, 0.7300845384597778, 0.009989690966904163]\n","Discriminator Loss: 0.5545948904473335\n","Generator Loss: [0.8490526080131531, 0.8083522319793701, 0.020350199192762375]\n","Discriminator Loss: 0.5725440850510495\n","Generator Loss: [0.7646400928497314, 0.7440179586410522, 0.010311062447726727]\n","Discriminator Loss: 0.567254264940857\n","Generator Loss: [0.788443386554718, 0.7669439315795898, 0.010749739594757557]\n","Discriminator Loss: 0.5472356570098782\n","Generator Loss: [0.8194252252578735, 0.7814922332763672, 0.018966499716043472]\n","Discriminator Loss: 0.5576814938249299\n","Generator Loss: [0.8074730634689331, 0.760407567024231, 0.02353273332118988]\n","Discriminator Loss: 0.5530070969762164\n","Generator Loss: [0.8468227982521057, 0.7930220365524292, 0.026900382712483406]\n","Discriminator Loss: 0.5481040955055505\n","Generator Loss: [0.7927547097206116, 0.7740280032157898, 0.009363357909023762]\n","Discriminator Loss: 0.5470443248041192\n","Generator Loss: [1.0995781421661377, 0.7798867225646973, 0.15984567999839783]\n","Discriminator Loss: 0.5463344775125734\n","Generator Loss: [0.8865312933921814, 0.7790048122406006, 0.05376323312520981]\n","Discriminator Loss: 0.543090616554764\n","Generator Loss: [0.8206599950790405, 0.7942982912063599, 0.013180864043533802]\n","Discriminator Loss: 0.5456779977048427\n","Generator Loss: [0.8008837699890137, 0.7758472561836243, 0.012518263421952724]\n","Discriminator Loss: 0.5537910559446573\n","Generator Loss: [0.7731870412826538, 0.746903121471405, 0.013141955249011517]\n","Discriminator Loss: 0.5573032068714383\n","Generator Loss: [0.8554912209510803, 0.8045686483383179, 0.025461284443736076]\n","Discriminator Loss: 0.5547910511777445\n","Generator Loss: [0.8154646754264832, 0.7977072596549988, 0.008878720924258232]\n","Discriminator Loss: 0.555623015623496\n","Generator Loss: [0.8319921493530273, 0.7668663263320923, 0.032562900334596634]\n","Discriminator Loss: 0.5453421931524645\n","Generator Loss: [0.9201909303665161, 0.7833364009857178, 0.06842725723981857]\n","Discriminator Loss: 0.544238253027288\n","Generator Loss: [0.9226549863815308, 0.7899959087371826, 0.06632955372333527]\n","Discriminator Loss: 0.5453888922165788\n","Generator Loss: [0.8109126091003418, 0.7849401235580444, 0.012986244633793831]\n","Discriminator Loss: 0.5477691140749812\n","Generator Loss: [0.8644218444824219, 0.7831441164016724, 0.04063887149095535]\n","Discriminator Loss: 0.5484914910157386\n","Generator Loss: [0.7911722660064697, 0.7695087194442749, 0.010831772349774837]\n","Discriminator Loss: 0.5563137592016574\n","Generator Loss: [0.8411539793014526, 0.8155509829521179, 0.012801485136151314]\n","Discriminator Loss: 0.5493859759917541\n","Generator Loss: [0.8172919154167175, 0.7733755111694336, 0.02195819653570652]\n","Discriminator Loss: 0.5421878118140739\n","Generator Loss: [0.8205469846725464, 0.7961552143096924, 0.012195873074233532]\n","Discriminator Loss: 0.5841195473731204\n","Generator Loss: [0.7653177380561829, 0.7255308628082275, 0.019893446937203407]\n","Discriminator Loss: 0.563510850081002\n","Generator Loss: [0.8150368928909302, 0.7939968109130859, 0.010520038194954395]\n","Discriminator Loss: 0.5627394690873189\n","Generator Loss: [0.824053168296814, 0.7826219797134399, 0.020715607330203056]\n","Discriminator Loss: 0.5553528062009718\n","Generator Loss: [0.7808042168617249, 0.7575478553771973, 0.011628182604908943]\n","Discriminator Loss: 0.5496476244443329\n","Generator Loss: [0.8557702302932739, 0.78821861743927, 0.03377582132816315]\n","Discriminator Loss: 0.5521263871578412\n","Generator Loss: [0.8002458214759827, 0.7763193845748901, 0.011963224038481712]\n","Discriminator Loss: 0.5541371748768142\n","Generator Loss: [0.8373287320137024, 0.7993485927581787, 0.018990062177181244]\n","Discriminator Loss: 0.5453653851436684\n","Generator Loss: [0.9932347536087036, 0.7650750875473022, 0.11407983303070068]\n","Discriminator Loss: 0.5517749033751898\n","Generator Loss: [0.8984073400497437, 0.7934140563011169, 0.05249662697315216]\n","Discriminator Loss: 0.5421208266197937\n","Generator Loss: [0.8268759250640869, 0.7852173447608948, 0.020829297602176666]\n","Discriminator Loss: 0.5469151433935622\n","Generator Loss: [0.8102611303329468, 0.7846386432647705, 0.012811234220862389]\n","Discriminator Loss: 0.545661949316127\n","Generator Loss: [0.8073521852493286, 0.7895296812057495, 0.008911238051950932]\n","Discriminator Loss: 0.5493572930536175\n","Generator Loss: [1.4402521848678589, 0.7825309038162231, 0.32886064052581787]\n","Discriminator Loss: 0.5580755873888847\n","Generator Loss: [0.7676827907562256, 0.7431984543800354, 0.012242158874869347]\n","Discriminator Loss: 0.5502678363336599\n","Generator Loss: [0.8095694780349731, 0.7913455367088318, 0.00911197904497385]\n","Discriminator Loss: 0.5692525082849897\n","Generator Loss: [0.789210319519043, 0.7514342069625854, 0.018888065591454506]\n","Discriminator Loss: 0.549078651558375\n","Generator Loss: [0.8230480551719666, 0.8010081052780151, 0.011019984260201454]\n","Discriminator Loss: 0.5514262889691963\n","Generator Loss: [0.8040646314620972, 0.7823994159698486, 0.01083260029554367]\n","Discriminator Loss: 0.5464822411831847\n","Generator Loss: [0.8194994926452637, 0.7954961061477661, 0.01200167927891016]\n","Discriminator Loss: 0.5438947981710953\n","Generator Loss: [0.8113717436790466, 0.7898004055023193, 0.010785678401589394]\n","Discriminator Loss: 0.5452094770553231\n","Generator Loss: [0.8350034952163696, 0.7822268605232239, 0.02638830803334713]\n","Discriminator Loss: 0.5442721708404861\n","Generator Loss: [0.8365687131881714, 0.7945362329483032, 0.021016253158450127]\n","Discriminator Loss: 0.5434841604837857\n","Generator Loss: [0.8642985820770264, 0.7798631191253662, 0.04221772775053978]\n","Discriminator Loss: 0.5430783978335967\n","Generator Loss: [0.8034225106239319, 0.7788988947868347, 0.012261809781193733]\n","Discriminator Loss: 0.5481930412670408\n","Generator Loss: [0.7946177124977112, 0.7745500802993774, 0.010033829137682915]\n","Discriminator Loss: 0.5415326829643163\n","Generator Loss: [0.823025643825531, 0.7850488424301147, 0.01898840256035328]\n","Discriminator Loss: 0.5418235861943685\n","Generator Loss: [0.8050798773765564, 0.7865254282951355, 0.009277237579226494]\n","Discriminator Loss: 0.5392657687552855\n","Generator Loss: [0.805907666683197, 0.7860889434814453, 0.009909355081617832]\n","Discriminator Loss: 0.5401546424611752\n","Generator Loss: [0.8010023832321167, 0.7804827690124512, 0.010259808041155338]\n","Discriminator Loss: 0.5401320303208195\n","Generator Loss: [0.8215969800949097, 0.7736636400222778, 0.023966677486896515]\n","Discriminator Loss: 0.5419457601665272\n","Generator Loss: [0.9132380485534668, 0.7759108543395996, 0.06866361200809479]\n","Discriminator Loss: 0.5451534583762623\n","Generator Loss: [0.8306713700294495, 0.8046037554740906, 0.013033797033131123]\n","Discriminator Loss: 0.5439790808668477\n","Generator Loss: [0.9431880712509155, 0.7837622165679932, 0.07971294224262238]\n","Discriminator Loss: 0.5466013889817987\n","Generator Loss: [2.0841379165649414, 0.7494914531707764, 0.6673231720924377]\n","Discriminator Loss: 0.546669874951931\n","Generator Loss: [1.2411328554153442, 0.7961792349815369, 0.22247682511806488]\n","Discriminator Loss: 0.5414986248115383\n","Generator Loss: [0.8361866474151611, 0.8094650506973267, 0.013360793702304363]\n","Discriminator Loss: 0.5413460982672404\n","Generator Loss: [0.8138323426246643, 0.7903468012809753, 0.011742777191102505]\n","Discriminator Loss: 0.5408612623341469\n","Generator Loss: [0.805208683013916, 0.785598874092102, 0.009804909117519855]\n","Discriminator Loss: 0.5467281335550069\n","Generator Loss: [0.7829132676124573, 0.752763032913208, 0.015075113624334335]\n","Discriminator Loss: 0.5564134395099245\n","Generator Loss: [0.8047189712524414, 0.7739923000335693, 0.015363343060016632]\n","Discriminator Loss: 0.5782697820477551\n","Generator Loss: [1.0480928421020508, 0.7806798219680786, 0.13370652496814728]\n","Discriminator Loss: 0.5663306881442622\n","Generator Loss: [0.8266487121582031, 0.7865846157073975, 0.020032046362757683]\n","Discriminator Loss: 0.5539636593020987\n","Generator Loss: [0.8204469680786133, 0.7887986302375793, 0.015824172645807266]\n","Discriminator Loss: 0.5594740893484413\n","Generator Loss: [0.8243579268455505, 0.7670572996139526, 0.0286503117531538]\n","Discriminator Loss: 0.5559173621331865\n","Generator Loss: [0.8051571249961853, 0.7791523337364197, 0.013002404011785984]\n","Discriminator Loss: 0.5497914599600335\n","Generator Loss: [0.8736979365348816, 0.7866717576980591, 0.04351309686899185]\n","Discriminator Loss: 0.5547325072875537\n","Generator Loss: [0.8004030585289001, 0.7783553600311279, 0.01102385576814413]\n","Discriminator Loss: 0.5504177875445748\n","Generator Loss: [0.9824438691139221, 0.7822954654693604, 0.10007419437170029]\n","Discriminator Loss: 0.5534984480873391\n","Generator Loss: [0.7900200486183167, 0.7669215202331543, 0.011549251154065132]\n","Discriminator Loss: 0.5445235124188912\n","Generator Loss: [0.8237498998641968, 0.7968379259109497, 0.013455992564558983]\n","Discriminator Loss: 0.5430775698223442\n","Generator Loss: [0.8165732026100159, 0.7912055253982544, 0.012683829292654991]\n","Discriminator Loss: 0.5478435069799161\n","Generator Loss: [0.9332120418548584, 0.7821989059448242, 0.07550656050443649]\n","Discriminator Loss: 0.5454477096864139\n","Generator Loss: [0.8068732619285583, 0.7775251865386963, 0.014674024656414986]\n","Discriminator Loss: 0.5431794619034918\n","Generator Loss: [0.8128798007965088, 0.7884796857833862, 0.012200063094496727]\n","Discriminator Loss: 0.5552423540202653\n","Generator Loss: [0.8006573915481567, 0.780360996723175, 0.010148201137781143]\n","Discriminator Loss: 0.5460727358076838\n","Generator Loss: [0.8087267875671387, 0.7812708616256714, 0.013727965764701366]\n","Discriminator Loss: 0.5418751710103606\n","Generator Loss: [0.8512411117553711, 0.7853073477745056, 0.032966867089271545]\n","Discriminator Loss: 0.5380128143951879\n","Generator Loss: [0.808394730091095, 0.7797520160675049, 0.014321367256343365]\n","Discriminator Loss: 0.5530762700122978\n","Generator Loss: [0.795987606048584, 0.7712623476982117, 0.01236263383179903]\n","Discriminator Loss: 0.5446218494107598\n","Generator Loss: [0.8127669095993042, 0.7907419800758362, 0.011012454517185688]\n","Discriminator Loss: 0.5453063424492939\n","Generator Loss: [0.8041320443153381, 0.7793152928352356, 0.01240838598459959]\n","Discriminator Loss: 0.5438080735930271\n","Generator Loss: [0.9058980345726013, 0.7717077136039734, 0.06709516048431396]\n","Discriminator Loss: 0.5445772516413854\n","Generator Loss: [0.7967900037765503, 0.7714337706565857, 0.012678123079240322]\n","Discriminator Loss: 0.5482302706113842\n","Generator Loss: [0.8361155986785889, 0.8039038181304932, 0.016105877235531807]\n","Discriminator Loss: 0.5413377887352908\n","Generator Loss: [0.8386483788490295, 0.7906895279884338, 0.023979421705007553]\n","Discriminator Loss: 0.5424190709327377\n","Generator Loss: [0.8540675640106201, 0.805809736251831, 0.024128902703523636]\n","Discriminator Loss: 0.5505550106299779\n","Generator Loss: [0.8121188282966614, 0.7850816249847412, 0.013518615625798702]\n","Discriminator Loss: 0.5464045604153398\n","Generator Loss: [0.8352609276771545, 0.7905657291412354, 0.02234761044383049]\n","Discriminator Loss: 0.5420193568006653\n","Generator Loss: [0.8329918384552002, 0.7917667627334595, 0.020612547174096107]\n","Discriminator Loss: 0.5445653279830367\n","Generator Loss: [0.8095259666442871, 0.7846965789794922, 0.012414693832397461]\n","Discriminator Loss: 0.5453716079991864\n","Generator Loss: [0.8174121379852295, 0.7934470772743225, 0.011982539668679237]\n","Discriminator Loss: 0.547900710873364\n","Generator Loss: [0.8042156100273132, 0.7754963040351868, 0.014359654858708382]\n","Discriminator Loss: 0.5452856240372057\n","Generator Loss: [0.8192713856697083, 0.7772862911224365, 0.020992547273635864]\n","Discriminator Loss: 0.547124614517088\n","Generator Loss: [0.8163111209869385, 0.7846963405609131, 0.0158073790371418]\n","Discriminator Loss: 0.5484848734795378\n","Generator Loss: [0.8104788661003113, 0.7801895141601562, 0.015144662000238895]\n","Discriminator Loss: 0.5433531377120744\n","Generator Loss: [0.8021283745765686, 0.7756234407424927, 0.013252457603812218]\n","Discriminator Loss: 0.5471369167607918\n","Generator Loss: [0.819066047668457, 0.7848461270332336, 0.0171099454164505]\n","Discriminator Loss: 0.5468772127878765\n","Generator Loss: [0.8154690265655518, 0.7952645421028137, 0.010102255269885063]\n","Discriminator Loss: 0.5434137774427654\n","Generator Loss: [0.825400173664093, 0.7868996858596802, 0.01925024390220642]\n","Discriminator Loss: 0.5458446883149008\n","Generator Loss: [0.8056120872497559, 0.7768430709838867, 0.014384521171450615]\n","Discriminator Loss: 0.545843547180084\n","Generator Loss: [0.7953163981437683, 0.7689782381057739, 0.013169080018997192]\n","Discriminator Loss: 0.5442227116673166\n","Generator Loss: [0.7993606328964233, 0.7755042314529419, 0.01192820630967617]\n","Discriminator Loss: 0.5509370026557008\n","Generator Loss: [0.8175123333930969, 0.7461118102073669, 0.03570025786757469]\n","Discriminator Loss: 0.5467468397528137\n","Generator Loss: [0.9588040113449097, 0.7920641899108887, 0.0833698958158493]\n","Discriminator Loss: 0.5486607305228972\n","Generator Loss: [0.8076058030128479, 0.7879549860954285, 0.009825400076806545]\n","Discriminator Loss: 0.5462250664440944\n","Generator Loss: [0.8494506478309631, 0.7808850407600403, 0.03428281471133232]\n","Discriminator Loss: 0.5416376631310413\n","Generator Loss: [0.8102689385414124, 0.7837244868278503, 0.013272212818264961]\n","Discriminator Loss: 0.5485258639964741\n","Generator Loss: [0.8063474893569946, 0.7894119024276733, 0.008467793464660645]\n","Discriminator Loss: 0.5445576066322246\n","Generator Loss: [0.8094823360443115, 0.7886977791786194, 0.010392279364168644]\n","Discriminator Loss: 0.5453608065536173\n","Generator Loss: [0.8412659168243408, 0.7788915634155273, 0.031187165528535843]\n","Discriminator Loss: 0.5491565869488113\n","Generator Loss: [0.7679203152656555, 0.7415220737457275, 0.01319910679012537]\n","Discriminator Loss: 0.5427076152018344\n","Generator Loss: [0.8687092065811157, 0.787497878074646, 0.04060565307736397]\n","Discriminator Loss: 0.5399207963250774\n","Generator Loss: [0.7790848016738892, 0.7576645612716675, 0.010710131376981735]\n","Discriminator Loss: 0.5619362426987209\n","Generator Loss: [0.832444429397583, 0.7911416292190552, 0.020651403814554214]\n","Discriminator Loss: 0.5531907602626234\n","Generator Loss: [0.7966809868812561, 0.7772341966629028, 0.009723389521241188]\n","Discriminator Loss: 0.5534300752742638\n","Generator Loss: [0.841982364654541, 0.7515337467193604, 0.045224305242300034]\n","Discriminator Loss: 0.5479866374625999\n","Generator Loss: [0.8051782846450806, 0.7844098806381226, 0.010384196415543556]\n","Discriminator Loss: 0.5471865359013464\n","Generator Loss: [0.8181458711624146, 0.7985521554946899, 0.009796860627830029]\n","Discriminator Loss: 0.5430939111465705\n","Generator Loss: [0.8250711560249329, 0.7947207093238831, 0.01517521496862173]\n","Discriminator Loss: 0.5394883792068867\n","Generator Loss: [0.8505302667617798, 0.8317312002182007, 0.009399543516337872]\n","Discriminator Loss: 0.5560365357414412\n","Generator Loss: [0.7702921628952026, 0.7494924068450928, 0.01039988361299038]\n","Discriminator Loss: 0.5545107587258826\n","Generator Loss: [0.7954131960868835, 0.7774611711502075, 0.008976010605692863]\n","Discriminator Loss: 0.5502258439564685\n","Generator Loss: [0.8022807240486145, 0.7760863304138184, 0.01309718657284975]\n","Discriminator Loss: 0.5465061252780288\n","Generator Loss: [0.816545844078064, 0.7972500920295715, 0.009647868573665619]\n","Discriminator Loss: 0.546889626486518\n","Generator Loss: [0.8107933402061462, 0.7896926403045654, 0.010550344362854958]\n","Discriminator Loss: 0.5479180185975565\n","Generator Loss: [0.8080822825431824, 0.7810500264167786, 0.013516128994524479]\n","Discriminator Loss: 0.5458700049239269\n","Generator Loss: [0.8143565058708191, 0.7913981676101685, 0.011479168199002743]\n","Discriminator Loss: 0.5440310233479977\n","Generator Loss: [0.8775035738945007, 0.7734941244125366, 0.05200472101569176]\n","Discriminator Loss: 0.5422562747735356\n","Generator Loss: [0.8133511543273926, 0.7918868064880371, 0.010732173919677734]\n","Discriminator Loss: 0.5437771185897873\n","Generator Loss: [0.8140872716903687, 0.7855225801467896, 0.014282353222370148]\n","Discriminator Loss: 0.5442034572806733\n","Generator Loss: [0.8105109930038452, 0.7845847606658936, 0.012963119894266129]\n","Discriminator Loss: 0.5420342092511419\n","Generator Loss: [0.8392807841300964, 0.7845238447189331, 0.02737848274409771]\n","Discriminator Loss: 0.5422413094656804\n","Generator Loss: [0.9031659960746765, 0.7909446358680725, 0.056110680103302]\n","Discriminator Loss: 0.5436352975648333\n","Generator Loss: [0.8108407258987427, 0.7782570719718933, 0.01629183255136013]\n","Discriminator Loss: 0.5396151768018171\n","Generator Loss: [0.849852979183197, 0.7749789953231812, 0.03743700310587883]\n","Discriminator Loss: 0.541401121963645\n","Generator Loss: [0.7854173183441162, 0.7625609636306763, 0.011428162455558777]\n","Discriminator Loss: 0.5384926798724337\n","Generator Loss: [0.8140431046485901, 0.7890462875366211, 0.012498402036726475]\n","Discriminator Loss: 0.5408618597921304\n","Generator Loss: [0.8218239545822144, 0.7883831858634949, 0.01672038808465004]\n","Discriminator Loss: 0.5385538137234107\n","Generator Loss: [1.0227837562561035, 0.780807614326477, 0.12098807096481323]\n","Discriminator Loss: 0.5452747068629833\n","Generator Loss: [0.8096398711204529, 0.7835406064987183, 0.01304963044822216]\n","Discriminator Loss: 0.5495519051810334\n","Generator Loss: [0.8124927282333374, 0.7943472862243652, 0.009072721935808659]\n","Discriminator Loss: 0.5431998044314241\n","Generator Loss: [0.7897972464561462, 0.7715579867362976, 0.009119624271988869]\n","Discriminator Loss: 0.546780160822891\n","Generator Loss: [0.8074912428855896, 0.7892037630081177, 0.009143739007413387]\n","Discriminator Loss: 0.5448159025736459\n","Generator Loss: [0.791771411895752, 0.7684870958328247, 0.011642160825431347]\n","Discriminator Loss: 0.543148789877705\n","Generator Loss: [0.7981076240539551, 0.7773479223251343, 0.010379836894571781]\n","Discriminator Loss: 0.5427901909297361\n","Generator Loss: [0.8201833367347717, 0.7603089809417725, 0.029937168583273888]\n","Discriminator Loss: 0.5441016870217936\n","Generator Loss: [0.868709921836853, 0.789405882358551, 0.03965200483798981]\n","Discriminator Loss: 0.5463684391252173\n","Generator Loss: [0.8177817463874817, 0.7965013980865479, 0.010640163905918598]\n","Discriminator Loss: 0.5453200327938248\n","Generator Loss: [0.8322180509567261, 0.7787971496582031, 0.026710454374551773]\n","Discriminator Loss: 0.5434648347818438\n","Generator Loss: [0.8390034437179565, 0.764269232749939, 0.037367090582847595]\n","Discriminator Loss: 0.5427567453771189\n","Generator Loss: [0.8008642792701721, 0.773577094078064, 0.013643595390021801]\n","Discriminator Loss: 0.5501670059020398\n","Generator Loss: [0.7866394519805908, 0.7679013609886169, 0.009369050152599812]\n","Discriminator Loss: 0.5539191274710902\n","Generator Loss: [0.7880237698554993, 0.7678204774856567, 0.010101635940372944]\n","Discriminator Loss: 0.547191062978527\n","Generator Loss: [0.7801859974861145, 0.7596645951271057, 0.010260691866278648]\n","Discriminator Loss: 0.5464551674049289\n","Generator Loss: [1.123718500137329, 0.7938502430915833, 0.16493411362171173]\n","Discriminator Loss: 0.5516825217018777\n","Generator Loss: [0.7922763228416443, 0.7718087434768677, 0.010233795270323753]\n","Discriminator Loss: 0.5462501787242218\n","Generator Loss: [0.8034802079200745, 0.7723672389984131, 0.015556489117443562]\n","Discriminator Loss: 0.5426593499651062\n","Generator Loss: [0.7970273494720459, 0.7772806882858276, 0.009873325936496258]\n","Discriminator Loss: 0.5465442411104959\n","Generator Loss: [0.804087221622467, 0.7757956385612488, 0.014145796187222004]\n","Discriminator Loss: 0.5421521420039426\n","Generator Loss: [0.7862753868103027, 0.7691974639892578, 0.00853897538036108]\n","Discriminator Loss: 0.5428052376182677\n","Generator Loss: [0.7847228050231934, 0.7662543654441833, 0.009234217926859856]\n","Discriminator Loss: 0.5390708710656327\n","Generator Loss: [0.8154984712600708, 0.7750071287155151, 0.02024567872285843]\n","Discriminator Loss: 0.5406281258501622\n","Generator Loss: [0.7868964672088623, 0.7685211300849915, 0.009187664836645126]\n","Discriminator Loss: 0.5431623321710504\n","Generator Loss: [0.7852360010147095, 0.7636877298355103, 0.01077413372695446]\n","Discriminator Loss: 0.5426315836466529\n","Generator Loss: [0.8254026770591736, 0.8045457601547241, 0.01042847242206335]\n","Discriminator Loss: 0.5482922138253343\n","Generator Loss: [0.7711455225944519, 0.7433445453643799, 0.01390047650784254]\n","Discriminator Loss: 0.5497610626789537\n","Generator Loss: [0.7744368314743042, 0.7567440271377563, 0.008846387267112732]\n","Discriminator Loss: 0.5438867651828332\n","Generator Loss: [0.8308128714561462, 0.8084543347358704, 0.011179279536008835]\n","Discriminator Loss: 0.5485972315173058\n","Generator Loss: [0.8178765773773193, 0.7803823947906494, 0.018747102469205856]\n","Discriminator Loss: 0.5592697182637494\n","Generator Loss: [0.778473973274231, 0.7480857372283936, 0.015194111503660679]\n","Discriminator Loss: 0.5468081230501411\n","Generator Loss: [0.7923184037208557, 0.7663078308105469, 0.013005277141928673]\n","Discriminator Loss: 0.5471602701582015\n","Generator Loss: [0.8296858668327332, 0.7995564937591553, 0.015064695850014687]\n","Discriminator Loss: 0.5524129554378305\n","Generator Loss: [0.8043518662452698, 0.7577584981918335, 0.02329668216407299]\n","Discriminator Loss: 0.5440168045606697\n","Generator Loss: [1.0133180618286133, 0.8027312755584717, 0.105293408036232]\n","Discriminator Loss: 0.5500627203500699\n","Generator Loss: [0.8087503910064697, 0.7709439992904663, 0.018903205171227455]\n","Discriminator Loss: 0.5430041893860107\n","Generator Loss: [0.795177698135376, 0.7743545770645142, 0.010411555878818035]\n","Discriminator Loss: 0.5621754786952806\n","Generator Loss: [0.7770800590515137, 0.7416987419128418, 0.01769065298140049]\n","Discriminator Loss: 0.5485441533455742\n","Generator Loss: [0.7870795726776123, 0.7670218348503113, 0.010028854943811893]\n","Discriminator Loss: 0.5449576841474482\n","Generator Loss: [0.8211842179298401, 0.8012983202934265, 0.009942944161593914]\n","Discriminator Loss: 0.5416517191279127\n","Generator Loss: [1.1355342864990234, 0.7814339399337769, 0.17705020308494568]\n","Discriminator Loss: 0.5554290394538839\n","Generator Loss: [0.8091056942939758, 0.780803382396698, 0.01415115687996149]\n","Discriminator Loss: 0.5447002230721409\n","Generator Loss: [0.792435348033905, 0.7642192840576172, 0.01410803385078907]\n","Discriminator Loss: 0.5468445959231758\n","Generator Loss: [0.810192346572876, 0.7852768301963806, 0.012457766570150852]\n","Discriminator Loss: 0.5448362570314202\n","Generator Loss: [0.7885257601737976, 0.7556972503662109, 0.016414254903793335]\n","Discriminator Loss: 0.543728718021157\n","Generator Loss: [0.8757928609848022, 0.7750421762466431, 0.05037534609436989]\n","Discriminator Loss: 0.5408224795501155\n","Generator Loss: [0.7991735339164734, 0.7794992923736572, 0.009837117046117783]\n","Discriminator Loss: 0.5426881500825402\n","Generator Loss: [0.829108715057373, 0.8067256212234497, 0.011191558092832565]\n","Discriminator Loss: 0.5434466925680681\n","Generator Loss: [0.8061392903327942, 0.7744123935699463, 0.015863435342907906]\n","Discriminator Loss: 0.5398593665158842\n","Generator Loss: [0.827990710735321, 0.7847851514816284, 0.021602779626846313]\n","Discriminator Loss: 0.5405540140145604\n","Generator Loss: [0.8095677495002747, 0.786236047744751, 0.011665858328342438]\n","Discriminator Loss: 0.5438337566556584\n","Generator Loss: [0.822739839553833, 0.7919981479644775, 0.015370849519968033]\n","Discriminator Loss: 0.5492818326192719\n","Generator Loss: [0.8036285042762756, 0.7824770212173462, 0.010575753636658192]\n","Discriminator Loss: 0.5467148223669938\n","Generator Loss: [0.8049960136413574, 0.7720092535018921, 0.016493383795022964]\n","Discriminator Loss: 0.5434318887487279\n","Generator Loss: [0.8277338743209839, 0.7990614771842957, 0.014336185529828072]\n","Discriminator Loss: 0.5466523285276708\n","Generator Loss: [0.8069223165512085, 0.7790757417678833, 0.013923277147114277]\n","Discriminator Loss: 0.5514690407089802\n","Generator Loss: [0.7684298753738403, 0.7424379587173462, 0.012995961122214794]\n","Discriminator Loss: 0.5603437118279544\n","Generator Loss: [0.821113109588623, 0.7488120794296265, 0.036150529980659485]\n","Discriminator Loss: 0.5501835653267335\n","Generator Loss: [0.8238489627838135, 0.7924914360046387, 0.015678770840168]\n","Discriminator Loss: 0.5435434577666456\n","Generator Loss: [1.0365238189697266, 0.8070314526557922, 0.11474616825580597]\n","Discriminator Loss: 0.5465423593541345\n","Generator Loss: [0.8376665711402893, 0.8145250678062439, 0.011570747010409832]\n","Discriminator Loss: 0.5495925780596735\n","Generator Loss: [0.8347758650779724, 0.7688442468643188, 0.03296581283211708]\n","Discriminator Loss: 0.5414186800771859\n","Generator Loss: [0.8168219327926636, 0.7825214266777039, 0.01715024560689926]\n","Discriminator Loss: 0.5402365962090698\n","Generator Loss: [0.8144941329956055, 0.7929072380065918, 0.010793444700539112]\n","Discriminator Loss: 0.5396286577902174\n","Generator Loss: [0.8212894201278687, 0.7991859912872314, 0.011051720008254051]\n","Discriminator Loss: 0.5440033967915952\n","Generator Loss: [0.8549587726593018, 0.8102268576622009, 0.022365953773260117]\n","Discriminator Loss: 0.5446481330200186\n","Generator Loss: [0.8004888892173767, 0.7805137634277344, 0.009987548924982548]\n","Discriminator Loss: 0.5473279736279437\n","Generator Loss: [0.766249418258667, 0.7468915581703186, 0.009678921662271023]\n","Discriminator Loss: 0.545713986472947\n","Generator Loss: [0.828914225101471, 0.7803442478179932, 0.02428499236702919]\n","Discriminator Loss: 0.5456186472547415\n","Generator Loss: [0.8239718079566956, 0.7830682992935181, 0.02045176364481449]\n","Discriminator Loss: 0.5442335996076508\n","Generator Loss: [0.8841897249221802, 0.7875636219978333, 0.048313040286302567]\n","Discriminator Loss: 0.5485963269788954\n","Generator Loss: [0.7913934588432312, 0.7630282044410706, 0.014182614162564278]\n","Discriminator Loss: 0.5407389542087913\n","Generator Loss: [0.8200988173484802, 0.7948657274246216, 0.012616532854735851]\n","Discriminator Loss: 0.5469345915880695\n","Generator Loss: [0.8258500099182129, 0.8017563819885254, 0.012046821415424347]\n","Discriminator Loss: 0.538683251823386\n","Generator Loss: [0.8606825470924377, 0.785703182220459, 0.03748967871069908]\n","Discriminator Loss: 0.5440795223803434\n","Generator Loss: [0.8115379810333252, 0.7914958596229553, 0.01002105325460434]\n","Discriminator Loss: 0.5399460503467708\n","Generator Loss: [0.8126329183578491, 0.7855681777000427, 0.01353236474096775]\n","Discriminator Loss: 0.5383053928562731\n","Generator Loss: [0.8144986033439636, 0.7892184257507324, 0.012640086002647877]\n","Discriminator Loss: 0.5417783122975379\n","Generator Loss: [0.8441717028617859, 0.777393639087677, 0.03338903561234474]\n","Discriminator Loss: 0.53952112244815\n","Generator Loss: [0.8065832257270813, 0.7838940620422363, 0.01134459488093853]\n","Discriminator Loss: 0.541076643847191\n","Generator Loss: [0.8165062665939331, 0.7864042520523071, 0.015050993300974369]\n","Discriminator Loss: 0.5448942253806308\n","Generator Loss: [0.820098340511322, 0.776736319065094, 0.021681003272533417]\n","Discriminator Loss: 0.5469793727224896\n","Generator Loss: [0.7998108863830566, 0.7783684730529785, 0.010721218772232533]\n","Discriminator Loss: 0.5450818852950761\n","Generator Loss: [0.9761871099472046, 0.7766293287277222, 0.09977889806032181]\n","Discriminator Loss: 0.547837002963206\n","Generator Loss: [0.7807422280311584, 0.7531939744949341, 0.013774113729596138]\n","Discriminator Loss: 0.5455731590664072\n","Generator Loss: [0.8137001991271973, 0.79573655128479, 0.008981837891042233]\n","Discriminator Loss: 0.5455631706063286\n","Generator Loss: [0.8406822681427002, 0.8117714524269104, 0.01445540226995945]\n","Discriminator Loss: 0.5326176706003025\n","Generator Loss: [0.846248209476471, 0.8217239379882812, 0.012262130156159401]\n","Discriminator Loss: 0.5454920357078663\n","Generator Loss: [0.9440436363220215, 0.8174493312835693, 0.06329716742038727]\n","Discriminator Loss: 0.5395015753001644\n","Generator Loss: [0.8369947671890259, 0.7989386320114136, 0.019028078764677048]\n","Discriminator Loss: 0.5707447827517171\n","Generator Loss: [0.8189364671707153, 0.7961009740829468, 0.011417745612561703]\n","Discriminator Loss: 0.54338728501898\n","Generator Loss: [0.8013558387756348, 0.7662879228591919, 0.01753394864499569]\n","Discriminator Loss: 0.5421918929205276\n","Generator Loss: [0.8345178365707397, 0.8096882104873657, 0.012414798140525818]\n","Discriminator Loss: 0.5430358630774208\n","Generator Loss: [0.8177187442779541, 0.7969627976417542, 0.01037798821926117]\n","Discriminator Loss: 0.5477442830469954\n","Generator Loss: [0.8126161098480225, 0.7905464172363281, 0.011034861207008362]\n","Discriminator Loss: 0.5553427600780196\n","Generator Loss: [0.7864815592765808, 0.7669785022735596, 0.009751537814736366]\n","Discriminator Loss: 0.5442505517676182\n","Generator Loss: [0.8107640743255615, 0.7822233438491821, 0.014270374551415443]\n","Discriminator Loss: 0.5475512034463463\n","Generator Loss: [0.7962478995323181, 0.7764015197753906, 0.00992319080978632]\n","Discriminator Loss: 0.5464826490242558\n","Generator Loss: [0.82596755027771, 0.7654150724411011, 0.030276240780949593]\n","Discriminator Loss: 0.546607072075858\n","Generator Loss: [0.8258056640625, 0.800584077835083, 0.012610787525773048]\n","Discriminator Loss: 0.5460713618922455\n","Generator Loss: [0.9785301685333252, 0.7828907370567322, 0.0978197306394577]\n","Discriminator Loss: 0.5442556794296252\n","Generator Loss: [0.7997263669967651, 0.7803232669830322, 0.00970153696835041]\n","Discriminator Loss: 0.5524218810205639\n","Generator Loss: [0.767550528049469, 0.7470168471336365, 0.010266835801303387]\n","Discriminator Loss: 0.5648289130876947\n","Generator Loss: [0.7666722536087036, 0.742958128452301, 0.01185707189142704]\n","Discriminator Loss: 0.5513128789334587\n","Generator Loss: [0.886184811592102, 0.8193267583847046, 0.033429019153118134]\n","Discriminator Loss: 0.5514292998996098\n","Generator Loss: [0.8251020908355713, 0.805984377861023, 0.0095588443800807]\n","Discriminator Loss: 0.5549569740251172\n","Generator Loss: [0.7811223268508911, 0.7509028911590576, 0.015109728090465069]\n","Discriminator Loss: 0.5503150646363792\n","Generator Loss: [0.804085373878479, 0.7838120460510254, 0.010136658325791359]\n","Discriminator Loss: 0.5452276605592488\n","Generator Loss: [0.8110920786857605, 0.7935540080070496, 0.008769034408032894]\n","Discriminator Loss: 0.5439420699922266\n","Generator Loss: [0.7982263565063477, 0.7806823253631592, 0.008772024884819984]\n","Discriminator Loss: 0.543750988703323\n","Generator Loss: [0.8207347393035889, 0.7855536937713623, 0.017590517178177834]\n","Discriminator Loss: 0.5458303863670153\n","Generator Loss: [0.8303558826446533, 0.8011704683303833, 0.014592720195651054]\n","Discriminator Loss: 0.5429591231877566\n","Generator Loss: [0.9301798939704895, 0.7699403166770935, 0.0801197960972786]\n","Discriminator Loss: 0.5423600706708385\n","Generator Loss: [0.8510932922363281, 0.7906606197357178, 0.030216332525014877]\n","Discriminator Loss: 0.5391872658692591\n","Generator Loss: [0.8371503949165344, 0.7827634215354919, 0.027193496003746986]\n","Discriminator Loss: 0.5537078424795254\n","Generator Loss: [0.8484216332435608, 0.7993637323379517, 0.024528946727514267]\n","Discriminator Loss: 0.5502965972555103\n","Generator Loss: [0.7953411936759949, 0.7643134593963623, 0.015513879247009754]\n","Discriminator Loss: 0.543945374031864\n","Generator Loss: [1.8319071531295776, 0.7632508277893066, 0.5343281626701355]\n","Discriminator Loss: 0.5341557660140097\n","Generator Loss: [0.8335917592048645, 0.8000074625015259, 0.016792139038443565]\n","Discriminator Loss: 0.5528048753440089\n","Generator Loss: [0.8310573101043701, 0.8053223490715027, 0.01286748331040144]\n","Discriminator Loss: 0.5392992217275605\n","Generator Loss: [0.8172294497489929, 0.7919865846633911, 0.012621444649994373]\n","Discriminator Loss: 0.5377988989243931\n","Generator Loss: [0.795072615146637, 0.7761852741241455, 0.009443681687116623]\n","Discriminator Loss: 0.5380698280969227\n","Generator Loss: [0.8291988372802734, 0.7988312840461731, 0.015183770097792149]\n","Discriminator Loss: 0.5394122527577565\n","Generator Loss: [0.9940713047981262, 0.7987010478973389, 0.09768513590097427]\n","Discriminator Loss: 0.5491675251250854\n","Generator Loss: [0.8130733370780945, 0.7851930856704712, 0.013940135017037392]\n","Discriminator Loss: 0.5503784800312133\n","Generator Loss: [0.8094576001167297, 0.7833698391914368, 0.013043893501162529]\n","Discriminator Loss: 0.5372173289579223\n","Generator Loss: [1.0843111276626587, 0.789091169834137, 0.14760996401309967]\n","Discriminator Loss: 0.5483076105811051\n","Generator Loss: [0.7799314856529236, 0.7538483142852783, 0.013041581958532333]\n","Discriminator Loss: 0.5468158812809634\n","Generator Loss: [0.8532690405845642, 0.8051304817199707, 0.024069277569651604]\n","Discriminator Loss: 0.5409881097266407\n","Generator Loss: [0.8166369795799255, 0.7951685190200806, 0.01073423121124506]\n","Discriminator Loss: 0.5398289087552257\n","Generator Loss: [0.8356255888938904, 0.7947338819503784, 0.020445866510272026]\n","Discriminator Loss: 0.5433221215735102\n","Generator Loss: [0.8423269391059875, 0.8083524107933044, 0.016987277194857597]\n","Discriminator Loss: 0.5383936526368416\n","Generator Loss: [0.8157142400741577, 0.7953187227249146, 0.010197772644460201]\n","Discriminator Loss: 0.5381634565796958\n","Generator Loss: [0.965241551399231, 0.789501428604126, 0.08787006139755249]\n","Discriminator Loss: 0.539934249264661\n","Generator Loss: [0.8413358330726624, 0.7891615033149719, 0.026087157428264618]\n","Discriminator Loss: 0.549609846146268\n","Generator Loss: [0.8496267795562744, 0.8006161451339722, 0.024505309760570526]\n","Discriminator Loss: 0.5506635061601628\n","Generator Loss: [0.8842288851737976, 0.8581836223602295, 0.013022626750171185]\n","Discriminator Loss: 0.543676305264853\n","Generator Loss: [0.8409375548362732, 0.8210417032241821, 0.009947937913239002]\n","Discriminator Loss: 0.5404676390480745\n","Generator Loss: [0.8162779211997986, 0.7972671985626221, 0.009505365043878555]\n","Discriminator Loss: 0.5512909028966533\n","Generator Loss: [0.9125878810882568, 0.8920513987541199, 0.010268238373100758]\n","Discriminator Loss: 0.5460151718507404\n","Generator Loss: [0.8071886897087097, 0.7877373695373535, 0.009725655429065228]\n","Discriminator Loss: 0.5457307983069768\n","Generator Loss: [0.8209620714187622, 0.8025858402252197, 0.009188123047351837]\n","Discriminator Loss: 0.5547108174268942\n","Generator Loss: [0.8179291486740112, 0.7992958426475525, 0.009316666051745415]\n","Discriminator Loss: 0.5427194801050064\n","Generator Loss: [0.796708881855011, 0.7786486744880676, 0.009030099958181381]\n","Discriminator Loss: 0.5456070773093415\n","Generator Loss: [0.8172408938407898, 0.7945253849029541, 0.011357763782143593]\n","Discriminator Loss: 0.5383989072897748\n","Generator Loss: [0.81894850730896, 0.7969187498092651, 0.011014875955879688]\n","Discriminator Loss: 0.5366740996323642\n","Generator Loss: [0.8281928300857544, 0.8041871786117554, 0.012002835050225258]\n","Discriminator Loss: 0.559111456044775\n","Generator Loss: [0.7653259634971619, 0.7379421591758728, 0.013691888190805912]\n","Discriminator Loss: 0.5679977903591862\n","Generator Loss: [0.786525309085846, 0.764048159122467, 0.0112385805696249]\n","Discriminator Loss: 0.5575559111821349\n","Generator Loss: [1.0385655164718628, 0.8214923143386841, 0.10853660106658936]\n","Discriminator Loss: 0.5506931279905984\n","Generator Loss: [0.8846147656440735, 0.7761160135269165, 0.054249368607997894]\n","Discriminator Loss: 0.5436764124824549\n","Generator Loss: [0.8643926978111267, 0.8049987554550171, 0.029696965590119362]\n","Discriminator Loss: 0.557137247483297\n","Generator Loss: [0.8154188990592957, 0.7919087409973145, 0.011755065992474556]\n","Discriminator Loss: 0.5461005569413828\n","Generator Loss: [0.8243896961212158, 0.7926928997039795, 0.015848413109779358]\n","Discriminator Loss: 0.54961975299193\n","Generator Loss: [0.8397694230079651, 0.7987769842147827, 0.020496226847171783]\n","Discriminator Loss: 0.5470877972256858\n","Generator Loss: [0.790906548500061, 0.7662711143493652, 0.012317723594605923]\n","Discriminator Loss: 0.5494759884531959\n","Generator Loss: [0.8265902400016785, 0.7984168529510498, 0.014086684212088585]\n","Discriminator Loss: 0.5482338633737527\n","Generator Loss: [0.8523097634315491, 0.7829445004463196, 0.03468262404203415]\n","Discriminator Loss: 0.5458950017928146\n","Generator Loss: [0.8120595216751099, 0.7915676832199097, 0.010245919227600098]\n","Discriminator Loss: 0.5432611668620666\n","Generator Loss: [0.8234981894493103, 0.7866739630699158, 0.01841212622821331]\n","Discriminator Loss: 0.5441878066030768\n","Generator Loss: [0.8097667694091797, 0.7861303091049194, 0.011818238534033298]\n","Discriminator Loss: 0.5419328407165267\n","Generator Loss: [0.8254041075706482, 0.8068899512290955, 0.009257075376808643]\n","Discriminator Loss: 0.5468739741154423\n","Generator Loss: [0.8019132614135742, 0.781661331653595, 0.010125975124537945]\n","Discriminator Loss: 0.5442060788755043\n","Generator Loss: [0.8009140491485596, 0.779960036277771, 0.010476997122168541]\n","Discriminator Loss: 0.557158696672559\n","Generator Loss: [0.7892807722091675, 0.7693387866020203, 0.009970985352993011]\n","Discriminator Loss: 0.554029898457884\n","Generator Loss: [0.7993775010108948, 0.7768858075141907, 0.011245854198932648]\n","Discriminator Loss: 0.5457823011165601\n","Generator Loss: [0.8252202272415161, 0.806739091873169, 0.00924056675285101]\n","Discriminator Loss: 0.547388953459631\n","Generator Loss: [0.7960354089736938, 0.7772842049598694, 0.009375608526170254]\n","Discriminator Loss: 0.5561399843281833\n","Generator Loss: [0.8507460355758667, 0.8297432661056519, 0.010501369833946228]\n","Discriminator Loss: 0.5590127744671918\n","Generator Loss: [0.8323328495025635, 0.7926100492477417, 0.019861385226249695]\n","Discriminator Loss: 0.5498484538738921\n","Generator Loss: [0.8202590346336365, 0.802228569984436, 0.009015223942697048]\n","Discriminator Loss: 0.5482468253339903\n","Generator Loss: [0.8196210861206055, 0.7986019849777222, 0.010509537532925606]\n","Discriminator Loss: 0.5414167831622763\n","Generator Loss: [0.8218653202056885, 0.7970593571662903, 0.012402968481183052]\n","Discriminator Loss: 0.5555122621681221\n","Generator Loss: [0.8069830536842346, 0.780985414981842, 0.012998831458389759]\n","Discriminator Loss: 0.545432759949108\n","Generator Loss: [0.7653326988220215, 0.7476747035980225, 0.008828997611999512]\n","Discriminator Loss: 0.5482600899631507\n","Generator Loss: [0.7986690402030945, 0.7748011350631714, 0.011933938600122929]\n","Discriminator Loss: 0.5490672442592768\n","Generator Loss: [0.8033587336540222, 0.7615942358970642, 0.02088225819170475]\n","Discriminator Loss: 0.5752288600797328\n","Generator Loss: [0.7708996534347534, 0.6905750036239624, 0.040162310004234314]\n","Discriminator Loss: 0.5744574068994552\n","Generator Loss: [0.7496160864830017, 0.732383131980896, 0.008616476319730282]\n","Discriminator Loss: 0.5538871637691045\n","Generator Loss: [0.8383970856666565, 0.8103505969047546, 0.014023248106241226]\n","Discriminator Loss: 0.5504261626883817\n","Generator Loss: [0.8116855621337891, 0.7918988466262817, 0.00989334937185049]\n","Discriminator Loss: 0.5728900620379136\n","Generator Loss: [0.8302107453346252, 0.7928656339645386, 0.018672563135623932]\n","Discriminator Loss: 0.5575136333882256\n","Generator Loss: [0.7980548739433289, 0.7799599170684814, 0.009047484956681728]\n","Discriminator Loss: 0.5664514636446256\n","Generator Loss: [0.916066586971283, 0.7646991014480591, 0.07568374276161194]\n","Discriminator Loss: 0.5765374429902295\n","Generator Loss: [0.788608968257904, 0.7603639960289001, 0.014122485183179379]\n","Discriminator Loss: 0.5549807172210421\n","Generator Loss: [0.7781829237937927, 0.7304341793060303, 0.023874372243881226]\n","Discriminator Loss: 0.5667200441748719\n","Generator Loss: [0.7973254919052124, 0.7762003540992737, 0.01056255679577589]\n","Discriminator Loss: 0.5628302019904368\n","Generator Loss: [0.7930645942687988, 0.7667819261550903, 0.013141347095370293]\n","Discriminator Loss: 0.5568904929532437\n","Generator Loss: [0.8476621508598328, 0.8125483393669128, 0.017556896433234215]\n","Discriminator Loss: 0.5521762395164842\n","Generator Loss: [0.8092837333679199, 0.7843754887580872, 0.012454131618142128]\n","Discriminator Loss: 0.5508644825986266\n","Generator Loss: [1.0099267959594727, 0.7783845067024231, 0.11577111482620239]\n","Discriminator Loss: 0.5505249795569398\n","Generator Loss: [0.786548912525177, 0.7615717649459839, 0.012488567270338535]\n","Discriminator Loss: 0.5485965097868757\n","Generator Loss: [0.8004923462867737, 0.7750290632247925, 0.012731640599668026]\n","Discriminator Loss: 0.5474196732138807\n","Generator Loss: [0.7374013662338257, 0.7188246250152588, 0.009288376197218895]\n","Discriminator Loss: 0.5740762206660293\n","Generator Loss: [0.7765618562698364, 0.7579359412193298, 0.009312965907156467]\n","Discriminator Loss: 0.5879592524943291\n","Generator Loss: [0.7893524765968323, 0.7624345421791077, 0.013458960689604282]\n","Discriminator Loss: 0.5959783910075203\n","Generator Loss: [1.497661828994751, 0.7649433612823486, 0.36635923385620117]\n","Discriminator Loss: 0.5664646333316341\n","Generator Loss: [0.7513328194618225, 0.7282997369766235, 0.011516543105244637]\n","Discriminator Loss: 0.5573196547029511\n","Generator Loss: [0.8176361322402954, 0.7800676226615906, 0.01878424361348152]\n","Discriminator Loss: 0.5481062586404732\n","Generator Loss: [0.7866843342781067, 0.7677309513092041, 0.009476698935031891]\n","Discriminator Loss: 0.5497913593353587\n","Generator Loss: [0.7712846398353577, 0.7502889037132263, 0.010497857816517353]\n","Discriminator Loss: 0.5523543078343209\n","Generator Loss: [0.80635666847229, 0.7716049551963806, 0.017375865951180458]\n","Discriminator Loss: 0.5491999897549249\n","Generator Loss: [0.7966285943984985, 0.7450274229049683, 0.025800582021474838]\n","Discriminator Loss: 0.5610041592844937\n","Generator Loss: [0.7944730520248413, 0.7635919451713562, 0.01544056087732315]\n","Discriminator Loss: 0.5504067366400704\n","Generator Loss: [0.8534766435623169, 0.8339550495147705, 0.009760808199644089]\n","Discriminator Loss: 0.5542919858835376\n","Generator Loss: [0.858790934085846, 0.7689660787582397, 0.044912420213222504]\n","Discriminator Loss: 0.5558773230804945\n","Generator Loss: [0.8256743550300598, 0.7697908878326416, 0.027941742911934853]\n","Discriminator Loss: 0.5424448028497864\n","Generator Loss: [0.8904469013214111, 0.7627431154251099, 0.06385190039873123]\n","Discriminator Loss: 0.5432025026857445\n","Generator Loss: [4.131214618682861, 0.7559055089950562, 1.6876544952392578]\n","Discriminator Loss: 0.5466275138787751\n","Generator Loss: [0.797605037689209, 0.7781690359115601, 0.00971800833940506]\n","Discriminator Loss: 0.5406943334910466\n","Generator Loss: [0.8300107717514038, 0.8004031777381897, 0.014803807251155376]\n","Discriminator Loss: 0.5601559431561327\n","Generator Loss: [0.8073062896728516, 0.7865533828735352, 0.01037645060569048]\n","Discriminator Loss: 0.5518535099254223\n","Generator Loss: [0.8059966564178467, 0.7866238951683044, 0.009686370380222797]\n","Discriminator Loss: 0.5509287177610531\n","Generator Loss: [0.8136483430862427, 0.7917624711990356, 0.01094293873757124]\n","Discriminator Loss: 0.5498523883779853\n","Generator Loss: [0.8150426149368286, 0.7826905846595764, 0.0161760151386261]\n","Discriminator Loss: 0.5438697984427563\n","Generator Loss: [0.9061377048492432, 0.795982837677002, 0.05507742241024971]\n","Discriminator Loss: 0.5475764234361122\n","Generator Loss: [0.8299992084503174, 0.8002109527587891, 0.01489413995295763]\n","Discriminator Loss: 0.5447929597276016\n","Generator Loss: [0.9722870588302612, 0.7610907554626465, 0.10559815168380737]\n","Discriminator Loss: 0.5509949228417099\n","Generator Loss: [0.8025916218757629, 0.7836925983428955, 0.009449513629078865]\n","Discriminator Loss: 0.5483916253433563\n","Generator Loss: [0.8166987895965576, 0.7946832776069641, 0.011007750406861305]\n","Discriminator Loss: 0.5428924714942696\n","Generator Loss: [0.875834584236145, 0.7966552972793579, 0.03958962857723236]\n","Discriminator Loss: 0.5517586144014786\n","Generator Loss: [0.8322275876998901, 0.7850000262260437, 0.023613765835762024]\n","Discriminator Loss: 0.552779504987484\n","Generator Loss: [0.8476854562759399, 0.7609381675720215, 0.04337364435195923]\n","Discriminator Loss: 0.5566081436681998\n","Generator Loss: [0.7972656488418579, 0.7639681100845337, 0.016648773103952408]\n","Discriminator Loss: 0.5450813042252776\n","Generator Loss: [1.230022668838501, 0.7857381701469421, 0.22214221954345703]\n","Discriminator Loss: 0.5461522658638387\n","Generator Loss: [0.8028101921081543, 0.7851681709289551, 0.008821012452244759]\n","Discriminator Loss: 0.539926747118443\n","Generator Loss: [1.037856101989746, 0.8085880875587463, 0.11463400721549988]\n","Discriminator Loss: 0.5428532076248302\n","Generator Loss: [0.824224591255188, 0.8048587441444397, 0.00968293845653534]\n","Discriminator Loss: 0.5564072724137077\n","Generator Loss: [0.8025398850440979, 0.7763240337371826, 0.013107934035360813]\n","Discriminator Loss: 0.5495905810048498\n","Generator Loss: [0.8267600536346436, 0.8041484951972961, 0.01130579225718975]\n","Discriminator Loss: 0.5483302272768924\n","Generator Loss: [0.7832785844802856, 0.758913516998291, 0.01218253280967474]\n","Discriminator Loss: 0.5401113001735212\n","Generator Loss: [0.8212036490440369, 0.7980595827102661, 0.011572045274078846]\n","Discriminator Loss: 0.5487429258228076\n","Generator Loss: [0.8285112977027893, 0.804111897945404, 0.01219970639795065]\n","Discriminator Loss: 0.5519017545066163\n","Generator Loss: [0.798206090927124, 0.7714821100234985, 0.013361990451812744]\n","Discriminator Loss: 0.5439516729347815\n","Generator Loss: [0.8120629191398621, 0.7849525213241577, 0.013555210083723068]\n","Discriminator Loss: 0.544577103217307\n","Generator Loss: [0.8101286292076111, 0.79304039478302, 0.008544116280972958]\n","Discriminator Loss: 0.558741351597746\n","Generator Loss: [0.8179062604904175, 0.7901860475540161, 0.013860105536878109]\n","Discriminator Loss: 0.5509391915766173\n","Generator Loss: [0.7795456647872925, 0.7498907446861267, 0.014827447943389416]\n","Discriminator Loss: 0.5548652723464329\n","Generator Loss: [0.8081424832344055, 0.7865227460861206, 0.010809879750013351]\n","Discriminator Loss: 0.5441029682096996\n","Generator Loss: [0.788047730922699, 0.7645379304885864, 0.01175489742308855]\n","Discriminator Loss: 0.5495524088310049\n","Generator Loss: [0.7966260313987732, 0.773684024810791, 0.011471008881926537]\n","Discriminator Loss: 0.5481349890760612\n","Generator Loss: [0.8105477094650269, 0.7777597904205322, 0.016393963247537613]\n","Discriminator Loss: 0.5461918090068139\n","Generator Loss: [0.7936173677444458, 0.7735887765884399, 0.010014282539486885]\n","Discriminator Loss: 0.5415771223342745\n","Generator Loss: [0.8107522130012512, 0.7938164472579956, 0.008467871695756912]\n","Discriminator Loss: 0.5503560169236152\n","Generator Loss: [0.8132396936416626, 0.7680872678756714, 0.02257622592151165]\n","Discriminator Loss: 0.5470506623478286\n","Generator Loss: [0.806760311126709, 0.7848176956176758, 0.010971320793032646]\n","Discriminator Loss: 0.5509886539293802\n","Generator Loss: [0.8049789071083069, 0.7823070287704468, 0.011335952207446098]\n","Discriminator Loss: 0.5444559323623253\n","Generator Loss: [0.8386272192001343, 0.8080352544784546, 0.015295978635549545]\n","Discriminator Loss: 0.5476613916907809\n","Generator Loss: [0.8067638278007507, 0.7830793857574463, 0.011842209845781326]\n","Discriminator Loss: 0.5582933632795175\n","Generator Loss: [0.7873835563659668, 0.7552552223205566, 0.016064172610640526]\n","Discriminator Loss: 0.543851617265318\n","Generator Loss: [0.8120889067649841, 0.7930433750152588, 0.009522775188088417]\n","Discriminator Loss: 0.5499948009455693\n","Generator Loss: [0.8031290769577026, 0.7826583385467529, 0.010235371068120003]\n","Discriminator Loss: 0.5468607583625271\n","Generator Loss: [0.8601564764976501, 0.8013674020767212, 0.02939452975988388]\n","Discriminator Loss: 0.5444798536555027\n","Generator Loss: [0.804942786693573, 0.7802486419677734, 0.012347079813480377]\n","Discriminator Loss: 0.5475066811195575\n","Generator Loss: [1.0139672756195068, 0.7908450365066528, 0.11156110465526581]\n","Discriminator Loss: 0.5485167851147708\n","Generator Loss: [0.8089215755462646, 0.7826215028762817, 0.013150044716894627]\n","Discriminator Loss: 0.5438452839007368\n","Generator Loss: [0.8201649188995361, 0.7875155210494995, 0.016324710100889206]\n","Discriminator Loss: 0.5449942882669347\n","Generator Loss: [0.8035411238670349, 0.7845255136489868, 0.009507816284894943]\n","Discriminator Loss: 0.5457096310965426\n","Generator Loss: [0.7756590843200684, 0.7475355267524719, 0.014061788097023964]\n","Discriminator Loss: 0.5505528400462936\n","Generator Loss: [0.7923810482025146, 0.7724745273590088, 0.009953251108527184]\n","Discriminator Loss: 0.5468296419712715\n","Generator Loss: [0.9117945432662964, 0.7994364500045776, 0.05617906153202057]\n","Discriminator Loss: 0.5406385922760819\n","Generator Loss: [0.8137578964233398, 0.7873616218566895, 0.01319815218448639]\n","Discriminator Loss: 0.5425622041384486\n","Generator Loss: [0.8080639839172363, 0.7876549959182739, 0.010204507037997246]\n","Discriminator Loss: 0.5444763698960742\n","Generator Loss: [1.419407606124878, 0.7802708745002747, 0.31956833600997925]\n","Discriminator Loss: 0.5424227775220061\n","Generator Loss: [0.9563345909118652, 0.7849236726760864, 0.08570545166730881]\n","Discriminator Loss: 0.5460775418796402\n","Generator Loss: [0.8309469223022461, 0.806456983089447, 0.012244965881109238]\n","Discriminator Loss: 0.5423931043005723\n","Generator Loss: [4.068415641784668, 0.7712340354919434, 1.6485909223556519]\n","Discriminator Loss: 0.5423700430983445\n","Generator Loss: [0.8193148374557495, 0.7977164387702942, 0.010799199342727661]\n","Discriminator Loss: 0.5549647712323349\n","Generator Loss: [0.7883275151252747, 0.7647458910942078, 0.01179080456495285]\n","Discriminator Loss: 0.5446720697964338\n","Generator Loss: [0.9703303575515747, 0.7716755867004395, 0.09932739287614822]\n","Discriminator Loss: 0.5461632108799677\n","Generator Loss: [0.9201160669326782, 0.7912739515304565, 0.06442106515169144]\n","Discriminator Loss: 0.5452633123404667\n","Generator Loss: [0.7918936014175415, 0.7690019607543945, 0.011445810087025166]\n","Discriminator Loss: 0.5446796225005528\n","Generator Loss: [0.7844954133033752, 0.7630650401115417, 0.010715184733271599]\n","Discriminator Loss: 0.5435799779424997\n","Generator Loss: [0.7850262522697449, 0.7657517194747925, 0.009637255221605301]\n","Discriminator Loss: 0.5428995051333914\n","Generator Loss: [0.7892786860466003, 0.7638319730758667, 0.012723357416689396]\n","Discriminator Loss: 0.5439220403613945\n","Generator Loss: [0.8348655104637146, 0.7904989719390869, 0.022183261811733246]\n","Discriminator Loss: 0.5453829459893313\n","Generator Loss: [0.8085653781890869, 0.7889684438705444, 0.009798459708690643]\n","Discriminator Loss: 0.537460687726707\n","Generator Loss: [0.797023594379425, 0.7731715440750122, 0.011926017701625824]\n","Discriminator Loss: 0.5429085453670268\n","Generator Loss: [1.0619925260543823, 0.7736617922782898, 0.14416538178920746]\n","Discriminator Loss: 0.5554975705217657\n","Generator Loss: [0.8264878392219543, 0.8005975484848022, 0.012945142574608326]\n","Discriminator Loss: 0.5459640030894661\n","Generator Loss: [0.7873396873474121, 0.7650350332260132, 0.011152323335409164]\n","Discriminator Loss: 0.5494483746497281\n","Generator Loss: [0.8388660550117493, 0.7994633913040161, 0.01970132254064083]\n","Discriminator Loss: 0.5413038155929826\n","Generator Loss: [0.8495748043060303, 0.7897071838378906, 0.029933812096714973]\n","Discriminator Loss: 0.5454766603961616\n","Generator Loss: [0.7958503365516663, 0.7681082487106323, 0.013871032744646072]\n","Discriminator Loss: 0.5560897740524524\n","Generator Loss: [0.781118631362915, 0.7623741030693054, 0.00937227625399828]\n","Discriminator Loss: 0.5464529417213271\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 5/10 [1:38:17<1:16:26, 917.31s/it] "]},{"name":"stdout","output_type":"stream","text":["Generator Loss: [0.81328284740448, 0.7890542149543762, 0.012114319950342178]\n","Epoch: 5\n","Discriminator Loss: 0.5415784695687762\n","Generator Loss: [0.8042622208595276, 0.7816140651702881, 0.011324082501232624]\n","Discriminator Loss: 0.5444556025213387\n","Generator Loss: [0.8073804378509521, 0.7778698205947876, 0.014755300246179104]\n","Discriminator Loss: 0.5443500910605508\n","Generator Loss: [0.8139985799789429, 0.7919936180114746, 0.01100246887654066]\n","Discriminator Loss: 0.5480192042523413\n","Generator Loss: [0.8024030327796936, 0.7664250135421753, 0.01798899658024311]\n","Discriminator Loss: 0.5463639624158532\n","Generator Loss: [1.0365960597991943, 0.7776924967765808, 0.12945176661014557]\n","Discriminator Loss: 0.545099601498805\n","Generator Loss: [1.1435757875442505, 0.7755281925201416, 0.18402381241321564]\n","Discriminator Loss: 0.5453717750151554\n","Generator Loss: [0.8194019794464111, 0.7900916337966919, 0.014655167236924171]\n","Discriminator Loss: 0.54110787863101\n","Generator Loss: [0.8125702142715454, 0.7841219902038574, 0.014224113896489143]\n","Discriminator Loss: 0.5465355790074682\n","Generator Loss: [0.8734807968139648, 0.789117693901062, 0.04218156263232231]\n","Discriminator Loss: 0.5450385189797089\n","Generator Loss: [0.8490784168243408, 0.7757018804550171, 0.036688271909952164]\n","Discriminator Loss: 0.5448304904457473\n","Generator Loss: [0.8315144181251526, 0.7811973094940186, 0.025158558040857315]\n","Discriminator Loss: 0.541677619503389\n","Generator Loss: [0.8083696365356445, 0.7844634056091309, 0.011953127570450306]\n","Discriminator Loss: 0.5507596302431921\n","Generator Loss: [0.7971549034118652, 0.7665771245956421, 0.015288891270756721]\n","Discriminator Loss: 0.5463659019915212\n","Generator Loss: [0.8379082679748535, 0.8115027546882629, 0.013202765956521034]\n","Discriminator Loss: 0.5463313390628173\n","Generator Loss: [0.8299089074134827, 0.7622839212417603, 0.0338125042617321]\n","Discriminator Loss: 0.5469172202720074\n","Generator Loss: [0.792056679725647, 0.7541734576225281, 0.018941618502140045]\n","Discriminator Loss: 0.5450479954834009\n","Generator Loss: [0.8079256415367126, 0.7812005281448364, 0.013362560421228409]\n","Discriminator Loss: 0.5454885754897987\n","Generator Loss: [0.7960814237594604, 0.776526689529419, 0.009777354076504707]\n","Discriminator Loss: 0.5403582926373929\n","Generator Loss: [0.8138759732246399, 0.7756034135818481, 0.01913628727197647]\n","Discriminator Loss: 0.5401376011741377\n","Generator Loss: [0.8046907782554626, 0.7806733846664429, 0.012008709833025932]\n","Discriminator Loss: 0.5420355277810813\n","Generator Loss: [0.8628664612770081, 0.7866852879524231, 0.03809059038758278]\n","Discriminator Loss: 0.5465124233123788\n","Generator Loss: [0.8038824200630188, 0.7801564335823059, 0.011862989515066147]\n","Discriminator Loss: 0.5435268066303252\n","Generator Loss: [0.8654435873031616, 0.787855327129364, 0.0387941338121891]\n","Discriminator Loss: 0.54623641858052\n","Generator Loss: [0.7995061874389648, 0.7621281743049622, 0.018689006567001343]\n","Discriminator Loss: 0.5547255367382604\n","Generator Loss: [0.8197953104972839, 0.7975504994392395, 0.011122417636215687]\n","Discriminator Loss: 0.5649170740707632\n","Generator Loss: [0.8454042673110962, 0.7590160369873047, 0.04319411888718605]\n","Discriminator Loss: 0.5422434884712857\n","Generator Loss: [0.8327721357345581, 0.810849130153656, 0.010961513966321945]\n","Discriminator Loss: 0.5488827576373296\n","Generator Loss: [0.7914842367172241, 0.7716445922851562, 0.009919817559421062]\n","Discriminator Loss: 0.54674221552159\n","Generator Loss: [0.786973237991333, 0.7681373357772827, 0.009417964145541191]\n","Discriminator Loss: 0.5556774993347062\n","Generator Loss: [0.8117714524269104, 0.7947340607643127, 0.008518708869814873]\n","Discriminator Loss: 0.5417589945673171\n","Generator Loss: [0.8042271137237549, 0.7832174897193909, 0.010504798032343388]\n","Discriminator Loss: 0.5422912645917677\n","Generator Loss: [0.8291686177253723, 0.8030126094818115, 0.013078009709715843]\n","Discriminator Loss: 0.5440056011648267\n","Generator Loss: [0.8007150888442993, 0.7784832715988159, 0.011115905828773975]\n","Discriminator Loss: 0.5468787972331484\n","Generator Loss: [0.7681073546409607, 0.7494786977767944, 0.009314325638115406]\n","Discriminator Loss: 0.5493368811785331\n","Generator Loss: [0.8305436968803406, 0.8090397715568542, 0.010751950554549694]\n","Discriminator Loss: 0.5479396295286278\n","Generator Loss: [0.8421747088432312, 0.8076416850090027, 0.01726650446653366]\n","Discriminator Loss: 0.5438268670841353\n","Generator Loss: [0.8044809103012085, 0.7719308137893677, 0.01627504825592041]\n","Discriminator Loss: 0.5500169579299836\n","Generator Loss: [0.7640132904052734, 0.7474675178527832, 0.008272886276245117]\n","Discriminator Loss: 0.5435650977324258\n","Generator Loss: [0.9051645994186401, 0.7887434959411621, 0.058210548013448715]\n","Discriminator Loss: 0.5655255459514592\n","Generator Loss: [0.791461169719696, 0.7709991931915283, 0.010230974294245243]\n","Discriminator Loss: 0.5618221638887917\n","Generator Loss: [0.7672520279884338, 0.742089569568634, 0.012581218965351582]\n","Discriminator Loss: 0.5612235301050532\n","Generator Loss: [0.7974498867988586, 0.7591242790222168, 0.019162815064191818]\n","Discriminator Loss: 0.5538558789321542\n","Generator Loss: [0.8240910172462463, 0.8021400570869446, 0.010975469835102558]\n","Discriminator Loss: 0.5494276079953124\n","Generator Loss: [0.803145706653595, 0.7790670990943909, 0.012039292603731155]\n","Discriminator Loss: 0.5530813322566246\n","Generator Loss: [0.800702691078186, 0.781810998916626, 0.009445839561522007]\n","Discriminator Loss: 0.5519642621075036\n","Generator Loss: [1.0170907974243164, 0.7486089468002319, 0.13424089550971985]\n","Discriminator Loss: 0.5569359192850243\n","Generator Loss: [0.853658139705658, 0.7800337076187134, 0.03681220859289169]\n","Discriminator Loss: 0.5455627694427676\n","Generator Loss: [0.8361011743545532, 0.8147834539413452, 0.01065884716808796]\n","Discriminator Loss: 0.5443964881233114\n","Generator Loss: [0.8147925138473511, 0.7884448170661926, 0.01317383348941803]\n","Discriminator Loss: 0.5456437750726764\n","Generator Loss: [0.8002573251724243, 0.7742478847503662, 0.013004731386899948]\n","Discriminator Loss: 0.5454364812103449\n","Generator Loss: [0.7978182435035706, 0.7701207399368286, 0.013848737813532352]\n","Discriminator Loss: 0.5511244130975683\n","Generator Loss: [0.7929332852363586, 0.7721834778785706, 0.010374907404184341]\n","Discriminator Loss: 0.5464742554431723\n","Generator Loss: [0.8192988038063049, 0.7930775880813599, 0.01311061903834343]\n","Discriminator Loss: 0.5466648206183891\n","Generator Loss: [0.8285803198814392, 0.8079578876495361, 0.010311228223145008]\n","Discriminator Loss: 0.5457841830939287\n","Generator Loss: [0.8166005611419678, 0.7730657458305359, 0.02176741510629654]\n","Discriminator Loss: 0.5449386947657331\n","Generator Loss: [0.906366765499115, 0.7848547697067261, 0.06075599417090416]\n","Discriminator Loss: 0.5469203592401755\n","Generator Loss: [0.8472056984901428, 0.7994059324264526, 0.023899879306554794]\n","Discriminator Loss: 0.5531599139812897\n","Generator Loss: [0.8128911256790161, 0.7941216230392456, 0.009384760633111]\n","Discriminator Loss: 0.5569734869350214\n","Generator Loss: [0.8290542364120483, 0.8110341429710388, 0.009010054171085358]\n","Discriminator Loss: 0.558681767566668\n","Generator Loss: [0.832658052444458, 0.7520859241485596, 0.04028605297207832]\n","Discriminator Loss: 0.5555040882791218\n","Generator Loss: [0.8122320175170898, 0.7862186431884766, 0.01300668902695179]\n","Discriminator Loss: 0.5521841053432581\n","Generator Loss: [0.8175338506698608, 0.7999262809753418, 0.008803796954452991]\n","Discriminator Loss: 0.5447526683819888\n","Generator Loss: [0.8007380366325378, 0.7827016115188599, 0.009018214419484138]\n","Discriminator Loss: 0.5470378641912248\n","Generator Loss: [0.8415660262107849, 0.7829298377037048, 0.02931809611618519]\n","Discriminator Loss: 0.5456228811017354\n","Generator Loss: [0.8002151846885681, 0.7717792987823486, 0.01421793270856142]\n","Discriminator Loss: 0.5429283600533381\n","Generator Loss: [0.8200519680976868, 0.790152370929718, 0.014949798583984375]\n","Discriminator Loss: 0.5515957848838298\n","Generator Loss: [0.8084460496902466, 0.7704896926879883, 0.018978163599967957]\n","Discriminator Loss: 0.5542083616028322\n","Generator Loss: [0.7806633710861206, 0.7606322765350342, 0.010015546344220638]\n","Discriminator Loss: 0.5469133272190447\n","Generator Loss: [0.8447083234786987, 0.8116031885147095, 0.016552580520510674]\n","Discriminator Loss: 0.5462620923208306\n","Generator Loss: [0.7968460321426392, 0.7737418413162231, 0.011552102863788605]\n","Discriminator Loss: 0.5478637451578834\n","Generator Loss: [0.8027794361114502, 0.7859524488449097, 0.008413491770625114]\n","Discriminator Loss: 0.5463756263270625\n","Generator Loss: [0.7856998443603516, 0.7664162516593933, 0.009641803801059723]\n","Discriminator Loss: 0.5539218518442794\n","Generator Loss: [0.8079206347465515, 0.7678567171096802, 0.020031949505209923]\n","Discriminator Loss: 0.5453072426462313\n","Generator Loss: [0.892233669757843, 0.7659183740615845, 0.06315764784812927]\n","Discriminator Loss: 0.5499900556606008\n","Generator Loss: [0.8156059384346008, 0.7485798001289368, 0.03351307660341263]\n","Discriminator Loss: 0.5504203002055874\n","Generator Loss: [0.8001708388328552, 0.7833565473556519, 0.008407151326537132]\n","Discriminator Loss: 0.5443845779482217\n","Generator Loss: [0.804606020450592, 0.7859771251678467, 0.009314454160630703]\n","Discriminator Loss: 0.5420981046881934\n","Generator Loss: [0.807989776134491, 0.7895898222923279, 0.009199988096952438]\n","Discriminator Loss: 0.5485985326049558\n","Generator Loss: [0.8127232789993286, 0.7915953397750854, 0.010563976131379604]\n","Discriminator Loss: 0.5691709884376905\n","Generator Loss: [0.853570818901062, 0.8081843852996826, 0.0226932093501091]\n","Discriminator Loss: 0.5477938846051984\n","Generator Loss: [0.8018626570701599, 0.7788165807723999, 0.011523034423589706]\n","Discriminator Loss: 0.5467329250641342\n","Generator Loss: [0.7960054874420166, 0.7715979218482971, 0.012203794904053211]\n","Discriminator Loss: 0.5435037267816369\n","Generator Loss: [0.8655149936676025, 0.7840718030929565, 0.040721602737903595]\n","Discriminator Loss: 0.5530547583839507\n","Generator Loss: [0.8164520859718323, 0.7855499982833862, 0.015451057814061642]\n","Discriminator Loss: 0.5496804302165401\n","Generator Loss: [0.8241154551506042, 0.7760563492774963, 0.024029552936553955]\n","Discriminator Loss: 0.5431528558328864\n","Generator Loss: [0.9649368524551392, 0.8004496097564697, 0.08224360644817352]\n","Discriminator Loss: 0.5490316761024587\n","Generator Loss: [0.8185381889343262, 0.7978456616401672, 0.010346255265176296]\n","Discriminator Loss: 0.5449343382988445\n","Generator Loss: [0.8559457659721375, 0.7811044454574585, 0.03742066025733948]\n","Discriminator Loss: 0.5456951039423075\n","Generator Loss: [0.8030651211738586, 0.7828785181045532, 0.010093308053910732]\n","Discriminator Loss: 0.5475269271137222\n","Generator Loss: [0.8119061589241028, 0.7937938570976257, 0.009056150913238525]\n","Discriminator Loss: 0.5428209692945529\n","Generator Loss: [0.9669053554534912, 0.7966342568397522, 0.0851355642080307]\n","Discriminator Loss: 0.5446333123873046\n","Generator Loss: [0.8045520782470703, 0.7863069772720337, 0.009122554212808609]\n","Discriminator Loss: 0.5459341405075975\n","Generator Loss: [0.7857733964920044, 0.7601377964019775, 0.012817793525755405]\n","Discriminator Loss: 0.5447060163369315\n","Generator Loss: [0.8375232219696045, 0.7840004563331604, 0.026761388406157494]\n","Discriminator Loss: 0.5405817624050542\n","Generator Loss: [0.8153809905052185, 0.7852532267570496, 0.015063873492181301]\n","Discriminator Loss: 0.545139345118514\n","Generator Loss: [0.8202793002128601, 0.7904943227767944, 0.014892490580677986]\n","Discriminator Loss: 0.5386540700774276\n","Generator Loss: [0.8111782073974609, 0.7846218943595886, 0.013278156518936157]\n","Discriminator Loss: 0.54220546371198\n","Generator Loss: [0.8243387341499329, 0.7827860116958618, 0.02077636867761612]\n","Discriminator Loss: 0.5496654964299523\n","Generator Loss: [0.8525837063789368, 0.7883932590484619, 0.032095231115818024]\n","Discriminator Loss: 0.5433041525820954\n","Generator Loss: [0.8266310691833496, 0.7717783451080322, 0.027426347136497498]\n","Discriminator Loss: 0.5422130382987689\n","Generator Loss: [0.8076377511024475, 0.7891308069229126, 0.009253481402993202]\n","Discriminator Loss: 0.5492320964021928\n","Generator Loss: [0.7829751968383789, 0.7620800733566284, 0.010447571985423565]\n","Discriminator Loss: 0.5445263942474412\n","Generator Loss: [0.8252843618392944, 0.7879876494407654, 0.01864837110042572]\n","Discriminator Loss: 0.5463746471959894\n","Generator Loss: [0.8678528070449829, 0.7891013622283936, 0.039375729858875275]\n","Discriminator Loss: 0.5437560673699409\n","Generator Loss: [0.7967984676361084, 0.7744749784469604, 0.011161734350025654]\n","Discriminator Loss: 0.5435008334161466\n","Generator Loss: [0.8011084794998169, 0.780788779258728, 0.01015984732657671]\n","Discriminator Loss: 0.542924169629714\n","Generator Loss: [0.8008469939231873, 0.7804360389709473, 0.010205483995378017]\n","Discriminator Loss: 0.5418628813486066\n","Generator Loss: [0.7934072017669678, 0.7713258266448975, 0.011040699668228626]\n","Discriminator Loss: 0.5447680160796153\n","Generator Loss: [0.8269144892692566, 0.796489417552948, 0.015212532132863998]\n","Discriminator Loss: 0.5455838370335186\n","Generator Loss: [0.7924482822418213, 0.7699182033538818, 0.011265034787356853]\n","Discriminator Loss: 0.5475405522893197\n","Generator Loss: [0.7877812385559082, 0.766695499420166, 0.010542882606387138]\n","Discriminator Loss: 0.5429252565354545\n","Generator Loss: [0.8184981346130371, 0.7972910404205322, 0.010603560134768486]\n","Discriminator Loss: 0.543036997461968\n","Generator Loss: [0.8249748945236206, 0.7824868559837341, 0.02124400995671749]\n","Discriminator Loss: 0.5471971082315576\n","Generator Loss: [0.8020893335342407, 0.7666796445846558, 0.017704831436276436]\n","Discriminator Loss: 0.5469801499821187\n","Generator Loss: [0.7918471097946167, 0.7685824632644653, 0.011632310226559639]\n","Discriminator Loss: 0.550234082955285\n","Generator Loss: [0.8314140439033508, 0.7963693141937256, 0.017522377893328667]\n","Discriminator Loss: 0.5464124004465702\n","Generator Loss: [0.8226248025894165, 0.7878832817077637, 0.017370769754052162]\n","Discriminator Loss: 0.542742269262817\n","Generator Loss: [0.8175642490386963, 0.7932980060577393, 0.012133119627833366]\n","Discriminator Loss: 0.5430799710284191\n","Generator Loss: [0.8141469359397888, 0.7856409549713135, 0.014252984896302223]\n","Discriminator Loss: 0.538315670250995\n","Generator Loss: [1.559096097946167, 0.7893638014793396, 0.3848661482334137]\n","Discriminator Loss: 0.5447081311540387\n","Generator Loss: [0.8134945631027222, 0.790524959564209, 0.011484804563224316]\n","Discriminator Loss: 0.5470024977839785\n","Generator Loss: [0.7796568274497986, 0.7514206767082214, 0.014118076302111149]\n","Discriminator Loss: 0.5457912717974978\n","Generator Loss: [3.294506311416626, 0.8143115043640137, 1.2400974035263062]\n","Discriminator Loss: 0.5401995602169336\n","Generator Loss: [0.8178570866584778, 0.7913534045219421, 0.013251828029751778]\n","Discriminator Loss: 0.5505209989460127\n","Generator Loss: [0.776892900466919, 0.7581637501716614, 0.009364566765725613]\n","Discriminator Loss: 0.5453772576420306\n","Generator Loss: [0.8178423643112183, 0.7790800333023071, 0.019381174817681313]\n","Discriminator Loss: 0.5473149194713187\n","Generator Loss: [0.7984867095947266, 0.7790528535842896, 0.009716914966702461]\n","Discriminator Loss: 0.5432989104165244\n","Generator Loss: [0.8017330765724182, 0.7814749479293823, 0.010129069909453392]\n","Discriminator Loss: 0.544270497501202\n","Generator Loss: [0.8132501244544983, 0.776119589805603, 0.018565265461802483]\n","Discriminator Loss: 0.5680743896955391\n","Generator Loss: [0.8316388130187988, 0.7857273817062378, 0.02295573055744171]\n","Discriminator Loss: 0.5544322714122245\n","Generator Loss: [0.8230005502700806, 0.7706706523895264, 0.0261649489402771]\n","Discriminator Loss: 0.5492339572374476\n","Generator Loss: [0.804442822933197, 0.7869186401367188, 0.008762102574110031]\n","Discriminator Loss: 0.5464239623979665\n","Generator Loss: [1.1145135164260864, 0.8012319803237915, 0.15664075314998627]\n","Discriminator Loss: 0.5460311153656221\n","Generator Loss: [0.8885624408721924, 0.7846651077270508, 0.051948681473731995]\n","Discriminator Loss: 0.5459829538231133\n","Generator Loss: [0.8143495321273804, 0.789445698261261, 0.012451919727027416]\n","Discriminator Loss: 0.5446591923555388\n","Generator Loss: [0.7970184683799744, 0.7734096050262451, 0.011804421432316303]\n","Discriminator Loss: 0.5486684016796062\n","Generator Loss: [0.8141500949859619, 0.7894025444984436, 0.01237377617508173]\n","Discriminator Loss: 0.5534542797759059\n","Generator Loss: [0.8277601599693298, 0.778650164604187, 0.024555010721087456]\n","Discriminator Loss: 0.5477278204925824\n","Generator Loss: [0.8087359070777893, 0.7920293807983398, 0.00835326686501503]\n","Discriminator Loss: 0.5439800312287844\n","Generator Loss: [0.8553383946418762, 0.7921395301818848, 0.03159944340586662]\n","Discriminator Loss: 0.5516207368018513\n","Generator Loss: [0.9221372604370117, 0.7889769077301025, 0.06658017635345459]\n","Discriminator Loss: 0.5452476122682128\n","Generator Loss: [0.9074978828430176, 0.777910590171814, 0.064793661236763]\n","Discriminator Loss: 0.5430700182478176\n","Generator Loss: [0.8173341751098633, 0.7929437756538391, 0.012195193208754063]\n","Discriminator Loss: 0.5464499147929018\n","Generator Loss: [0.8589926958084106, 0.7801153063774109, 0.039438679814338684]\n","Discriminator Loss: 0.5464849132004019\n","Generator Loss: [0.7908076643943787, 0.770655632019043, 0.01007602084428072]\n","Discriminator Loss: 0.5437299882469233\n","Generator Loss: [0.8090291023254395, 0.7852969765663147, 0.011866076849400997]\n","Discriminator Loss: 0.5412132537530852\n","Generator Loss: [0.8287957310676575, 0.7864850759506226, 0.021155323833227158]\n","Discriminator Loss: 0.542599745798725\n","Generator Loss: [0.7987028360366821, 0.7756534814834595, 0.011524662375450134]\n","Discriminator Loss: 0.5493488442425587\n","Generator Loss: [0.8252740502357483, 0.7872952222824097, 0.018989406526088715]\n","Discriminator Loss: 0.5529399447950709\n","Generator Loss: [0.7840334177017212, 0.7645314931869507, 0.009750962257385254]\n","Discriminator Loss: 0.549468963927211\n","Generator Loss: [0.8211630582809448, 0.7815151214599609, 0.019823962822556496]\n","Discriminator Loss: 0.5476650646705821\n","Generator Loss: [0.8085074424743652, 0.7865623235702515, 0.010972552001476288]\n","Discriminator Loss: 0.5500861353611981\n","Generator Loss: [0.8495354056358337, 0.7839758396148682, 0.03277977928519249]\n","Discriminator Loss: 0.5546378304024984\n","Generator Loss: [0.8008686900138855, 0.7784789800643921, 0.011194867081940174]\n","Discriminator Loss: 0.5529229213025246\n","Generator Loss: [0.816005289554596, 0.7798256278038025, 0.01808982528746128]\n","Discriminator Loss: 0.5431275624068803\n","Generator Loss: [1.0032627582550049, 0.7818431854248047, 0.1107097789645195]\n","Discriminator Loss: 0.5441442715382436\n","Generator Loss: [0.9097122550010681, 0.807467520236969, 0.051122378557920456]\n","Discriminator Loss: 0.5439082935663464\n","Generator Loss: [0.8408177495002747, 0.8007322549819946, 0.02004273608326912]\n","Discriminator Loss: 0.5480678282774534\n","Generator Loss: [0.7948542237281799, 0.7706367373466492, 0.012108751572668552]\n","Discriminator Loss: 0.5500229700555792\n","Generator Loss: [0.7883394360542297, 0.7717121839523315, 0.008313615806400776]\n","Discriminator Loss: 0.5463667161620833\n","Generator Loss: [1.4316508769989014, 0.7852749824523926, 0.3231879472732544]\n","Discriminator Loss: 0.5574593652345357\n","Generator Loss: [0.7948496341705322, 0.7717617750167847, 0.011543942615389824]\n","Discriminator Loss: 0.5538415195333073\n","Generator Loss: [0.8061286807060242, 0.7890994548797607, 0.008514621295034885]\n","Discriminator Loss: 0.5460075817100005\n","Generator Loss: [0.8077287077903748, 0.7715128064155579, 0.018107952550053596]\n","Discriminator Loss: 0.5372804513426672\n","Generator Loss: [0.8271294236183167, 0.8067284226417542, 0.010200504213571548]\n","Discriminator Loss: 0.5534611074144777\n","Generator Loss: [0.7933929562568665, 0.7730900645256042, 0.010151451453566551]\n","Discriminator Loss: 0.5535343275842024\n","Generator Loss: [0.7873682379722595, 0.7648912072181702, 0.011238519102334976]\n","Discriminator Loss: 0.5427212491376849\n","Generator Loss: [0.8039149045944214, 0.7836994528770447, 0.010107740759849548]\n","Discriminator Loss: 0.5446587339356483\n","Generator Loss: [0.8505852222442627, 0.8005135655403137, 0.02503582462668419]\n","Discriminator Loss: 0.5545488499446947\n","Generator Loss: [0.8334959745407104, 0.7941807508468628, 0.019657619297504425]\n","Discriminator Loss: 0.5613788664340973\n","Generator Loss: [1.1767587661743164, 1.095869541168213, 0.040444642305374146]\n","Discriminator Loss: 1.6203337907791138\n","Generator Loss: [6024.30810546875, 6024.28515625, 0.011434094049036503]\n","Discriminator Loss: 2.350649356842041\n","Generator Loss: [13409632256.0, 2339.7705078125, 6704815104.0]\n","Discriminator Loss: 0.8373083025217056\n","Generator Loss: [16931.21484375, 16860.3515625, 35.43174743652344]\n","Discriminator Loss: 1.421527182470129\n","Generator Loss: [26269575168.0, 9400.326171875, 13134782464.0]\n","Discriminator Loss: 1.0955623444026514\n","Generator Loss: [478.17608642578125, 315.6910400390625, 81.24252319335938]\n","Discriminator Loss: 0.6490496729475126\n","Generator Loss: [207.16421508789062, 35.225196838378906, 85.96951293945312]\n","Discriminator Loss: 0.7531040606709212\n","Generator Loss: [2298.43994140625, 2138.66064453125, 79.88958740234375]\n","Discriminator Loss: 0.7704625143560406\n","Generator Loss: [360.7198791503906, 227.63668823242188, 66.54159545898438]\n","Discriminator Loss: 0.7087774047940911\n","Generator Loss: [137.5384521484375, 25.92742156982422, 55.805519104003906]\n","Discriminator Loss: 0.5863837309225346\n","Generator Loss: [317.15478515625, 219.2294921875, 48.962650299072266]\n","Discriminator Loss: 0.7996477422711905\n","Generator Loss: [196.38143920898438, 110.54474639892578, 42.91834259033203]\n","Discriminator Loss: 0.6955625898772269\n","Generator Loss: [256.84576416015625, 181.3836212158203, 37.73107147216797]\n","Discriminator Loss: 0.7619742497336119\n","Generator Loss: [151.04681396484375, 84.84698486328125, 33.099918365478516]\n","Discriminator Loss: 0.6313396258337889\n","Generator Loss: [103.01605224609375, 44.91411590576172, 29.05097007751465]\n","Discriminator Loss: 0.6838906346238218\n","Generator Loss: [128.4405059814453, 75.34271240234375, 26.54889678955078]\n","Discriminator Loss: 0.7307422917801887\n","Generator Loss: [98.50114440917969, 55.949790954589844, 21.275678634643555]\n","Discriminator Loss: 0.6416842565231491\n","Generator Loss: [73.78678131103516, 32.65717315673828, 20.564804077148438]\n","Discriminator Loss: 0.6670978851907421\n","Generator Loss: [54.2041130065918, 16.180042266845703, 19.012035369873047]\n","Discriminator Loss: 0.6510320364614017\n","Generator Loss: [44.240318298339844, 8.543927192687988, 17.848196029663086]\n","Discriminator Loss: 0.5907672339235432\n","Generator Loss: [52.150184631347656, 19.25455665588379, 16.44781494140625]\n","Discriminator Loss: 0.5788899071048945\n","Generator Loss: [57.23462677001953, 28.41596221923828, 14.409331321716309]\n","Discriminator Loss: 0.7267550063552335\n","Generator Loss: [37.73548889160156, 10.189455032348633, 13.773016929626465]\n","Discriminator Loss: 0.6824386200896697\n","Generator Loss: [30.239280700683594, 4.398441791534424, 12.920419692993164]\n","Discriminator Loss: 0.6597252998617478\n","Generator Loss: [31.646690368652344, 8.404032707214355, 11.621328353881836]\n","Discriminator Loss: 0.6912931753031444\n","Generator Loss: [27.8858642578125, 2.2430968284606934, 12.821383476257324]\n","Discriminator Loss: 0.6024514358432498\n","Generator Loss: [22.984024047851562, 0.8598173260688782, 11.062103271484375]\n","Discriminator Loss: 0.6165394159033895\n","Generator Loss: [26.368305206298828, 5.980953216552734, 10.193675994873047]\n","Discriminator Loss: 0.667506517318543\n","Generator Loss: [20.627246856689453, 1.1170427799224854, 9.755102157592773]\n"]}],"source":["stage2 = StackGanStage2()\n","stage2.train_stage2()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"stackgan.ipynb","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2534241,"sourceId":5140550,"sourceType":"datasetVersion"},{"datasetId":4349178,"sourceId":7470861,"sourceType":"datasetVersion"},{"datasetId":4425350,"sourceId":7601702,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
